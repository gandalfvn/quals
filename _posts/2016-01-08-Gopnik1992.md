---
layout: post
title: "Why the child's theory of mind really is a theory"
date: 2016-01-08 14:10:48
category: Theory of mind
---

{% reference Gopnik1992 %}

# Summary

In this paper, Gopnik & Wellman lay out a wealth of developmental evidence supporting the theory theory, and describe precisely what they take the theory theory to be and what it would predict. They begin with a definition of "theoretical construct":

> Theoretical constructs are abstract entities postulated, or recruited from elsewhere, to provide a separate causal-explanatory level of analysis that accounts for evidential phenomena... Theoretical constructs need not be definitely unobservable, but they must be appeals to a set of entities removed from, and underlying, the evidential phenomena themselves... Theoretical constructs do not work independently, they work together in systems characterized by laws or structure. (pg. 146-7)

There are two primary characteristics of theories: they are abstract, and they coherent. Because of these two properties, theories are able to generate predictions about a wide range of behavior, including situations that go beyond the data experienced so far. Theories may also produce incorrect predictions, in cases where they do not perfectly match the phenomena being described. Theories additionally provide *explanatory depth* because, by definition, they "produce interpretations of evidence, not simply descriptions of evidence and generalizations about it". Finally, theories can be modified in the face of new, conflicting evidence.

Gopnik & Wellman describe theory change as follows. First, there is an initial theory. If evidence is encountered that conflicts with this theory, it will initially be ignored as noise, and eventually start to accumulate as ad-hoc auxiliary rules tacked on to the original theory. Eventually, as the original theory gets unweildy, if a new competing theory is encountered or developed, it may replace the old theory. Elements of the new theory may first appear in the auxiliary rules, but its full predictive power is not utilized until it fully subsumes the old theory.

Gopnik & Wellman argue that this process of theory change is evident in the development of theory of mind in children, particularly between 2.5 and around 4 years of age. There are several stages of development:

1. 2-year-olds have some understanding of desires and perceptions. However, these concepts are relatively rudimentary and are "nonrepresentational": desires are "drives towards object" and perceptions are an "awareness of objects". This leads to simple causal rules like "if X can see an object, and X desires the object, then X will try to get the object".
2. At 3, children begin to exhibit some (non-representational) understanding of beliefs, though it does not seem to have much of an effect on their behavior. While it seems that they typically have a notion of belief that directly reflects what is true in the world, there is some evidence that 3-year-olds can acknowledge the idea of a false belief. Even if they might be able to acknowledge the idea of a false belief, though, their theory of how the world works does not include beliefs as a factor in producing actions.
3. By 4 or 5 years, children seem to have developed a "representational theory of mind" in which desires, perceptions, beliefs, and pretenses are all included as *representations* of reality, rather than being reflections of reality itself (i.e., *intentional*).

Gopnik & Wellman next cite considerable evidence supporting this theory:

* **Explanations**: 2-year-old's answers to questions like "why is she doing that?" tend to reflect desires ("she wants it"), while 3- and 4-year-old's answers reflect beliefs ("she thinks it's there").
* **Predictions**: With the initial desire-perception theory, children should be able to predict that desires will differ and that people will do things given that they have a desire. They should also be able to predict the perceptions of other, but not that things might be perceived *differently* by multiple people. And, they should not be able to predict anything that relies on the notion that people have different beliefs that may not reflect the true world. Evidence for this comes from the false-belief task, but other types of tasks as well (appearance-reality tasks, questions about sources of beliefs, understanding pictorial representation systems).
* **Interpretations**: Also with the initial desire-perception theory, children should initially ignore evidence that is counter to the theory. Indeed, children will misreport evidence they have just heard (e.g. someone saying "I think it is blue", when it is white).
* **Transitional phenomena**: Children seem to initially realize that concepts like perception and desire do not necessarily reflect the world (i.e., they *misrepresent* what is true) earlier than they come to the same realization about beliefs. When pressed, 3-year-olds may beging to explain inconsistencies in terms of misrepresentation when those inconsistencies are pointed out to them.

Next, Gopnik & Wellman turn to their critique of simulation theory. They focus on two main issues: first, "the centrality of your own mind in any understanding of the minds of others", and second, "how development should proceed" (pg. 160). The first point is similar to the one made by [Stich & Nichols]({{site.baseurl}}{% post_url 2016-01-07-Stich1992 %}) about how a simulation should give the same results for oneself and for others (provided the inputs are correct). For the second point, they argue that ST should predict a developmental trajectory in which children make errors on states that are "hard" to simulate, and that they should originally make "egocentric" errors that reflect an inability to modify their simulations to reflect other people's states. They give empirical evidence based on these two issues:

1. *"Three-year-old children make false attributions to themselves, that exactly parallel their false attributions to others."* From a simulation point of view, it doesn't make sense why children would make mistakes about their *own* mental states if they were just reading off the results from whatever mechanism is used to run the simulations. Additionally, 3-year-olds are good at reporting their mental states in terms of desires and perceptions, but not beliefs. Why would the simulation account predict *a priori* for this to be the case? What is special about beliefs in the simulation case that makes them harder to simulate?
2. *"Three-year-old children make correct non-egocentric attributions to themselves and others for some mental states."* Children can report that other people have different perceptions and desires than their own, so in terms of the simulation account, they are clearly able to adjust their simulations to incorporate other people's perceptions and desires. Why can they not do the same for beliefs?
3. *"Children refer to only some mental states in their explanations, and refer to different mental states at different stages of their development."* Younger children tend to give explanations about other people's behavior in terms of desires, while older children appeal to beliefs. And, in either of these scenarios, children are preferring beliefs in desires over "fears and fantasies, pains and sensations or any of a vast number of experientially available mental states". What about simulation theory would predict that they answer in this way? Why is "she fears that kitty is lost" a dispreferred explanation to "she wants the kitty" or "she thinks the kitty is there"? In theory theory, desires and beliefs are core constructs, and thus make sense as the types of concepts that children appeal to.
4. *"Children's understanding of other psychological phenomena changes in parallel with their understanding of false belief."* While three-year-olds can report their beliefs, they seem unable to report *how* they got that information, even if it was just told to them. Additionally, three-year-olds seem to be unable to gauge the reliability of a source of information (e.g. distinguishing between someone who knows, vs. someone who is just guessing). Theory theory predicts these other types of behavior because knowing where information comes from and how reliable it is is related to the idea of seeing beliefs as representational—i.e., separate entities from what they correspond to in the world. It is not clear why simulation theory would *a priori* predict these types of results.

# Takeaways

In reading this paper I had a bit of a small epiphany regarding why simulation seems to be such a loaded word, and why it seems to be such a complicated issue (not just in terms of theory of mind, but in general).

Gopnik & Wellman seem to really be proposing what is a *computational-level* analysis of theory of mind: we begin with theories based on our initial capabilities (i.e., perception) and urges (i.e., desires). The problem is then to update those theories as new information comes along. The specific changes that Gopnik & Wellman predict are really predictions about how a particular solution to that problem ends up playing out.

The simulation theorists ([Gordon]({{site.baseurl}}{% post_url 2016-01-07-Gordon1992 %}) and [Goldman]({{site.baseurl}}{% post_url 2016-01-08-Goldman1992 %})) are proposing a squarely *algorithmic-level* account. It is highly mechanistic and focuses on the solution as a particular algorithm (the one that is already implemented by our brains), though it is vague on the specific representation of inputs and outputs (beliefs, desires, perceptions, decisionse—but it is not clear exactly what forms those take).

In constrast, simulation *can* actually be the (approximate) solution to a computational-level problem. In physical simulation, the computational-level problem can be expressed analytically as a differential equation, but it cannot be solved analytically. Numerical simulation, though an approximation, is the only known way to solve the problem short of setting up the actual physical situation in the real world. In probabilistic simulation, the story is similar. The computational-level problem can be expressed analytically, but often cannot be solve analytically. Monte-Carlo simulation is one particular class of methods for approximating the solution.

Thus, sometimes simulation is the correct approach when it is the approximate solution to a computational-level problem. However, it is not *always* the right answer, and is certainly not the right solution *a priori*. In particular, I think it is a mistake to talk about simulation as being the right solution just on the basis of having a black-box mechanism at our disposal. Just because a tool might be available for use doesn't make it the *right* tool to use. I think instead of talking about whether simulation is the mechanism that is used by the mind in some particular case, the discussion should be about what the *best* thing to use would be, and whether that happens to be simulation or not.
