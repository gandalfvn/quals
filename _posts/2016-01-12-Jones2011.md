---
layout: post
title: "Bayesian fundamentalism or enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition"
date: 2016-01-12 10:59:51
category: Challenges for probabilistic models of cognition
---

{% reference Jones2011 %}

# Summary

Jones & Love critique the use of Bayesian models of cognition, contrasting *Bayesian Fundamentalism*, which:

> ... adheres to the tenet that human behavior can be explained through rational analysis — once the correct probabilistic interpretation of the task environment has been identified — without recourse to process, representation, resource limitations, or physiological or developmental data. (pg. 170)

with *Bayesian Enlightenment*, which:

> ... goes beyond the dogma of pure rational analysis and actively attemps to integrate with other avenues of inquiry in cognitive science... [it] thus treats Bayesian models as making both rational and mechanistic commitments, and it takes as a goal the joint evaluation of both. (pg. 170)

Jones & Love compare Bayesian Fundamentalism to Behaviorism and Evolutionary Psychology, arguing that Bayesian Fundamentalism doesn't care about what is going on in the mind, only about the constraints of the environment and what the rational solution to the problem is given those constraints. They argue that despite this position, Bayesian models really *do* make mechanistic assumptions about representations and processes. For example, in critiquing [Kemp et al.]({{site.baseurl}}{% post_url 2015-11-09-Kemp2007 %}):

> However, it is not Bayes' Rule or even the notion of overhypotheses that drives the prediction; rather it is the particular overhypotheses that were built into the model. In other words, the model was endowed with the capability to recognize a particular pattern (viz., regularity across words in which perceptual dimensions are relevant to meaning), so the fact that it indeed recognizes that pattern when presented with it is not surprising or theoretically informative. (pg. 178)

They also argue that despite emphasizing how important it is to specify the specific environmental constraints giving rise to the model, Bayesian Fundamentalists rarely do so. And, even if they did, it is nearly impossible to know what the right set of constraints are; in general it is possible to give a post-hoc argument for a particular set of constraints being "rational":

> The relevant environment for rational action could be the local environment present in the laboratory task, similar situations (however defined) that the person has experienced, all experiences over the person's life, all experiences of our species, all experiences of all ancestral organisms traced back to single cell organisms, and so on. Furthermore, once the relevant environment is specified and characterized, the rational theorist has considerable flexibility in characterizing which relevant measures or statistics from the environment should enter into the optimality calculations. (pg. 181)

Jones & Love note furthermore that from an evolutionary perspective, it is not *behavior* that is optimized but *mechanism* that is optimized. Whatever the existing mechanism is, it is perhaps a slightly more optimized version of the mechanism that was there before, and so on. That means only that if the behavior is optimized at all, it is locally optimal, not globally optimal, because it depends on the process of optimization that has occurred. But even then, many things may simply be accidents or side effects and are not even optimized at all.

Jones & Love argue that Bayesian Fundamentalism says nothing about development because Bayesian models are mechanism-free, and therefore it is not clear what develops:

> One key question for any developmental model is what develops. In rational models, the answer is that nothing develops. Rational models are mechanism-free, leaving only information sampled to change over time. Although some aspects of development are driven by acquisition of more observations, other aspects of development clearly reflect maturational changes in the mechanism. (pg. 182)

Finally, the suggest that Bayesian Enlightenment holds promise by:

1. Treating Bayesian representations (e.g. the generative model or parameteters of conjugate priors) as actual hypotheses about psychological representations
2. Investigating Bayesian approximation algorithms (e.g. MCMC) as possible algorithmic-level processes
3. Performing rational analysis not within the context of the environment, but within the context of a mechanistic model

# Takeaways

Jones & Love actually make a lot of good points about how Bayesian models can be useful and fruitful. I disagree with their claims that they aren't (or weren't) already being used in that way. I also disagree with their characterization that Bayesian models aren't informative if they recognize the patterns they were built to recognize (e.g. in the discussion of Kemp et al.). It is the connection with the data that makes them informative: if we have a hypothesis about how people reason in a particular domain, we can instantiate that explicit hypothesis in a Bayesian model and verify that the hypothesis does actually produce the same behavior as people. The model is just a tool for making the assumptions and implications of the hypothesis explicit.

I am not sure why Jones & Love think that Bayesian models aren't committed to hypotheses about psychological representation. The sense in which the computation of Bayesian models "doesn't matter" is that Bayesians are not committed to the specific way by which the predictions of the model are computed; what they are committed to is the structure of the model. The structured representations offered by Bayesian models are their most powerful feature, in my opinion.
