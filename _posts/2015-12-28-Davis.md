---
layout: post
title: "The scope and limits of simulation in automated reasoning"
date: 2015-12-28 16:54:26
category: Physical reasoning with dynamics models
---

{% reference Davis %}

# Summary

Davis & Marcus summarize some of the challenges that they seen in using simulation for automated reasoning (and in particular physical reasoning). The twelve challenges that they discuss are:

1. **Choosing an appropriate model**. For example, what kind of cutting instrument should be used for firewood? (Not scissors.)
2. **Discretizing time**. There are various issues with this, for example having objects pass through each other, running into instability due to inappropriate discretizations, or violating constrainst such as conservation of matter.
3. **Discontinuous dynamics**. Small perturbations can lead to large changes in dynamics (e.g. rolling a die, chaotic systems).
4. **Choosing an idealization**. What representation should be used in the simulation? e.g., should a pendulum be represented by a point mass moving on a circle, by a rigid shape constrained to be a certain distance from the origin, by a rigid shape on a (deformable) string, etc.?
5. **Drawing "easy" inferences**. Some types of inferences are "easy" but simulation is not well-suited to them. For example:
  * Invariance under irrelevant changes (if a jar fits on a shelf, and you fill it, it will still fit on the shelf)
  * Invariance under changes of scale (e.g. kinematics is the same regardless of the size of the objects)
  * Approximation (e.g. containers of similar sizes will hold similar amounts of liquid)
  * Ordering on a relevant dimension (e.g. if a toy fits in a box, then it will fit in a larger box)
  * Decomposition (e.g. if there are two independent subsystems, they can be reasoned about independently)
  * Rules of thumb (e.g. you spill coffee in your office --> it won't end up in another office)
6. **Incorporating extra-physical information**. Sometimes information that is non-physical can still be useful in predicting physical systems (e.g. general accuracy of a baseball pitcher)
7. **Incomplete information**. You don't always have fully specified information about the system, such as occluded geometry, unobservable properties (like mass, friction), unknown dynamics, etc.
8. **Irrelevant information**. How should the automated system determine what information is irrelevant and should be excluded? This is something that is typically done by hand (by the modeller) but in the general case needs to be handled by the system.
9. **Range of scales**. How can simulation scale from very small scales (e.g. everyday objects) to very large scales (e.g. stars)?
10. **Tasks other than prediction**. There are other types of tasks that simulation does not deal well with:
  * Interpolation
  * Planning
  * Inferring object shape
  * Inferring physical properties
  * Design of systems
  * Comparative analysis (determining how a modification to a system would affect the solution)
11. **The frame problem**. Reasoning separately about things that are independent.
12. **Checking simulations with common sense**. If simulation is the only reasoning mechanism, then there is nothing to check whether the simulation is correct.

# Takeaways

I certainly agree with Davis & Marcus that simulation isn't the only form of reasoning, and I think there are very interesting questions regarding how we trade off between different forms of reasoning. Any system that uses simulation almost certainly needs to incorporate the following choices:

1. Determine what type of reasoning to use, based on features of the problem.
2. If simulation is appropriate, determine how to set up the simulation and how to evaluate/interpret the results of the simulation.

I would argue that the majority of Davis & Marcus' objections are just reformulations of the question of how to address these two choices. Points 1, 2, 4, 7, 8, 9, 11, and 12 are instances of "how to set up an appropriate simulation and interpret it?". Points 5, 6, and 10 are instances of "should simulation be used, or something else?". The only remaining point is Point 3, which doesn't seem like a problem to me. Reasoning about the world is inherently uncertain, and any reasoning system needs to be able to take that type of uncertainty into account. Simulation needs to be precise in one sense (that things need to be fully specified), but it does not need to be *accurate*, and the specification need not necessarily be precise, either.

I do think that the questions of whether to use simulation or something else, and then *how* to use simulation, are incredibly important and extremely difficult questions. Also, the fact that I've boiled 12 points down into 2 doesn't mean that those two choices aren't *incredibly* difficult and don't have multiple subquestions. But I don't think that the fact that simulation is a difficult topic means that we shouldn't use it or research its capabilities and the extent of its utility. It seems to me that simulation—and generative models more broadly—are really powerful because they're so flexible and can apply in so many ways to so many types of scenarios. The question is how to take full advantage of that flexbility.
