{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Bayes-Adaptive Monte-Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{% reference Guez2013 %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt, log\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I implemented an abstract base class for running BAMCP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BAMCP(object, metaclass=ABCMeta):\n",
    "\n",
    "    actions = None\n",
    "    states = None\n",
    "    max_reward = None\n",
    "\n",
    "    def __init__(self, discount=0.95, epsilon=0.5, exploration=3):\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.exploration = exploration\n",
    "\n",
    "        self.N = None\n",
    "        self.Q = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def mdp_dist(self, history):\n",
    "        \"\"\"Sampling distribution for MDPs. Must be overwritten by subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reward_func(self, state, action, mdp):\n",
    "        \"\"\"Reward function for the given state/action. Must be overwritten\n",
    "        by subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition_func(self, state, action, mdp):\n",
    "        \"\"\"Transition function for the given state/action. Must be overwritten\n",
    "        by subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def valid_actions(self, state, mdp):\n",
    "        \"\"\"Returns a list of valid actions that can be taken in the given\n",
    "        state. Must be overwritten by subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def rollout_policy(self, state, history, mdp):\n",
    "        \"\"\"Uniform rollout policy.\"\"\"\n",
    "        actions = self.valid_actions(state, mdp)\n",
    "        if len(actions) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return np.random.choice(actions)\n",
    "\n",
    "    def tree_policy(self, state, history, mdp):\n",
    "        \"\"\"UCT tree policy.\"\"\"\n",
    "        actions = self.valid_actions(state, mdp)\n",
    "        if len(actions) == 0:\n",
    "            return None\n",
    "        elif len(actions) == 1:\n",
    "            return actions[0]\n",
    "\n",
    "        N = self.N[(state, history)]\n",
    "        values = np.empty(len(actions))\n",
    "        for i in range(len(actions)):\n",
    "            Q = self.Q[(state, history)][actions[i]]\n",
    "            Ni = self.N[(state, history, actions[i])]\n",
    "            if Ni == 0:\n",
    "                values[i] = np.inf\n",
    "            else:\n",
    "                values[i] = Q + self.exploration * sqrt(log(N / Ni))\n",
    "\n",
    "        return np.random.choice(actions[values == np.max(values)])\n",
    "\n",
    "    def value_func(self, state, history):\n",
    "        \"\"\"Returns the value of the given state, as well as the best action to take.\"\"\"\n",
    "        actions = list(self.Q[(state, history)].keys())\n",
    "        values = np.empty(len(actions))\n",
    "        for i in range(len(actions)):\n",
    "            values[i] = self.Q[(state, history)][actions[i]]\n",
    "\n",
    "        if len(actions) == 1:\n",
    "            return values[0], actions[0]\n",
    "\n",
    "        idx = np.random.choice(np.nonzero(values == np.max(values))[0])\n",
    "        return values[idx], actions[idx]\n",
    "\n",
    "    def search(self, state, history, max_time=1):\n",
    "        \"\"\"Run a search from the root of the tree. Return the value of the root\n",
    "        and the next action to take.\n",
    "\n",
    "        \"\"\"\n",
    "        self.N = defaultdict(lambda: 0)\n",
    "        self.Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "        t = time.time()\n",
    "        while (time.time() - t) < max_time:\n",
    "            self.simulate(state, history, self.mdp_dist(history))\n",
    "\n",
    "        return self.value_func(state, history)\n",
    "\n",
    "    def rollout(self, state, history, mdp, depth):\n",
    "        \"\"\"Execute a rollout simulation from a leaf node in the tree.\"\"\"\n",
    "\n",
    "        if state is None:\n",
    "            return 0\n",
    "        if (self.discount ** depth * self.max_reward) < self.epsilon:\n",
    "            return 0\n",
    "\n",
    "        action = self.rollout_policy(state, history, mdp)\n",
    "        new_state = self.transition_func(state, action, mdp)\n",
    "        new_history = history + ((state, action, new_state),)\n",
    "\n",
    "        curr_reward = self.reward_func(state, action, mdp)\n",
    "        future_reward = self.rollout(new_state, new_history, mdp, depth + 1)\n",
    "        reward = curr_reward + self.discount * future_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def simulate(self, state, history, mdp, depth=0):\n",
    "        \"\"\"Simulate an episode from the given state, following the tree policy\n",
    "        until a leaf node is reached, at which point the rollout policy is used.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if state is None:\n",
    "            return 0\n",
    "        if (self.discount ** depth * self.max_reward) < self.epsilon:\n",
    "            return 0\n",
    "\n",
    "        # leaf node\n",
    "        if self.N[(state, history)] == 0:\n",
    "            action = self.rollout_policy(state, history, mdp)\n",
    "            new_state = self.transition_func(state, action, mdp)\n",
    "            new_history = history + ((state, action, new_state),)\n",
    "\n",
    "            curr_reward = self.reward_func(state, action, mdp)\n",
    "            future_reward = self.rollout(new_state, new_history, mdp, depth)\n",
    "            reward = curr_reward + self.discount * future_reward\n",
    "\n",
    "            self.N[(state, history)] = 1\n",
    "            self.N[(state, history, action)] = 1\n",
    "            self.Q[(state, history)][action] = reward\n",
    "\n",
    "        # non-leaf node\n",
    "        else:\n",
    "            action = self.tree_policy(state, history, mdp)\n",
    "            new_state = self.transition_func(state, action, mdp)\n",
    "            new_history = history + ((state, action, new_state),)\n",
    "\n",
    "            curr_reward = self.reward_func(state, action, mdp)\n",
    "            future_reward = self.simulate(new_state, new_history, mdp, depth + 1)\n",
    "            reward = curr_reward + self.discount * future_reward\n",
    "\n",
    "            self.N[(state, history)] += 1\n",
    "            self.N[(state, history, action)] += 1\n",
    "\n",
    "            dQ = (reward - self.Q[(state, history)][action]) / self.N[(state, history, action)]\n",
    "            self.Q[(state, history)][action] += dQ\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out the `BAMCP` base class on the toy problem given in Section 3.1.1 (page 855) of Guez et al.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ToyBAMCP(BAMCP):\n",
    "\n",
    "    actions = np.array([0, 1])\n",
    "    states = np.array([0, 1, 2, 3, 4, 5])\n",
    "    max_reward = 2\n",
    "\n",
    "    def __init__(self, p, *args, **kwargs):\n",
    "        super(ToyBAMCP, self).__init__(*args, **kwargs)\n",
    "        self.p = p\n",
    "\n",
    "        self.MDP1 = np.zeros((6, 2, 6))\n",
    "        self.MDP1[0, 1, 5] = 1\n",
    "        self.MDP1[0, 0, 1] = 0.8\n",
    "        self.MDP1[0, 0, 2] = 0.2\n",
    "        self.MDP1[1, 0, 3] = 1\n",
    "        self.MDP1[1, 1, 4] = 1\n",
    "        self.MDP1[2, 0, 4] = 1\n",
    "        self.MDP1[2, 1, 3] = 1\n",
    "\n",
    "        self.MDP2 = np.zeros((6, 2, 6))\n",
    "        self.MDP2[0, 1, 5] = 1\n",
    "        self.MDP2[0, 0, 1] = 0.2\n",
    "        self.MDP2[0, 0, 2] = 0.8\n",
    "        self.MDP2[1, 0, 4] = 1\n",
    "        self.MDP2[1, 1, 3] = 1\n",
    "        self.MDP2[2, 0, 3] = 1\n",
    "        self.MDP2[2, 1, 4] = 1\n",
    "\n",
    "    def mdp_dist(self, history):\n",
    "        p1 = self.p\n",
    "        p2 = 1 - self.p\n",
    "        for sas in history:\n",
    "            p1 *= self.MDP1[sas]\n",
    "            p2 *= self.MDP2[sas]\n",
    "\n",
    "        Z = p1 + p2\n",
    "        p1 = p1 / Z\n",
    "        p2 = p2 / Z\n",
    "\n",
    "        if np.random.rand() < p1:\n",
    "            return self.MDP1\n",
    "        else:\n",
    "            return self.MDP2\n",
    "\n",
    "    def reward_func(self, state, action, mdp):\n",
    "        if state == 3:\n",
    "            return 2\n",
    "        elif state == 4:\n",
    "            return -2\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def transition_func(self, state, action, mdp):\n",
    "        p = mdp[state, action]\n",
    "        if (p == 0).all():\n",
    "            return None\n",
    "        return np.random.choice(self.states, p=p)\n",
    "\n",
    "    def valid_actions(self, state, mdp):\n",
    "        ok = ~((mdp[state] == 0).all(axis=1))\n",
    "        if not ok.any():\n",
    "            return np.array([])\n",
    "        else:\n",
    "            return self.actions[ok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our toy BAMCP, create an instance of it where both MDPs have equal prior probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "planner = ToyBAMCP(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the planner from the first state, $s=0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.64245762711864185, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "history = tuple()\n",
    "planner.search(0, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the action ($a=0$) recommended by the previous search, and run the planner from the next state ($s=1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1482566953006605, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 1\n",
    "history = ((0, 0, 1),)\n",
    "planner.search(state, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again take the recommended action ($a=0$), and run the planner from the next state ($s=1$), finding that this is a terminal state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 3\n",
    "history = ((0, 0, 1), (1, 0, 3))\n",
    "planner.search(state, history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
