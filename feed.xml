<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 04 Jan 2016 17:50:17 -0800</pubDate>
    <lastBuildDate>Mon, 04 Jan 2016 17:50:17 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Qualitative modeling</title>
        <description>&lt;p&gt;&lt;span id=&quot;Forbus2011&quot;&gt;Forbus, K. D. (2011). Qualitative modeling. &lt;i&gt;Wiley Interdisciplinary Reviews: Cognitive Science&lt;/i&gt;, &lt;i&gt;2&lt;/i&gt;(4), 374–391. doi:10.1002/wcs.115&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this article, Forbus summarizes the state of the field of qualitative reasoning, explaining how qualitative reasoning works and what it can be applied to. He begins by outlining three principles that are core to qualitative modeling:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Discretization&lt;/em&gt; – qualitative representations are almost always discrete.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Relevance&lt;/em&gt; – the manner in which the discretization is chosen (an in general how the model is set up) depends on the relevance of different aspects of the situation that is being modeled.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ambiguity&lt;/em&gt; – predictions made by qualitative models are ambiguous and there may be multiple possible predictions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Next, Forbus describes what a qualitative representation is. I won’t go into detail on this, since it’s mostly covered by my notes on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Kuipers1986.html&quot;&gt;Kuipers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Given the definitions for qualitative representations, Forbus describes the qualitative mathematical operations that can be applied to them. One of &lt;em&gt;qualitative proportionality&lt;/em&gt;, which is that “all else being equal, if $B$ increases, then $A$ will increase, and if $B$ decreases, then $A$ will decrease” (pg. 4). To get around the fact that not all functions are monotonic, Forbus describes to options. First, functions can be broken up into &lt;em&gt;model fragments&lt;/em&gt; where monotonicity holds within a given fragment. Second, a &lt;em&gt;compositional modeling language&lt;/em&gt; can be used instead. The notation used in the explanation of the compositional modeling language isn’t explained, though, so I’m not entirely clear on how it works. Same thing goes for &lt;em&gt;confluences&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Forbus moves on to describe the pieces of a qualitative model, which include &lt;em&gt;processes&lt;/em&gt; (e.g., heat flow), &lt;em&gt;components&lt;/em&gt; (individual discrete parts that can be combined), and &lt;em&gt;fields&lt;/em&gt; (a division of space in qualitatively distinct regions where some qualitative parameter is constant). These pieces, represented as model fragments, are assembed in the &lt;em&gt;compositional modeling&lt;/em&gt; methodology to form a &lt;em&gt;domain theory&lt;/em&gt;. There are also &lt;em&gt;modeling assumptions&lt;/em&gt; which represent choices that need to be made depending on their relevance to the situation (for example, whether a thermal object should be considered a regular thermal object or just a temperature source).&lt;/p&gt;

&lt;p&gt;The qualitative model can then be used in a qualitative simulation. I also won’t go into details here as I already wrote about it in my notes on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Kuipers1986.html&quot;&gt;Kuipers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Forbus discusses how qualitative modeling can be used to model causality and spatial reasoning. For spatial reasoning, there are a few considerations: topology (which can be represented using a region connection calculus), direction, position, and shape. Space itself also needs to be decomposed into discrete regions and edges; this can be done with something called &lt;em&gt;place vocabularies&lt;/em&gt; (e.g. this is what was used by the FROB system, see &lt;a href=&quot;/quals/mental%20models/2015/12/19/Gentner1983.html&quot;&gt;Chapter 4 of Gentner&lt;/a&gt;).&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The term “simulation” when used in “qualitative simulation” does seem to mean something along the lines of simulation of a physical process—though because it is discrete and approximate, it is perhaps closer to something like the simulations used in Monte-Carlo Tree Search. It would be interesting to see how qualitative simulation would do in certain reinforcement learning settings when combined with something like MCTS. For example, in playing Atari games, maybe something like qualitative simulation would be useful for efficiently determining high-level actions a player could take (rather than the low level actions of left/right/jump/etc).&lt;/p&gt;

&lt;p&gt;One thing that is not clear to me, though, is how qualitative modeling fits into scenarios where there is uncertainty about the environment. In one sense, it’s great at doing that, because you don’t necessarily need to numerically specify every parameter. In another sense, though, it doesn’t seem like qualitative modeling would work well in an uncertain case where, for example, it might be &lt;em&gt;possible&lt;/em&gt; for a situation to unfold in a certain way (depending on the initial conditions), but highly unlikely. It doesn’t seem like qualitative simulation is set up for being able to express the magnitude of uncertainty or quality.&lt;/p&gt;

&lt;p&gt;Also, despite their suggestion to the contrary, qualitative models suffer from many of the same issues that are discussed by &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Davis.html&quot;&gt;Davis &amp;amp; Marcus&lt;/a&gt; as physical simulation does. Importantly, qualitative modeling doesn’t free you from having to make hard choices about &lt;em&gt;how&lt;/em&gt; to set the simulation up, something that Forbus explicitly notes in this paper. You still have to make the relevant decisions about which properties are important, how to do the discretization, etc.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 08:58:54 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Forbus2011.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Forbus2011.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Qualitative simulation</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kuipers1986&quot;&gt;Kuipers, B. (1986). Qualitative Simulation. &lt;i&gt;Artificial Intelligence&lt;/i&gt;, &lt;i&gt;29&lt;/i&gt;(3), 289–338. doi:10.1016/0004-3702(86)90073-1&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Kuipers gives an overview of what qualitative simulation is, how it can be used, and what some its limitations are. He focuses specifically on the QSIM system, which is similar to other qualitative reasoning systems (e.g. from de Kleer, Forbus) but which also has a few differences. Regardless of the specific system, the point of qualitative reasoning is to break down continuous sytems into discrete analogues. Given a set of constraints that the continuous system must follow, along with an initial qualitative state description, qualitative reasoning determines how the system must evolve without necessarily specifying any precise values. For example, qualitative reasoning can be used to infer that if a ball is thrown upwards, it must eventually come down again (but the specific position and velocity of the ball need not be specified).&lt;/p&gt;

&lt;p&gt;Kuipers shows that the QSIM method will always produce the true behavior of the system that it describes, but it may additionally produce false behaviors if the proper constraints aren’t given (e.g. conservation of energy).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;qualitative-behavior&quot;&gt;Qualitative behavior&lt;/h2&gt;

&lt;p&gt;The qualitative description of a continuous and differentiable function $f:[a,b]\rightarrow\mathbb{R}^*$ relies on a set of &lt;em&gt;landmark values&lt;/em&gt;, which include 0, $f(a)$, $f(b)$, and $f(t)$ where $t$ is a critical point of the function. A &lt;em&gt;distinguished time point&lt;/em&gt; is the corresponding $t$ for a landmark value.&lt;/p&gt;

&lt;p&gt;Then, the &lt;em&gt;qualitative state&lt;/em&gt; of the function at a particular point in time is $\mathrm{QS}(f,t)=[\mathrm{qval},\mathrm{qdir}]$, where $\mathrm{qval}$ is either a landmark value or the interval between landmark values, and where $\mathrm{qdir}$ is $\mathrm{sgn}(f^\prime(t))$. The &lt;em&gt;qualitative behavior&lt;/em&gt; of a function is the sequence $[\mathrm{QS}(f,t_0),\mathrm{QS}(f,t_0,t_1),\mathrm{QS}(f,t_1),\ldots{},\mathrm{QS}(f,t_{n-1},t_n),\mathrm{QS}(f,t_n)]$, where each $t_i$ is a distinguished time point.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;qualitative state transition&lt;/em&gt; is either a &lt;em&gt;P-transition&lt;/em&gt;, which is the transition from a distinguished time point to an interval between distinguished time points: $\mathrm{QS}(f,t_i)\Rightarrow \mathrm{QS}(f, t_i, t_{i+1})$. Similarly, a &lt;em&gt;I-transition&lt;/em&gt; is the other way around: $\mathrm{QS}(f,t_{i-1},t_i)\Rightarrow\mathrm{QS}(f,t_i)$.&lt;/p&gt;

&lt;h2 id=&quot;qualitative-structure&quot;&gt;Qualitative structure&lt;/h2&gt;

&lt;p&gt;Constraints can be imposed on either a single function, or on a pair of functions, for example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ADD}(f, g, h)$ is defined as $f(t)+g(t)=h(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{MULT}(f, g, h)$ is defined as $f(t)\cdot{}g(t)=h(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{MINUS}(f, g)$ is defined as $f(t)=-g(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{DERIV}(f, g)$ is defined as $f^\prime(t)=g(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{M}^+(f, g)$ is defined as $f(t)=H(g(t))$ where $H^\prime(x)&amp;gt;0$. This rougly says that $f$ and $g$ are directly proportional&lt;/li&gt;
  &lt;li&gt;$\mathrm{M}^-(f, g)$ is defined as $f(t)=H(g(t))$ where $H^\prime(x)&amp;lt;0$. This roughly says that $f$ and $g$ are inversely proportional.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that these constraints only need to hold for $t$ within the domain of $f$ and $g$ (i.e., so you could have $x=\cos\theta$ on $\theta\in(0,\pi)$ and then $\mathrm{M}^-(\theta, x)$ will be true for that specific range of $\theta$).&lt;/p&gt;

&lt;h2 id=&quot;qualitative-simulation&quot;&gt;Qualitative simulation&lt;/h2&gt;

&lt;p&gt;The qualitative simulation takes as input the functions, constraints, landmark values, upper and lower range limits, an initial time point $t_0$, and initial qualitative values for the functions.&lt;/p&gt;

&lt;p&gt;The qualitative simulation returns as output the qualitative behavior descriptions for the given functions, which include: the distinguished time points, landmark values (which may include additional ones not given as input), and qualitative state descriptions for each distinguished time point and interal between time points.&lt;/p&gt;

&lt;p&gt;The way that the simulation works for a single state is, roughly:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;For each function, determine the P-transitions and I-transitions for the current qualitative state&lt;/li&gt;
  &lt;li&gt;For each constraint, form combinations of “transition tuples” for each of the functions involved in the constraint, and filter out those which violate the constraint&lt;/li&gt;
  &lt;li&gt;For each constraint, filter our transition tuples for which there are no transition tuples for adjacent constraints that have the same transition for the shared parameter (e.g. $(I4,I9)$ for $\mathrm{DERIV}(Y,V)$ would be filtered out if there is no corresponding tuple beginning with $I9$ for $\mathrm{DERIV}(V, A)$).&lt;/li&gt;
  &lt;li&gt;Generate possible global interpretations based on the remaining tuples&lt;/li&gt;
  &lt;li&gt;Check which interpretations constitute either a state that has been visited before (cylce) or if the function diverges (goes to infinity). These are leaves of the seach tree; others are next possible states.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This was a really nice, detailed explanation of how (one particular version of) qualitative simulation and reasoning works. I think it’s cool that you can show that this type of qualitative reasoning will produce the right answer (even if it might also produce other answers). It seems like the big challenge is—as Kuipers points out in the discussion—is determining what the constraints on the system are in the first place. This is somewhat related to the question of how you set up a physical simulation in the first place (i.e. object positions, velocities), though in this case the choices that need to be made are the functional constraints, rather than specifics about the object. Certaintly in qualitative simulation it seems that it is easier to specify information about the state of the object: the hard part is in determining what information to specify about the function.&lt;/p&gt;

&lt;p&gt;Also, this seems to apply specifically to continuous systems than can be described by a differential equation, though, so I’m interested in understanding better how this approach works with things that are not continuous (for example, colliding objects) or situations where there are additional types of constraints, such as inequality or equality constraints. I expect some of the next readings will give me a better sense of this.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 04:25:40 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Kuipers1986.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Kuipers1986.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Embodied language: a review of the role of the motor system in language comprehension</title>
        <description>&lt;p&gt;&lt;span id=&quot;Fischer2008&quot;&gt;Fischer, M. H., &amp;amp; Zwaan, R. A. (2008). Embodied language: a review of the role of the motor system in language comprehension. &lt;i&gt;Quarterly Journal Of Experimental Psychology&lt;/i&gt;, &lt;i&gt;61&lt;/i&gt;(6), 825–850. doi:10.1080/17470210701623605&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This is a review article by Fischer &amp;amp; Zwaan about whether and how the motor system is engaged in language comprehension. They are primarily interested in understanding how internal “simulations” by the motor system of an action affects, or is affected by, language describing the same action—a phenomena they term &lt;em&gt;motor resonance&lt;/em&gt;. They don’t give a precise definition of it, beyond that it is the “possibility that language comprehension may incorporate, and possibly even require as an essential component, some activity of the motor system that could be characterized as ‘motor resonance’” (pg. 826).&lt;/p&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan first review a set of theories regarding the relationship between perception and action. These include the &lt;em&gt;two-visual-pathways theory&lt;/em&gt; (which holds that there are two separate pathways in the brain, one for perception and one for action), the &lt;em&gt;theory of event coding&lt;/em&gt; or TEC (which holds that action and perception use the same underlying representations and thus compete for cognitive resources of those representations), &lt;em&gt;mirror neurons&lt;/em&gt; (which are neurons that are active when we engage in an action, and when we see others engaging in the same action), and &lt;em&gt;motor cognition&lt;/em&gt; (which is based on the idea that “we simulate our own as well as other people’s behavior as part of understanding it”, pg. 831).&lt;/p&gt;

&lt;p&gt;Next, Fischer &amp;amp; Zwaan discuss what mechanisms are involved in action simulation. The first mechanism they discuss is computation of affordances, i.e., that “the motor system spontaneously uses object information to compute possible actions in the light of one’s current posture and to select favourable responses” (pg. 831). Similarly, they discuss evidence that planning an action to a particular location can facilitate perception in that area. The second mechanism they discuss is motor resonance during action observation, which seems to be the idea that people use their motor system to make predictions about the actions of others during observation of those actions. The third mechanism is the time course of motor simulation, which they seem to suggest is involved specifically with forward prediction (and therefore facilitates judgements that involve a prediction, as opposed to a reverse inference or something unrelated). However, Fischer &amp;amp; Zwaan also state that “viewing the result of an action activates the processes that would bring about that result”, which is really an inference, not a forward simulation.&lt;/p&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan now turn to discussing how the motor system is involved in actual language comprehension. They make the distinction between &lt;em&gt;communicative motor resonance&lt;/em&gt; (i.e., motor simulation of speech production) versus &lt;em&gt;referential motor resonance&lt;/em&gt; (i.e., motor simulation of the action described by the language). They discuss evidence for communicative motor resonance in lower level phonological processing, and evidence for referential motor resonance in lexical access (i.e. the semantic meaning of individual words) as well as full sentence comprehension (in which information from multiple words must be integrated) and also discourse comprehension (which is, as they note, more ecologically valid as it is how we interpret language in real-world contexts).&lt;/p&gt;

&lt;p&gt;They close with three questions. First, the &lt;em&gt;association question&lt;/em&gt; is whether activation of the motor system co-occurs when performing a cognitive task (they state that they think the evidence reviewed in this article supports an answer of “yes” to this question). Second, the &lt;em&gt;necessity question&lt;/em&gt; is whether the activation of the motor system is required for language comprehension. Third, the &lt;em&gt;sufficiency question&lt;/em&gt; is whether activation of the motor system is sufficient for language comprehension.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan seem to use the term “simulation” here to mean neural activation (in the mirror neuron sense) of the same motor regions that are used for actually executing actions. This view, to me, isn’t that useful, though—talking about simulation as &lt;em&gt;neural&lt;/em&gt; simulation is either tautological or unrealistic, depending on how you view it. On the one hand, if we recognize actions in other people as the same types of actions we ourselves make, it would be incredibly surprising if there were no shared neural activity. On the other hand, if it is more than just a shared representation and is actually involved in &lt;em&gt;producing&lt;/em&gt; the action, then it doesn’t make much sense why motor simulation wouldn’t also produce the action.&lt;/p&gt;

&lt;p&gt;I don’t doubt that some sort of simulation that unifies perception, action, and cognition is involved in language comprehension, but talking about it at the level of neural simulation seems like the wrong level of abstraction. For me, it’s more helpful to think of it in terms of something like a mental model, a la &lt;a href=&quot;/quals/embodied%20language/2016/01/03/Matlock2004.html&quot;&gt;Matlock&lt;/a&gt;, &lt;a href=&quot;/quals/embodied%20language/2016/01/03/Bergen2007.html&quot;&gt;Bergen et al.&lt;/a&gt;, or &lt;a href=&quot;/quals/mental%20imagery/2016/01/01/Grush2004.html&quot;&gt;Grush&lt;/a&gt;. I don’t think these accounts are at odds with one another: you can still discuss motor simulation at the level of a mental model or emulator, but you don’t need to get into the tricky business of trying to interpret what it &lt;em&gt;actually&lt;/em&gt; means to observe correlations of neural activity.&lt;/p&gt;

&lt;p&gt;I do like the distinction that Fischer &amp;amp; Zwaan make between communicative and referential motor resonance. I would say that communicative motor resonance has more to do with perception—inferring the underlying motor process that is producing the auditory sensations. On the other hand, the referential motor resonance is more about cognition—constructing a mental simulation of percepts and actions for the purposes of comprehension and interpretation. (I am purposefully not using the terms “embodied” or “grounded” here: while I think it is incredibly useful for the mind to construct models of how perception and action work, I don’t think it is a requirement for &lt;em&gt;all&lt;/em&gt; of cognition to ground out in the real world).&lt;/p&gt;

&lt;p&gt;Overall, I’m a bit dissatisfied with the account in this article: the overarching hypothesis is that “motor simulation is required for language comprehension” but Fischer &amp;amp; Zwaan don’t really make an attempt to explain &lt;em&gt;why&lt;/em&gt; that’s the case. Simply imitating someone else’s action if you don’t know what the action is &lt;em&gt;for&lt;/em&gt; doesn’t help you understand it, and I think the same is true for the motor simulation account as well. I agree motor simulation is real, and believe it is involved in comprehension, but I want an account for why it helps and what it brings to the table. The argument that I’d make is that we construct mental models/simulations from which we can make predictions, inferences, etc., and taking actions within the model (i.e. imagining those actions) is the way in which we manipulate the model or test out our predictions. It’s not simply an imitation of the motor action, but a way of identifying and choosing hypotheses about what information a sentence is conveying.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 15:11:01 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Fischer2008.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Fischer2008.html</guid>
        
        
        <category>Embodied language</category>
        
      </item>
    
      <item>
        <title>Spatial and linguistic aspects of visual imagery in sentence comprehension</title>
        <description>&lt;p&gt;&lt;span id=&quot;Bergen2007&quot;&gt;Bergen, B. K., Lindsay, S., Matlock, T., &amp;amp; Narayanan, S. (2007). Spatial and linguistic aspects of visual imagery in sentence comprehension. &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;31&lt;/i&gt;(5), 733–64. doi:10.1080/03640210701530748&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Bergen et al. examine some of the ways that language triggers the use of visual mental imagery. In particular, they look at the Perky effect, which they describe as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In a seminal study, Perky (1910) asked participants to imagine seeing an object (such as a banana or leaf) while they were looking at a blank screen. At the same time, unbeknownst to them, an actual image of the same object was projected on the screen, starting below the threshold for conscious perception, but with progressively greater and greater illumination. Perky found that many participants continued to believe that they were still just imagining the stimulus and failed to recognize that there was actually a real, projected image even at levels where the projected image was perfectly perceptible to participants not simultaneously performing imagery. (pg. 736)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The Perky effect can be explained based on competition for the same set of resources: visual perception uses some of the same resources as mental imagery, and so if mental imagery is engaged, it can interfere with our ability to perceive things. This paradigm can be extended to measure how mental imagery is being used in language comprehension, by having people listen to a sentence just before performing a visual perception task.&lt;/p&gt;

&lt;p&gt;Using the above paradigm, Bergen et al. find that nouns and verbs that are associated with concrete motion or spatial locations (e.g. as in &lt;em&gt;The ceiling shook&lt;/em&gt; or &lt;em&gt;The ball dropped&lt;/em&gt;) do invoke mental imagery in the relevant location of the visual field. In contraast, metaphorical motion does not produce this effect (e.g. &lt;em&gt;The market fell&lt;/em&gt;), nor do abstract verbs that are associated with up/down but which do not literally mean up/down (e.g. &lt;em&gt;The quantity dwindled&lt;/em&gt;). Their conclusion is that concrete sentences evoke visual mental imagery, but metaphorical and abstract sentences—even if they do involve some form of motion of spatial location—do not engage imagery (or at least not in the same way).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In all experiments, the task was to listen to a sentence, after which a circle or square would appear in one portion of the screen (up, down, left, right). Participants had to judge whether the object was a circle or square. The hypothesis was that if mental imagery was being used in a particular portion of the visual field when the sentence was read, then it would take participants longer to judge the object if that object were in the same part of the visual field than if it was not.&lt;/p&gt;

&lt;p&gt;Experiment 1 looked at up/down verbs (e.g. &lt;em&gt;dropped&lt;/em&gt;, &lt;em&gt;rose&lt;/em&gt;). They did find that sentences with up verbs resulted in longer processing when the object was in the upper part of the screen than when it was in the lower part of the screen (5/5), and that sentences with down verbs resulted in longer proceessing when the object was in the lower part of the screen than when it was in the upper part of the screen (3/5).&lt;/p&gt;

&lt;p&gt;Experiment 2 looked at up/down nouns (e.g. &lt;em&gt;ceiling&lt;/em&gt;, &lt;em&gt;cellar&lt;/em&gt;). They found essentially the same results as before: sentences with up nouns resulted in longer processing times when the shape was in the upper part of the screen (3/5), and sentences with down nouns resulted in longer processing times when the shape was in the lower part of the screen (4/5).&lt;/p&gt;

&lt;p&gt;Experiment 3 looked at metaphorical motion (e.g. &lt;em&gt;The market fell&lt;/em&gt;), but did not find the effect either for metaphorical up verbs (2/5) or for metaphorical down verbs (2/5).&lt;/p&gt;

&lt;p&gt;Experiment 4 looked at abstract verbs with up/down connotations (e.g. &lt;em&gt;dwindled&lt;/em&gt;, &lt;em&gt;increased&lt;/em&gt;) and again did not find the effect either for abstract up verbs (3/5) or for abstract down verbs (2/5).&lt;/p&gt;

&lt;p&gt;Experiment 5 looked at axis (horizontal versus vertical) effects, rather than directional (up versus down) effects. They also did not find an effect here.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;First—wow, the Perky effect is awesome! I have heard variants of that before (e.g. it is hard to read and listen to language simultaneously) but I hadn’t heard of people actually conflating mental imagery and true perceptions.&lt;/p&gt;

&lt;p&gt;This paradigm used by Bergen et al. is really neat, though the fact that they didn’t find the expected effect for all of the individual sentences (even if they found it overall) is somewhat less compelling. It would be interesting to use eyetracking with these experiments to see if people are actually attending to the portion of the field that is hypothesized, particularly in the case of the abstract/metaphorical sentences—not finding saccades in those cases would strengthen their hypothesis that people are not actually constructing a concrete visual scene in the same way.&lt;/p&gt;

&lt;p&gt;On a different note, given that these types of sentences do seem to evoke mental imagery, it would be interesting to try to reverse &lt;a href=&quot;http://arxiv.org/abs/1411.4555&quot;&gt;some of&lt;/a&gt; the &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/deepimagesent/&quot;&gt;image captioning&lt;/a&gt; algorithms that are coming out of the machine learning literature in order to produce a type of simulated mental imagery, which perhaps could then augmented as more information comes in—so the first sentence might retrieve an image, which is then parsed into components, and as more sentences are heard, they modify the current scene to make it consistent with the linguistic description. Then, that scene could be used to do further reasoning about what is being said—e.g. verifying whether a description about it is true or not.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 11:17:02 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Bergen2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Bergen2007.html</guid>
        
        
        <category>Embodied language</category>
        
      </item>
    
      <item>
        <title>Fictive motion as cognitive simulation</title>
        <description>&lt;p&gt;&lt;span id=&quot;Matlock2004&quot;&gt;Matlock, T. (2004). Fictive motion as cognitive simulation. &lt;i&gt;Memory And Cognition&lt;/i&gt;, &lt;i&gt;32&lt;/i&gt;(8), 1389–1400. doi:10.3758/BF03206329&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Matlock asks whether mental simulation is involved in processing sentences that involve motion words but which do not actually describe literal motion (the term for this type of motion is &lt;em&gt;fictive motion&lt;/em&gt;). In a series of four quite elegant experiments, Matlock shows that people take longer to respond whether a sentence involving fictive motion was related to a story when the story described long/slow/difficult travel as opposed to short/fast/easy travel. The results suggest that people may be constructing a mental model of the scenario described by the story, and that then to understand the target sentence they simulate from this mental model. When the model actually involves motion that that is longer, slower, or more difficult, people accordingly run simulations that mimic those properties, and thus take a longer time to respond.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Experiment 1 used stories involving long-distance vs. short-distance travel. For example, the story might be about someone driving across the desert to visit a relative, and the desert is either large or small. The target sentence would then be something like “the road crosses the desert”. In the control study, they instead used target sentences like “the road is in the desert”. In this case they found participants took about 400ms longer to judge the target sentence in the long-distance stories.&lt;/p&gt;

&lt;p&gt;Experiment 2 used stories involving slow vs. fast motion. For example, the story might be about someone walking or running along a path. The target sentence would then be something like “the path follows the creek”. In the control study, they instead used target sentences like “the path is next to the creek”. In this case they found participants again took about 400ms longer to judge the target sentences in the slow-motion stories.&lt;/p&gt;

&lt;p&gt;Experiment 3 used stories involving difficult vs. easy terrain. For example, the story might be about someone driving along a coastline which is either jagged and hilly or straight and flat. The target sentence would then be something like “a road runs along the peninsula”. In the control study, they instead used target sentences like “there is a road along the peninsula”. In this case they found participants took about 350ms longer to judge the target sentences in the rough terrain stories.&lt;/p&gt;

&lt;p&gt;Experiment 4 used stories that didn’t actually involve motion at all—for example, a story describing an earthquake fault along terrain that is either difficult or easy. The target sentence would then be something like “an earthquake fault runs across the valley”. There was no control experiment here, but I would expect the results to be similar. They found participants took about 200ms longer to judge the target sentences in the difficult terrain stories.&lt;/p&gt;

&lt;p&gt;In all the experiments there were “filler” scenarios that did not involve fictive motion but did involve regular motion.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;It’s not surprising to me that people might use something like mental simulation to help them interpret sentences, but I am surprised that the motion words have as strong of an effect as they do: i.e., that “the road crosses the desert” takes longer to read for a long-distance scenario than a short-distance scenario, while “the road is in the desert” is read at pretty much the same speed. I’m really curious to think about how this type of mental simulation serves interpretation of language from a computational point of view. Is it something like: we construct this mental model, and then when asked if “the road crosses the desert” is true, we look at our mental model and verify (perhaps visually) that it does in fact cross the desert (which would then take longer for longer distances)? Whereas, for “the road is in the desert”, all that is required is to verify that the road is in the desert, which should take the same amount of time regardless of how big the desert is?&lt;/p&gt;

&lt;p&gt;I do wonder whether these results apply in the general case of interpreting language. For one, participants were explicitly told to imagine the scenario as they read it (and the first sentence of each story was of the form “imagine a road”). If they weren’t prompted to use imagery, would there still be an effect? Also, if the background story were described in much less detail, would there be an effect?&lt;/p&gt;

&lt;p&gt;I also wonder if this is an effect just of something like a priming effect, versus being explicitly about running a mental simulation. In particular, they didn’t test reaction times on target sentences that were irrelevant to the scenario. For example, if the sentence was “the river crosses the road” (when the story was about a road and didn’t mention a river), would people still take longer in the long-distance condition? If so, then that might point to something more like a priming effect that is engaged when the person reads a motion word like “crosses”, rather than them running a mental simulation (because there is no reason for a mental simulation about a river crossing a road to take longer, as it is irrelevant to the long-distance aspects of the story).&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 08:57:21 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Matlock2004.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Matlock2004.html</guid>
        
        
        <category>Embodied language</category>
        
      </item>
    
      <item>
        <title>On discriminative vs. generative classifiers: a comparison of logistic regression and naive Bayes</title>
        <description>&lt;p&gt;&lt;span id=&quot;Ng2002&quot;&gt;Ng, A. Y., &amp;amp; Jordan, M. I. (2002). On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes. &lt;i&gt;Advances In Neural Information Processing Systems&lt;/i&gt;, &lt;i&gt;14&lt;/i&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Ng &amp;amp; Jordan give a nice analysis of generative vs. discriminative classifiers, and show that there are actually two modes where either type of model might be preferred. They focus in particular on logistic regression vs. naive Bayes, but say that their analysis should be extendable to other generative-discriminative pairs of models.&lt;/p&gt;

&lt;p&gt;There are two key results in their analysis:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The asymptotic error of a generative linear classifier is greater than the asymptotic error of a discriminative linear classifier (which converges to the best linear classifier overall).&lt;/li&gt;
  &lt;li&gt;The number of training examples required for a discriminative linear classifier to reach its asymptotic error is $O(n)$, where $n$ is the &lt;a href=&quot;https://en.wikipedia.org/wiki/VC_dimension&quot;&gt;VC dimension&lt;/a&gt;. In contrast, the number of examples for a generative linear classifier is $O(\log{n})$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They show these propositions to be empirically true across a number of experiments on the UCI datasets.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The results from Ng &amp;amp; Jordan suggest that generative classifiers may be more useful in situations where there are small amounts of data. However, if there is a lot of data available, it may be more useful to use a discriminative classifier because it will ultimately have less error. The intuition behind this (I think?) has to do with the fact that the generative classifier needs to make assumptions about the distributions for both the likelihood and the prior, while the discriminative model does not. In some cases, if the assumptions are correct, then the generative classifier should have the same asymptotic error as the discriminative classifier. If my intuition about this is correct, then the question is essentially: can you estimate how incorrect your assumptions are, and then use that estimate (combined with knowledge about how much data you have) to determine whether to train a generative vs. discriminative classifier?&lt;/p&gt;

&lt;p&gt;Of course, there are also other reasons to potentially use a generative model besides just faster asymptotic error. In many cases, it may be necessary to be able to invert the model (i.e. sometimes you may need $p(x\vert y)$, and sometimes you may need $p(y\vert x)$). Intuitively, it seems like if you need to do this, it is going to be more efficient to train a generative model (from which you can compute both $p(y\vert x)$ and $p(x\vert y)$) rather than training multiple discriminative models.&lt;/p&gt;

&lt;p&gt;One question I have about this is regarding training discriminative models with generative models—if you learn a generative model first, and then use samples from it to train a discriminative model, how does that affect the error of the discriminative model? Can the discriminative model only do as well as the generative model, in that case? I want to say the answer is yes, but perhaps combined with true data from the world (in addition to samples from the generative model) the discriminative model could eventually achieve a lower error.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 07:45:06 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/03/Ng2002.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/03/Ng2002.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</title>
        <description>&lt;p&gt;&lt;span id=&quot;Clark2013&quot;&gt;Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. &lt;i&gt;The Behavioral And Brain Sciences&lt;/i&gt;, &lt;i&gt;36&lt;/i&gt;(3), 181–204. doi:10.1017/S0140525X12000477&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this BBS article, Clark lays out a grand unified theory of cognition based on the idea that our brains are predictive machines. At the high levels of processing, our brains construct hypotheses/predictions of our percepts, and at the low levels of processing, our sensory systems compute error signals between what is predicted and what is actually sensed. He begins with the following assumption about what the brain is trying to accomplish:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How, simply on the basis of patterns and changes in its own internal states, is [the brain] to alter and adapt its resources so as to tune itself to act as a useful node… for the origination of adaptive responses? Notice how different this conception is to ones in which the problem is posed as one of establishing a mapping relation between environmental and inner states. The task is not to find such a mapping but to infer the nature of the signal source (the world) from just the varying input signal itself. (pg. 183)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clark next defines his overarching theory as that of “action-oriented predictive processing”, and quotes from Hawkins &amp;amp; Blakeslee (2004):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As strange as it sounds, when your own behavior is involved, your predictions not only precede sensation, they determine sensation. Thinking of going to the next pattern in a sequence causes a cascading prediction of what you should experience next. As the cascading predition unfolds, it generates the motor commands necessary to fulfil the prediction. Thinking, predicting, and doing are all part of the same unfolding of sequences moving down the cortical hierarchy. (pg. 186)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, action is just a way to bring the world into alignment with the brain’s predictions.&lt;/p&gt;

&lt;p&gt;Clark goes on to describe a large body of work that supports these views, particularly from the realm of neuroscience and perceptual science. He also explains how the action-oriented predictive processing scheme unifies the levels of analysis, in that it is a neural implementation of what are actually generative Bayesian models.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I agree with a lot of what Clark says in this article, though there are a few points that I disagree with.&lt;/p&gt;

&lt;p&gt;First, “predictive” is &lt;em&gt;not&lt;/em&gt; the same as “generative” (though all generative models are predictive, in a sense). Discriminative models are also predictive, in the sense that you receive an input $x$ and predict an output $y$. Generative models, by contrast, can jointly predict $x$ and $y$ simultaneously, making it possible to predict $y$ from $x$ but also the other way around (predicting $x$ from $y$). Thus, generative models have more predictive power in the sense that they can be used to predict more relationships between the data, but that doesn’t mean discriminative models aren’t predictive. Clark uses “predictive” and “generative” interchangeably, but I think that is a mistake.&lt;/p&gt;

&lt;p&gt;I definitely agree that having a generative model is incredibly powerful because it allows us to make &lt;em&gt;a priori&lt;/em&gt; predictions in the absence of data. However, generative models can be more difficult to learn, and while they may be overall more general, that may come with the cost of a loss of precision for specific prediction tasks. Thus, discriminative models are important, too, and it is almost certainly incorrect to say that the brain never makes use of simple mappings.&lt;/p&gt;

&lt;p&gt;I’m not sure I agree with Clark’s characterization of action. I do like the idea that our minds make predictions, and then we use action to make those predictions come true, but I think it is important to make a distinction between predictions of the generative process in the world (i.e., what is going to happen next in the absence of action) versus predictions of how the default process might change &lt;em&gt;if&lt;/em&gt; we were to act on it.&lt;/p&gt;

&lt;p&gt;I am also not sure if I agree with the idea that the brain is trying to reduce &lt;em&gt;error&lt;/em&gt;, especially since “error” is not a well-defined term. Clark seems to use it in the sense of reducing entropy, but there’s a lot of other ways it could be used. By definition, any type of learning system is trying to optimize some objective function—the “error”—so while technically true, this  isn’t really that new of a concept, so I don’t feel that it provides all that much explanatory power.&lt;/p&gt;

&lt;p&gt;The more important component of Clark’s argument is that of the large part generative models probably play. Yet, Clark doesn’t go into details about how those models are actually constructed, beyond referring to a few things like the Helmholtz machine. That is where I think the truly difficult problems lie—determining &lt;em&gt;what&lt;/em&gt; the generative models are, &lt;em&gt;how&lt;/em&gt; we construct them in the first place, and &lt;em&gt;when&lt;/em&gt; a generative model is the thing that’s constructed as opposed to a discriminative model. There are particularly difficult questions to be answered in domains where the models must be able to handle incredibly high-dimensional data and parse it into complex, structured models. Saying the brain constructs generative models is a useful starting point (and it’s a starting point that I agree with), but it really doesn’t say anything about what the actual structure of those models is.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 06:05:44 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/03/Clark2013.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/03/Clark2013.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>The role of generative knowledge in object perception</title>
        <description>&lt;p&gt;&lt;span id=&quot;Battaglia2012&quot;&gt;Battaglia, P. W., Kersten, D., &amp;amp; Schrater, P. R. (2012). The Role of Generative Knowledge in Object Perception. In J. Trommershauser, K. P. Körding, &amp;amp; M. S. Landy (Eds.), &lt;i&gt;Sensory Cue Integration&lt;/i&gt;. Oxford University Press.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this chapter, Battaglia et al. describe the differing roles of the &lt;em&gt;generative process&lt;/em&gt;, people’s &lt;em&gt;generative knowledge&lt;/em&gt;, and how they related to perception (particularly for object perception). They first define several challenges/observations regarding perception:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The mapping from the world to our senses is often not invertible (e.g. recovering 3D shape from 2D data)&lt;/li&gt;
  &lt;li&gt;Many sensory cues are not actually measurements of the relevant property we’re interested in, but provide only “auxiliary” information&lt;/li&gt;
  &lt;li&gt;Sensory cues vary in quality relative to each other, depending on external and internal factors (e.g. fog, cataracts), and as a function of the world state.&lt;/li&gt;
  &lt;li&gt;Objects’ spatial and material properties follow highly predictable patterns&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Next, they define the &lt;em&gt;sensory generative process&lt;/em&gt; to be the true process in the world that generates our sensations. In contrast, &lt;em&gt;sensory generative knowledge&lt;/em&gt; is people’s assumptions about how the sensory generative process works. In some cases, the generative process and generative knowledge may be the same, though not necessarily. If generative knowledge is at least a good approximation to the generative process, though, it can provide important information about how to interpret our sensations (addressing the challenges listed previously). Battaglia et al. make the distinction between being &lt;em&gt;subjectively optimal&lt;/em&gt; (in which people make optimal use of their generative knowledge, but the knowledge does not match the generative process), &lt;em&gt;objectively optimal&lt;/em&gt; (in which people make optimal use of their generative knowledge, which matches the generative process), and &lt;em&gt;suboptimal&lt;/em&gt; (in which people do not make optimal use of their generative knowledge).&lt;/p&gt;

&lt;p&gt;Battaglia et al. also describe how a Bayesian observer model can account for several basic phenomena in perception: performing basic Bayesian inference, combining multiple cues, discounting nuisance information based on prior knowledge, and explaining away nuisance information based on auxiliary cues. In particular, discounting and explaining away rely heavily on generative knowledge, while basic Bayes and cue combination could theoretically be learned just via a discriminative mapping.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This chapter makes two important points:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Our internal models aren’t necessarily accurate imitations of the true generative process, but that doesn’t mean that people don’t make optimal use of the knowledge they have. Hence, when constructing models of cognition, it is important to be explicit about what the generative process is in the world that is generating people’s observations, and what generative knowledge we as scientists think people actually have.&lt;/li&gt;
  &lt;li&gt;Having structured generative knowledge is important, because it makes it easier to interpret ambiguous sensations into reliable perceptions. For simpler things, such as cue combination, we don’t necessarily need to have a full generative model since a simple discriminative model can often be sufficient. But, for reasoning about more complex systems, and to be able to explain phenomena like discounting or explaining away, generative knowledge is crucially important.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 14:18:13 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/02/Battaglia2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/02/Battaglia2012.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>The Helmholtz machine</title>
        <description>&lt;p&gt;&lt;span id=&quot;Dayan1995&quot;&gt;Dayan, P., Hinton, G. E., Neal, R. M., &amp;amp; Zemel, R. S. (1995). The Helmholtz machine. &lt;i&gt;Neural Computation&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;(5), 889–904. doi:10.1162/neco.1995.7.5.889&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Dayan et al. propose a method for learning about the underlying structure in data using self-supervised learning in a neural network. Specifically, they construct the network to have bottom-up &lt;em&gt;recognition&lt;/em&gt; weights and top-down &lt;em&gt;generative&lt;/em&gt; weights. The network is then trained according to a &lt;em&gt;wake-sleep algorithm&lt;/em&gt;, where the generative weights are trained during the “wake” phase and the recognition weights are trained during the “sleep” phase by simulating training examples from the generative model.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;The recognition probability of unit $j$ in layer $\ell$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_j^\ell(\phi,\mathbf{s}^{\ell-1})=\sigma\left(\sum_i s_i^{\ell-1}\phi_{i,j}^{\ell-1,\ell}\right)&lt;/script&gt;

&lt;p&gt;where $\sigma$ is the sigmoid function and $\phi$ are the recognition weights. As mentioned earlier, the recognition weights are trained during the sleep phase by simulating training data from the generative model.&lt;/p&gt;

&lt;p&gt;The generative probability of unit $j$ in layer $\ell$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_j^\ell(\theta,\mathbf{s}^{\ell+1})=\sigma\left(\sum_i s_i^{\ell+1}\theta_{i,j}^{\ell+1,\ell}\right)&lt;/script&gt;

&lt;p&gt;where $s$ are the unit activities. The generative model is trained by presenting data to input units and then activating units according to $q_j^\ell$. Then, the generative weights are updated to minimized the KL-divergence between the actual activations and the generative probabilities $p_j^\ell$.&lt;/p&gt;

&lt;p&gt;The wake and sleep phases are computed iteratively and over time the weights should converge such that $Q(d)=P(\theta,d)$.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The Helmholtz machine is a very cool idea, in that it is an unsupervised way of (approximately) learning highly complex structures by jointly training a recognition and generative model. This can be thought of as a way of implementing the EM algorithm in a neural network; it can also be interpreted as a particular type of autoencoder (e.g. if it is a one-layer Helmholtz machine, it is a folded-over autoencoder).&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 10:25:21 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/02/Dayan1995.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/02/Dayan1995.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>Learning compound multi-step controllers under unknown dynamics</title>
        <description>&lt;p&gt;&lt;span id=&quot;Han2015&quot;&gt;Han, W., Levine, S., &amp;amp; Abbeel, P. (2015). Learning Compound Multi-Step Controllers under Unknown Dynamics. &lt;i&gt;Proceedings Of the 28th IEEE/RSJ International Conference on Intelligent Robots and Systems&lt;/i&gt;. Retrieved from http://rll.berkeley.edu/reset_controller/reset_controller.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Han et al. build on the work of &lt;a href=&quot;/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Levine2015.html&quot;&gt;Levine et al.&lt;/a&gt; to learn controllers for tasks involving multiple steps. This is a difficult problem for traditional approaches to control, which assume that tasks are stationary (i.e., that the initial conditions are the same), which isn’t usually the case for compound tasks because the initial state of each step depends on the end state of the previous step. To address this, Han et al. formulate a way to learn both forward and &lt;em&gt;reverse&lt;/em&gt; controllers simultaneously. Then, in the compound task, if a forward controller fails, the reverse controller can be used to reset the movement and try again. Training compound controllers in this way is much more effective than trying to learn one global controller.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Training the forward and reset controllers is actually pretty straightfoward. Han et al. run the forward controller and reset controllers in sequence $N$ times, and use these samples to update both controllers according to the task-specific loss function (for the forward controller) or a reset cost (for the reset controller).&lt;/p&gt;

&lt;p&gt;Training the compound controller follows largely the same idea. Each forward controller is executed in sequence. If a controller fails, then the corresponding reverse controller is used to undo the action, and the forward controller is run again. This is repeated until the controller succeeds. After $N$ samples are collected for a controller, it is refit according to those samples. These $N$ samples count as one “iteration”, and all controllers are trained for $K$ iterations.&lt;/p&gt;

&lt;p&gt;As in the previous work, the individual linear-Gaussian controllers can also be used to train a more general parameterized policy (such as a neural network). The benefit of training both forward and reset controllers, though, means that the system can autonomously train the neural network by running the forward controllers, using those samples as training data for the network, and resetting automatically using the reset controller.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This way of training compound controllers is exciting, because it could potentially be integrated with higher-level planning algorithms to determine how to accomplish more sophisticated and complex tasks. This brings up an interesting distinction in the idea of simulation—that you can potentially have simulations at different levels of abstraction, as well as at different levels of granularity. In this case, the robot can learn a dynamics model for multiple motion primitives (the individual linear-Gaussian controllers) but it might also need to be able to learn a higher-level (perhaps more qualitative) form of simulation in order to reason about how to accomplish the task in the first place. In the case of screwing in a bolt, the robot might need high-level qualitative knowledge about how the task works (first need to pick up the wrench, then bring it to the bolt, then turn it because bolts need to be turned to go further into the hole, then repeat, etc.), but as shown by this work, it also needs low-level knowledge about the dynamics of the task in order to actually execute the subparts of the action.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 08:22:37 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Han2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Han2015.html</guid>
        
        
        <category>Planning under uncertain dynamics</category>
        
      </item>
    
  </channel>
</rss>
