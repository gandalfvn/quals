<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 14 Jan 2016 10:52:36 -0800</pubDate>
    <lastBuildDate>Thu, 14 Jan 2016 10:52:36 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Simulation of clothing with folds and wrinkles</title>
        <description>&lt;p&gt;&lt;span id=&quot;Bridson2003&quot;&gt;Bridson, R., Marino, S., &amp;amp; Fedkiw, R. (2003). Simulation of clothing with folds and wrinkles. &lt;i&gt;Proceedings Of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation&lt;/i&gt;, 28–36. Retrieved from http://dl.acm.org/citation.cfm?id=846281&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Bridson et al. propose a method for simulating clothing while preserving folds and wrinkles. They propose a mixed implicit/explicit time integration method, derive a physically accurate bending model, and handle interpenetrations in a way that preserves wrinkles.&lt;/p&gt;

&lt;p&gt;In the time integration method, they use an implicit method for the cloth’s velocity-dependent forces (damping forces) and explicit methods for velocity-independent forces (elastic forces). Because the implicit method introduces damping, it is appropriate for the damping forces; however, when applied to the elastic forces, it results in too much damping and doesn’t handle nonlinearities in the elastic forces well.&lt;/p&gt;

&lt;p&gt;The bending model considers the bend between two triangles in a mesh which share an edge. There are four vertices $x_i$ between the two triangles, each with an associated velocity $v_i$ and force $F_i$. The normals of the triangles are $n_1$ and $n_2$ and the angle between them is $\theta$. The velocities and forces make up a 12-dimensional space. Rather than directly using this space, Bridson et al. come up with a basis for the space which consists of: 3 rigid body translations, three rigid body rotations, two in-plane motions of vertex 1, two in-plane motions of vertex 2, one in-line stretching of edge 3-4, and the change in $\theta$ (the “bending mode”). The bending mode is orthogonal to all the other modes and is denoted $u=(u_1,u_2,u_3,u_4)$. Based on the condition of orthogonality, they derive values for each $u_i$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
u_1&amp;=\vert E\vert \frac{N_1}{\vert N_1\vert^2}\\
u_2&amp;=\vert E\vert \frac{N_2}{\vert N_2\vert^2}\\
u_3&amp;=\frac{(x_1-x_4)\cdot{}E}{\vert E\vert}\frac{N_1}{\vert N_1\vert^2}+\frac{(x_2-x_4)\cdot{}E}{\vert E\vert}\frac{N_2}{\vert N_2 \vert^2}\\
u_4&amp;=\frac{(x_1-x_3)\cdot{}E}{\vert E\vert}\frac{N_1}{\vert N_1\vert^2}+\frac{(x_2-x_3)\cdot{}E}{\vert E\vert}\frac{N_2}{\vert N_2 \vert^2}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $N_1=(x_1-x_3)\times (x_1-x_4)$ and $N_2=(x_2-x_4)\times(x_2-x_3)$ are the area weighted normals and $E=x_4-x_3$ is the common edge. Because $u$ is orthogonal to all the other modes of motion, the forces related to bending—the elastic bending force and the damping bending force—must be proportional to $u$ in order to preserve orthogonality. For the elastic force they use:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_i^e=k^e\frac{\vert E\vert^2}{\vert N_1\vert+\vert N_2\vert}\left(\sin(\theta/2)-\sin(\theta_0/2)\right)u_i&lt;/script&gt;

&lt;p&gt;where $k^e$ is a material property (the elastic bending stiffness) and $\theta_0$ is the &lt;em&gt;rest angle&lt;/em&gt;, which can be used to enforce that the cloth bends in a particular way. For the damping force they use:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_i^d=-k^d\vert E\vert (d\theta/dt)u_i&lt;/script&gt;

&lt;p&gt;where $k^d$ is a material property.&lt;/p&gt;

&lt;p&gt;To resolve interpentrations, rather than projecting the interpenetrating material onto the surface of the mesh (which causes wrinkles to be flattened out), they project the interpenetrating material into an interval $[0,\tau]$. This means that wrinkles may not be as peaked as they were (as they may only be peaked up to a height of $\tau$), but this will preserve the contours of the folds at least.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Jan 2016 02:09:22 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physically-based%20animation/2016/01/14/Bridson2003.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physically-based%20animation/2016/01/14/Bridson2003.html</guid>
        
        
        <category>Physically-based animation</category>
        
      </item>
    
      <item>
        <title>Nonconvex rigid bodies with stacking</title>
        <description>&lt;p&gt;&lt;span id=&quot;Guendelman2003&quot;&gt;Guendelman, E., Bridson, R., &amp;amp; Fedkiw, R. (2003). Nonconvex Rigid Bodies with Stacking. &lt;i&gt;ACM Transactions On Graphics&lt;/i&gt;, &lt;i&gt;22&lt;/i&gt;(3). doi:10.1145/882262.882358&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Guendelman et al. propose a method for detecting and resolving collisions when dealing with nonconvex rigid bodies, particularly in scenarios when those bodies need to stack. There are three main pieces to their approach: the geometric representation, the time integration method, and a &lt;em&gt;shock propagation&lt;/em&gt; method for propagation-based contact resolution.&lt;/p&gt;

&lt;p&gt;First, the geometric representation that they use is the typical triangular mesh, plus a &lt;em&gt;signed distance function&lt;/em&gt;, which gives the distance to the (closest) surface of the mesh along the normal vector. Distances inside the mesh are negative, and distances outside the mesh are positive. Such a distance function makes it easy to detect collisions because for a given vertex, you can check what the distance is relative to another object. If the distance is negative, then you know that the objects are interpenetrating.&lt;/p&gt;

&lt;p&gt;Second, the time integration method that they use has four components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Detect and resolve collisions&lt;/li&gt;
  &lt;li&gt;Update object velocities&lt;/li&gt;
  &lt;li&gt;Detect and resolve contacts&lt;/li&gt;
  &lt;li&gt;Update object positions&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By performing the steps in this order, this method avoids issues with undesirable behavior when friction is high (and therefore objects shouldn’t move). They give the example of a block on an inclined plane:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Consider a block sitting still on an inclined plane with a large coefficient of restitution, say $\epsilon=1$, and suppose that friction is large enough that the block should sit still. In a standard time stepping scheme, both position and velocity are updated first, followed by collision and contact resolution. During the position and velocity update, the block starts to fall under the effects of gravity. Then in the collision processing stage we detect a low velocity collision between the block and the plane, and since $\epsilon=1$ the block will change direction and bounce upwards at an angle down the incline. Then in the contact resolution stage, the block and the plane are separating so nothing happens. The block will eventually fall back to the inclined plane, and continue bouncing up and down incorrectly sliding down the inclined plane because of the time it spends in the ballistic phase.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They use a propagation method for resolving collisions, in which they predict where the objects will be, temporarily move them there, and then check for collisions and process them. This might result in new collisions, so they repeat the process a number of times (5). The method for resolving contacts is similar, except that they additionally use a &lt;em&gt;contact graph&lt;/em&gt; which reflects which objects are resting on which. They separate the graph into levels such that the objects in level $i$ are lower than the objects in level $i+1$, and then process the levels sequentially.&lt;/p&gt;

&lt;p&gt;Finally, after several iterations of contact resolution, they use a method called &lt;em&gt;shock propagation&lt;/em&gt;. In this method, they process the contacts at level $i$ and then temporarily fix the positions of those objects (zero the velocity and give them infinite mass). Then, when they process the contacts at level $i+1$, those objects cannot affect the objects at level $i$ and so therefore the contact resolution is propagated upward.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jan 2016 14:56:20 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physically-based%20animation/2016/01/13/Guendelman2003.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physically-based%20animation/2016/01/13/Guendelman2003.html</guid>
        
        
        <category>Physically-based animation</category>
        
      </item>
    
      <item>
        <title>Stable real-time deformations</title>
        <description>&lt;p&gt;&lt;span id=&quot;Muller2002&quot;&gt;Müller, M., Dorsey, J., &amp;amp; McMillan, L. (2002). Stable Real-time Deformations. &lt;i&gt;Proceedings Of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation&lt;/i&gt;. doi:10.1145/545261.545269&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Müller et al. introduce a stable method for deforming objects without increasing the volume of the object.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Finite Element Method&lt;/em&gt; gives a function $F$ that converts deformations into elastic forces:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{f}_\mathrm{elastic}=F(\mathbf{x}-\mathbf{x}_0)&lt;/script&gt;

&lt;p&gt;where $\mathbf{x}$ is the deformed location of the element and $\mathbf{x}_0$ is the original location of the element. The Jacobian of this matrix evaluated at $\mathbf{x}_0$ is called the &lt;em&gt;stiffness matrix&lt;/em&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K=\frac{\partial F}{\partial\mathbf{x}}(\mathbf{x}_0)&lt;/script&gt;

&lt;p&gt;In the linear case, $F$ is approximated by a first-order approximation, giving&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{f}_\mathrm{elastic}=K\cdot(\mathbf{x}-\mathbf{x}_0)&lt;/script&gt;

&lt;p&gt;However, this assumes that the transformation between $\mathbf{x}_0$ and $\mathbf{x}$ is linear (i.e., no rotations). If this assumption is violated, because linear elastic forces are invariant under translations but not rotations, the resulting simulation will over-stretch.&lt;/p&gt;

&lt;p&gt;To avoid this, Müller compute the forces in the original (unrotated) coordinate frame and then transform them back into the rotated coordinate frame:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{f}_\mathrm{elastic}=R_eK\cdot{}(R_e^{-1}\mathbf{x}-\mathbf{x}_0)&lt;/script&gt;

&lt;p&gt;where $R_e$ is a block matrix with the gobal rotation matrix $R_x$ repeated four times along the diagonal and zeros everywhere else (repeated once for each vertex). From this, they compute the force for the whole mesh by &lt;em&gt;warping&lt;/em&gt; the stiffness matrix according to the rotation of each vertex. The force at vertex $i$ is then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{f}_i=R_i\sum_{j=1}^n k_{ij}(R_i^{-1}\mathbf{x}_j-\mathbf{x}_{0j})&lt;/script&gt;

&lt;p&gt;where $R_i$ is the rotation matrix for vertex $i$. However, these &lt;em&gt;local&lt;/em&gt; rotations aren’t known (the global rigid-body rotation is known, but we want to know the local rotations which include the effect of deformation). Given the vectors for the original mesh ($\mathbf{u}$), and the vectors of the deformed/rotated mesh ($\mathbf{v}$), they compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{ij}=\sum_{k=1}^N (\mathbf{n}_i\cdot{}\mathbf{u}_k)(\mathbf{n}_j\cdot{}\mathbf{v}_k)&lt;/script&gt;

&lt;p&gt;where $\mathbf{n}_1$, $\mathbf{n}_2$, and $\mathbf{n}_3$ are the basis vectors of $\mathbb{R}^3$. Given $F$, they decompose it using SVD to get $F=USV^\top$, and then compute the rotation matrix as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R=VU^\top&lt;/script&gt;
</description>
        <pubDate>Wed, 13 Jan 2016 11:30:33 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physically-based%20animation/2016/01/13/Mueller2002.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physically-based%20animation/2016/01/13/Mueller2002.html</guid>
        
        
        <category>Physically-based animation</category>
        
      </item>
    
      <item>
        <title>Stable fluids</title>
        <description>&lt;p&gt;&lt;span id=&quot;Stam1999&quot;&gt;Stam, J. (1999). Stable Fluids. &lt;i&gt;Proceedings Of the 26th Annual Conference on Computer Graphics and Interactive Techniques&lt;/i&gt;. doi:10.1145/311535.311548&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces the first stable fluid simulation algorithm for computer graphics. This is a hard problem, because there is not analytic solution to Navier-Stokes. Previous methods had used explicit approximations such as Eulerian methods; however, these types of methods can become unstable and “blow up” for large timesteps. Using small enough timesteps is not a feasible option in computer graphics, in which the simulation needs to run in real time, thus prohibiting small timesteps.&lt;/p&gt;

&lt;p&gt;This algorithm is specific to contained fluids (i.e., gases) and approximates the following form of Navier-Stokes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \mathbf{u}}{\partial t}=\mathbf{P}\left(-(\mathbf{u}\cdot{}\nabla)\mathbf{u}+\nu\nabla^2\mathbf{u}+\mathbf{f}\right)&lt;/script&gt;

&lt;p&gt;where $\nabla\cdot{}\mathbf{u}=0$. Here, $\mathbf{u}$ is the velocity field, $\nabla$ is a vector of spatial partial derivatives, $\nu$ is the kinematic viscosity of the fluid, $\mathbf{f}$ is an external force, and $\mathbf{P}$ is a projection operator which projects any vector field onto its &lt;em&gt;divergence free&lt;/em&gt; part. Divergence free essentially means that at any point in the vector field, the flow towards that point is the same as the flow away from it.&lt;/p&gt;

&lt;p&gt;They compute the approximate solution to this equation in four steps. First, $\mathbf{w}_0$ is the solution from the previous time step:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}_0(\mathbf{x})=\mathbf{u}(\mathbf{x},t)&lt;/script&gt;

&lt;p&gt;Then, the first step is to &lt;em&gt;add force&lt;/em&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}_1(\mathbf{x})=\mathbf{w}_0+\Delta t\ \mathbf{f}(\mathbf{x},t)&lt;/script&gt;

&lt;p&gt;The second step is &lt;em&gt;advection&lt;/em&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}_2(\mathbf{x})=\mathbf{w}_1(\mathbf{p}(\mathbf{x},-\Delta t))&lt;/script&gt;

&lt;p&gt;where $\mathbf{p}(\mathbf{x},-\Delta t)$ says that the new velocity at $\mathbf{x}$ is the velocity at a location backwards in time by $-\Delta t$ (i.e., something like the velocity at the location given by $\dot{\mathbf{x}}\ \Delta t)$.&lt;/p&gt;

&lt;p&gt;The third step is &lt;em&gt;diffusion&lt;/em&gt;, in which the effects of viscosity are applied and solved as a linear system:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(\mathbf{I}-\nu\Delta t\nabla^2\right)\mathbf{w}_3(\mathbf{x})=\mathbf{w}_2(\mathbf{x})&lt;/script&gt;

&lt;p&gt;The fourth and final step is &lt;em&gt;projection&lt;/em&gt;, in which the system is projected back onto its divergence free part.&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 07:53:39 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physically-based%20animation/2016/01/12/Stam1999.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physically-based%20animation/2016/01/12/Stam1999.html</guid>
        
        
        <category>Physically-based animation</category>
        
      </item>
    
      <item>
        <title>Rational use of cognitive resources: levels of analysis between the computational and the algorithmic</title>
        <description>&lt;p&gt;&lt;span id=&quot;Griffiths2015&quot;&gt;Griffiths, T. L., Lieder, F., &amp;amp; Goodman, N. D. (2015). Rational Use of Cognitive Resources: Levels of Analysis Between the Computational and the Algorithmic. &lt;i&gt;Topics In Cognitive Science&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;(2), 217–229. doi:10.1111/tops.12142&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Griffiths et al. propose a focus on levels between the computational and algorithmic. They outline a method for approaching analysis at these intermediate levels called &lt;em&gt;resource-rational analysis&lt;/em&gt;. The key idea in resource-rational analysis is to begin with a computational-level analysis, assume an idealized &lt;em&gt;abstract computational architecture&lt;/em&gt; that solves the proplem posed at the computational level, and then examine how resources should be used optimally within that framework. It is specifically &lt;em&gt;not&lt;/em&gt; a proposal to modify the computational level:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Rather than blurring these lines and building constraints into computational-level theories, we suggest a different approach: define the computational-level theory without considering limitations on its execution, and then explore the consequences of those limitations as a further analysis that brings us closer to an algorithmic-level theory… Various proposals about limitations—or alternatively abstract computational architectures—provide us with levels of analysis between the computational and the algorithmic, and the principle of rationality provides us with a methodology for developing models at those intermediate levels. (pg. 220)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Griffiths et al. outline four steps in performing a resource-rational analysis:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;. Perform a computational-level analysis to determine the function of the system.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model of mental computation&lt;/strong&gt;. Pick a class of algorithms that approximate the optimal solution (e.g. particle filters), and define what the costs are in the model (e.g. number of samples).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Optimal resource allocation&lt;/strong&gt;. Determine which algorithm should be used in order to optimally trade off between accuracy and computational cost (i.e., be “resource rational”)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluate and refine&lt;/strong&gt;. Compare to human behavior, and revise assumptions in steps 1, 2, and 3 as necessary.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They posit a very specific definition of resource rationality, based on the notion of &lt;em&gt;value of computation&lt;/em&gt; (VOC), defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
c^*&amp;=\mathrm{arg}\max_{c\in C^n}\mathrm{VOC}(c)\\
\mathrm{VOC}(c)&amp;=\mathbb{E}_{P(B\vert c)}\left[\max_a \mathbb{E}_{P(Q,s\vert B)}[Q(s, a)]\right] - \mathrm{cost}(c)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $c$ is a computation, $s$ is the state, $a$ is the action, $Q(s,a)$ is the $Q$ function, and $B$ is the agent’s belief about $Q$ and $s$.&lt;/p&gt;

&lt;p&gt;Griffiths et al. note that in many cases, the initial resource-rational analysis may start by making certain assumptions of unbounded optimality (e.g., the ability to take perfect posterior samples). Further iterations of the analysis can identify these assumptions and apply a resource-rational analysis to them as well (e.g., correlated MCMC samples).&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I like this approach a lot, and I think that starting with approximation algorithms from computer science is likely to be incredibly fruitful. However, a lot of the algorithms from computer science have proven bounds for large $n$. I wonder to what extent there are other algorithms that computer scientists aren’t necessarily investigating that have provably better bounds (or other relevant properties) for small $n$? For example, are there inference algorithms that do poorly in terms of getting perfect posterior samples as $n$ goes to infinity, but which perhaps give somewhat better samples (e.g. less correlated) in the short term than more popular algorithms like MH?&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 07:53:39 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/rational%20process%20models/2016/01/12/Griffiths2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/rational%20process%20models/2016/01/12/Griffiths2015.html</guid>
        
        
        <category>Rational process models</category>
        
      </item>
    
      <item>
        <title>Selecting computations: theory and applications</title>
        <description>&lt;p&gt;&lt;span id=&quot;Hay2012&quot;&gt;Hay, N. J., Russell, S. J., Tolpin, D., &amp;amp; Shimony, S. E. (2012). Selecting Computations: Theory and Applications. &lt;i&gt;ArXiv Preprint ArXiv:1207.5878v1 [Cs.AI]&lt;/i&gt;. Retrieved from http://arxiv.org/abs/1207.5879&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper is quite related to &lt;a href=&quot;/quals/planning%20and%20decision%20making/2015/12/19/Guez2013.html&quot;&gt;Guez et al.&lt;/a&gt;, &lt;a href=&quot;/quals/planning%20and%20decision%20making/2015/12/16/Browne2012.html&quot;&gt;Browne et al.&lt;/a&gt;, and &lt;a href=&quot;/quals/planning%20under%20uncertain%20dynamics/2015/12/30/Bertuccelli2012.html&quot;&gt;Bertuccelli et al.&lt;/a&gt;. Here, Hay et al. suggest that to optimally make use of information earned when exploring a state/action space, a &lt;em&gt;metalevel&lt;/em&gt; decision problem needs to be solved:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Exploring unpromising or highly predictable paths to great depth is often wasteful; for a given amount of exploration, decision quality can be improved by directing exploration towards those actions sequences whose outcomes are helpful in selecting a good move. Thus, the &lt;em&gt;metalevel&lt;/em&gt; decision problem is to choose what future action sequences to explore (or, more generally, what deliberative computations to do), while the &lt;em&gt;object-level&lt;/em&gt; decision problem is to choose an action to execute in the real world. (pg. 1)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They show that the UCB1 or UCT bounds (used in bandit problems and MCTS) are suboptimal for the metalevel decision problem. The difference, they say, is that in bandit problems, the decision is relative to a utility you get in the real world, but in metalevel problems, the decision is relative to a cost of computation of &lt;em&gt;simulations&lt;/em&gt; rather than real-world actions.&lt;/p&gt;

&lt;p&gt;In some cases, the prior distribution of real-world outcomes is known and can be used in the simulations. However, in many other cases, the prior distribution on utilities is not available. Instead, the VOI (value of information) can be used as a way of determining what actions are good to take. These VOIs cannot be computed exactly, but under a few assumptions (myopic policy, samples are iid, expectation of a selection is equal to the sample mean, the distribution is bounded on both sides), they can be bounded from above. Hay et al. prove this bound, and show how it can be applied to MCTS: for root node sampling, rather than following the UCT policy, they use the VOI policy.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;A &lt;em&gt;metalevel decision process&lt;/em&gt; is denoted $M=(S,s_0,A_s,T,R)$, where $S$ are the states, $s_0$ is the intitial state, $A_s$ is the set of actions which includes simulated actions $E\in\mathcal{E}$ as well as the “stop” action $\perp$, $T$ is the transition function, and $R$ is the reward function, such that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
T(s,E,s^\prime)&amp;=p(E=e\ \vert\ E_1=e_1,\ldots{},E_n=e_n)\\
T(s,\perp,\perp)&amp;=1\\
R(s,E,s^\prime)&amp;=-c\\
R(s,\perp,\perp)&amp;=\max_i\mu_i(s)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mu_i(s)=\mathbb{E}[U_i\ \vert\ E_1=e_1,\ldots{},E_n=e_n]$. The value function of a policy $\pi$ for this MDP is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_M^\pi(s)=\mathbb{E}_M^\pi[-cN+\max_i\mu_i(S_N)\ \vert\ S_0=s]&lt;/script&gt;

&lt;p&gt;where $N$ is the total number of computations performed. The &lt;em&gt;expected&lt;/em&gt; number of computations is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E^{\pi^*}[N\ \vert\ S_0=s]\leq \frac{1}{c}\left(\mathbb{E}[\max_i U_i\ \vert\ S_0=s]-\max_i\mu_i(s)\right)&lt;/script&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;It would be interesting to see how this integrates with the Bayes-adaptive MCTS from &lt;a href=&quot;/quals/planning%20and%20decision%20making/2015/12/19/Guez2013.html&quot;&gt;Guez et al.&lt;/a&gt;. There, not only do they run simulations starting from the root node, they also resample the MDP from the prior distribution. So, I’m not sure whether the VOI approach here would still hold in that case. In general though, I do think it is important to take the metalevel decision making problem into account, so this is a really interesting direction to pursue particularly in the context of human reasoning (where our brains almost certainly need to make tradeoffs between doing more computation and acting).&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 05:15:18 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/rational%20process%20models/2016/01/12/Hay2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/rational%20process%20models/2016/01/12/Hay2012.html</guid>
        
        
        <category>Rational process models</category>
        
      </item>
    
      <item>
        <title>Bayesian fundamentalism or enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition</title>
        <description>&lt;p&gt;&lt;span id=&quot;Jones2011&quot;&gt;Jones, M., &amp;amp; Love, B. C. (2011). Bayesian fundamentalism or enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition. &lt;i&gt;The Behavioral And Brain Sciences&lt;/i&gt;, &lt;i&gt;34&lt;/i&gt;(4), 169–188. doi:10.1017/S0140525X10003134&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Jones &amp;amp; Love critique the use of Bayesian models of cognition, contrasting &lt;em&gt;Bayesian Fundamentalism&lt;/em&gt;, which:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… adheres to the tenet that human behavior can be explained through rational analysis — once the correct probabilistic interpretation of the task environment has been identified — without recourse to process, representation, resource limitations, or physiological or developmental data. (pg. 170)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;with &lt;em&gt;Bayesian Enlightenment&lt;/em&gt;, which:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… goes beyond the dogma of pure rational analysis and actively attemps to integrate with other avenues of inquiry in cognitive science… [it] thus treats Bayesian models as making both rational and mechanistic commitments, and it takes as a goal the joint evaluation of both. (pg. 170)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Jones &amp;amp; Love compare Bayesian Fundamentalism to Behaviorism and Evolutionary Psychology, arguing that Bayesian Fundamentalism doesn’t care about what is going on in the mind, only about the constraints of the environment and what the rational solution to the problem is given those constraints. They argue that despite this position, Bayesian models really &lt;em&gt;do&lt;/em&gt; make mechanistic assumptions about representations and processes. For example, in critiquing &lt;a href=&quot;/quals/theory%20learning/2016/01/09/Kemp2007.html&quot;&gt;Kemp et al.&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;However, it is not Bayes’ Rule or even the notion of overhypotheses that drives the prediction; rather it is the particular overhypotheses that were built into the model. In other words, the model was endowed with the capability to recognize a particular pattern (viz., regularity across words in which perceptual dimensions are relevant to meaning), so the fact that it indeed recognizes that pattern when presented with it is not surprising or theoretically informative. (pg. 178)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They also argue that despite emphasizing how important it is to specify the specific environmental constraints giving rise to the model, Bayesian Fundamentalists rarely do so. And, even if they did, it is nearly impossible to know what the right set of constraints are; in general it is possible to give a post-hoc argument for a particular set of constraints being “rational”:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The relevant environment for rational action could be the local environment present in the laboratory task, similar situations (however defined) that the person has experienced, all experiences over the person’s life, all experiences of our species, all experiences of all ancestral organisms traced back to single cell organisms, and so on. Furthermore, once the relevant environment is specified and characterized, the rational theorist has considerable flexibility in characterizing which relevant measures or statistics from the environment should enter into the optimality calculations. (pg. 181)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Jones &amp;amp; Love note furthermore that from an evolutionary perspective, it is not &lt;em&gt;behavior&lt;/em&gt; that is optimized but &lt;em&gt;mechanism&lt;/em&gt; that is optimized. Whatever the existing mechanism is, it is perhaps a slightly more optimized version of the mechanism that was there before, and so on. That means only that if the behavior is optimized at all, it is locally optimal, not globally optimal, because it depends on the process of optimization that has occurred. But even then, many things may simply be accidents or side effects and are not even optimized at all.&lt;/p&gt;

&lt;p&gt;Jones &amp;amp; Love argue that Bayesian Fundamentalism says nothing about development because Bayesian models are mechanism-free, and therefore it is not clear what develops:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;One key question for any developmental model is what develops. In rational models, the answer is that nothing develops. Rational models are mechanism-free, leaving only information sampled to change over time. Although some aspects of development are driven by acquisition of more observations, other aspects of development clearly reflect maturational changes in the mechanism. (pg. 182)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, the suggest that Bayesian Enlightenment holds promise by:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Treating Bayesian representations (e.g. the generative model or parameteters of conjugate priors) as actual hypotheses about psychological representations&lt;/li&gt;
  &lt;li&gt;Investigating Bayesian approximation algorithms (e.g. MCMC) as possible algorithmic-level processes&lt;/li&gt;
  &lt;li&gt;Performing rational analysis not within the context of the environment, but within the context of a mechanistic model&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Jones &amp;amp; Love actually make a lot of good points about how Bayesian models can be useful and fruitful. I disagree with their claims that they aren’t (or weren’t) already being used in that way. I also disagree with their characterization that Bayesian models aren’t informative if they recognize the patterns they were built to recognize (e.g. in the discussion of Kemp et al.). It is the connection with the data that makes them informative: if we have a hypothesis about how people reason in a particular domain, we can instantiate that explicit hypothesis in a Bayesian model and verify that the hypothesis does actually produce the same behavior as people. The model is just a tool for making the assumptions and implications of the hypothesis explicit.&lt;/p&gt;

&lt;p&gt;I am not sure why Jones &amp;amp; Love think that Bayesian models aren’t committed to hypotheses about psychological representation. The sense in which the computation of Bayesian models “doesn’t matter” is that Bayesians are not committed to the specific way by which the predictions of the model are computed; what they are committed to is the structure of the model. The structured representations offered by Bayesian models are their most powerful feature, in my opinion.&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 02:59:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/12/Jones2011.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/12/Jones2011.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Unfalsifiability and mutual translatability of major modeling schemes in choice reaction time</title>
        <description>&lt;p&gt;&lt;span id=&quot;Jones2014&quot;&gt;Jones, M., &amp;amp; Dzhafarov, E. N. (2014). Unfalsifiability and Mutual Translatability of Major Modeling Schemes for Choice Reaction Time. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;121&lt;/i&gt;(1), 1–32. doi:10.1037/a0034190&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Jones &amp;amp; Dzhafarov show that two popular response and response time (R&amp;amp;T) models—the &lt;em&gt;linear ballistic accumulator&lt;/em&gt; mode and &lt;em&gt;diffusion&lt;/em&gt; models—are unfalsifiable when distributional assumptions (which have previously been assumed to be an implementation detail) are removed. That is, they can be made to fit any R&amp;amp;T distribution, and thus do not provide any explanatory power regarding the mechanism behind responses and response times. Jones &amp;amp; Dzhafarov show how the LBM and diffusion models can be translated into another type of R&amp;amp;T model called the &lt;em&gt;Grice&lt;/em&gt; architecture, which is also unfalsifiable.&lt;/p&gt;

&lt;p&gt;They also discuss two assumptions regarding “selective influence”:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Stimulus information affects growth rates of the response processes, but not starting points or decision thresholds.&lt;/li&gt;
  &lt;li&gt;Speed-accuracy bias affects decision thresholds and possibly the starting point, but not the response processes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first assumption does not actually impose any constraints on the R&amp;amp;T models—they are still unfalsifiable. The second assumption does impose constraints on the R&amp;amp;T models, but Jones &amp;amp; Dzhafarov argue that this assumption is not well motivated. In particular, response processes can be recast in terms of decision thresholds and starting points, so it is not clear why the assumption needs to be made specifically about thresholds and starting points.&lt;/p&gt;

&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;$s$ — stimulus&lt;/li&gt;
  &lt;li&gt;$c$ — condition (or other factor)&lt;/li&gt;
  &lt;li&gt;$r$ — response&lt;/li&gt;
  &lt;li&gt;$t$ — response time&lt;/li&gt;
  &lt;li&gt;$G^{s,c}(r,t)$ — joint distribution over responses and upper bound on response time&lt;/li&gt;
  &lt;li&gt;$h^{s,c}(r,t)$ — joint hazard function specifying the probability density of giving response $r$ at time $t$, given that no response has occurred before $t$&lt;/li&gt;
  &lt;li&gt;$R_r^{s,c}(t)$ — response process (generally random)&lt;/li&gt;
  &lt;li&gt;$\theta_r^{s,c}$ — threshold&lt;/li&gt;
  &lt;li&gt;$T_r^{s,c}$ — first passage time (the time it would take $R_r^{s,c}$ to cross $\theta_r^{s,c}$ for the first time, independent of all other processes)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;grice-architecture&quot;&gt;Grice architecture&lt;/h2&gt;

&lt;p&gt;In the Grice model, the distribution of thresholds is independent of stimulus and condition. For each stimulus, there are $n$ deterministic response processes, and the thresholds $\theta_1,\ldots{},\theta_n$ are sampled from this fixed joint distribution before each trial. The processes then “race” to their respective thresholds, and whichever arrives first determines the response and response time.&lt;/p&gt;

&lt;h2 id=&quot;linear-ballistic-accumulation&quot;&gt;Linear ballistic accumulation&lt;/h2&gt;

&lt;p&gt;The LBA is also a race model. Each response process is a deterministic linear function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_r^{s,c}(t)=z_r^c+k_r^st&lt;/script&gt;

&lt;p&gt;where $z_r$ is the starting point of the process and $k_r^s$ is the process growth rate, which are sampled independently for each process on each trial. Thresholds $\theta_r^{s,c}=b^c$ are deterministic and shared across processes.&lt;/p&gt;

&lt;p&gt;Normally, it is assumed that starting points are sampled from a uniform distribution, and growth rates are sampled from a normal distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z_r^c&amp;\sim U(0, A^c)\\
k_r^s&amp;\sim \mathcal{N}(v_r^s, \eta^2)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;There is also a constant &lt;em&gt;nondecision time&lt;/em&gt; $t_0$ added to the first-passage time of the winning process.&lt;/p&gt;

&lt;p&gt;To summarize, the parameters are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$b^c$ — threshold for all response processes&lt;/li&gt;
  &lt;li&gt;$A^c$ — upper bound on starting points&lt;/li&gt;
  &lt;li&gt;$v_r^s$ — mean growth rate of response process $r$&lt;/li&gt;
  &lt;li&gt;$\eta$ — standard deviation of growth rate for all response proceses&lt;/li&gt;
  &lt;li&gt;$t_0$ — nondecision time&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the distributional assumptions on $z$ and $k$ are lifted, then the model is unfalsifiable.&lt;/p&gt;

&lt;h2 id=&quot;diffusion&quot;&gt;Diffusion&lt;/h2&gt;

&lt;p&gt;The response process is a stochastic process with a random starting point $z^c$, a random growth/drift rate $k^s$, a (usually fixed) diffusion rate $\sigma^2$, and a decay term with parameter $\beta$ (which is 0 in the Wiener model):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{d}R^{s,c}(t)=(k^s-\beta(R^{s,c}(t)-z^c))\mathrm{d}t + \sigma\mathrm{d}B(t)&lt;/script&gt;

&lt;p&gt;where $B(t)$ is Brownian motion and $R^{s,c}(0)=z^c$. The process terminates when it reaches 0 or a threshold $a^c$. The starting point is sampled from a uniform distribution, the growth rate from a normal distribution, and the nondecision time from a uniform distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z^c&amp;\sim \mathrm{Uniform}(\bar{z}^c-\frac{\delta_z}{2},\bar{z}^c+\frac{\delta_z}{2})\\
k^s&amp;\sim \mathcal{N}(v^s, \eta^2)\\
t_0&amp;\sim \mathrm{Uniform}(T_\mathrm{er}-\frac{\delta_t}{2},T_\mathrm{er}+\frac{\delta_t}{2})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\delta_z$ and is set such that $0\leq z^c\leq a^c$ and $\delta_t\leq 2T_\mathrm{er}$.&lt;/p&gt;

&lt;p&gt;To summarize, the parameters are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\beta$ — decay rate (set to 0 for the Wiener model)&lt;/li&gt;
  &lt;li&gt;$a^c$ — threshold separation&lt;/li&gt;
  &lt;li&gt;$\sigma$ — diffusion rate&lt;/li&gt;
  &lt;li&gt;$v^s$ — mean growth rate&lt;/li&gt;
  &lt;li&gt;$\eta$ — standard deviation of the growth-rate distribution&lt;/li&gt;
  &lt;li&gt;$\bar{z}^c$ — mean starting point&lt;/li&gt;
  &lt;li&gt;$\delta_z$ — range of the start-point distribution&lt;/li&gt;
  &lt;li&gt;$T_\mathrm{er}$ — mean nondecision time&lt;/li&gt;
  &lt;li&gt;$\delta_t$ — range of the nondecision distribution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The diffusion model without distributional assumptions on $z$, $k$, and $t_0$ is shown to be unfalsifiable, even when $\sigma=\delta_z=\delta_t=0$ and $\bar{z}^c=a^c/2$ for any $a^c=a$. Allowing these parameters to vary would in theory just make the model more flexible, but it’s already maximally flexible so in reality they don’t have much effect.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This article isn’t so much a challenge for probabilistic/Bayesian models of cognition, but it is important in the context of my own work in which I would like to be able to use reaction times as a proxy for things like simulation, and number of simulations. As a process model, though, I find the Grice framework a bit dissatisfying because its stochasticity doesn’t seem realistic. The important thing (to me) in these types of models would be the idea that the mind is getting some form of evidence over time—by discrete samples, or a continuous stream of perceptual samples, etc.—and thus the variability shouldn’t be in setting a threshold but in terms of the evidence. The DDM thus feels more interpretable to me, despite the fact that it can be translated into equivalent versions of the other models.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 10:42:58 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Jones2014.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Jones2014.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Optimal predictions in everyday cognition: the wisdom of individuals or crowds?</title>
        <description>&lt;p&gt;&lt;span id=&quot;Mozer2008&quot;&gt;Mozer, M. C., Pashler, H., &amp;amp; Homaei, H. (2008). Optimal predictions in everyday cognition: the wisdom of individuals or crowds? &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;32&lt;/i&gt;(7), 1133–1147. Retrieved from http://onlinelibrary.wiley.com/doi/10.1080/03640210802353016/abstract&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Mozer et al. question whether the conclusion of &lt;a href=&quot;/quals/probabilistic%20models%20of%20cognition/2015/11/08/Griffiths2006.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt;—that people have accurate knowledge of full prior distributions—is justified. In particular, they argue that the same results can be obtained with a model that only uses a few ($k$) samples from the prior. They show that a simple heuristic model (termed Min$k$) actually outperforms the full Bayesian model, and also matches the inter-participant variance better than the Bayesian model.&lt;/p&gt;

&lt;p&gt;Mozer et al. suggest that their Min$k$ model is perhaps more like an algorithmic-level theory, while the fully Bayesian model is a computational-level theory. However, they caution that by focusing purely on the computational level, important factors such as variability and processing time might be missed.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Mozer et al. suggest a few different models. First, the model from Griffiths &amp;amp; Tenenbaum (the &lt;em&gt;G&amp;amp;T-Bayesian&lt;/em&gt; model) predicts the median of the posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(t_{\mathrm{total}}\vert t_\mathrm{cur})\propto p(t_\mathrm{cur}\vert t_\mathrm{total})p(t_\mathrm{total})&lt;/script&gt;

&lt;p&gt;The &lt;em&gt;Min$k$&lt;/em&gt; model retrieves $k$ samples from memory and predicts the minimum sample that is greater than $t_\mathrm{cur}$. If no samples are greater than $t_\mathrm{cur}$, then the model predicts $(1+g)t_\mathrm{cur}$, where $g$ is guess proportion of the query.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;GT$k$Guess&lt;/em&gt; model is similar to Min$k$, except that instead of returning the minimum, it returns the median of the posterior computed with an empirical prior estimated via $k$ samples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(t_\mathrm{total})=\frac{1}{k}\sum_{i=1}^k \delta_{s_i,t_\mathrm{total}}&lt;/script&gt;

&lt;p&gt;If the prior estimate is less than $t_\mathrm{total}$, then the GT$k$Guess model performs the same guess with $g$ as the Min$k$ model.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;GT$k$Smooth&lt;/em&gt; model doesn’t guess, but instead uses something like a kernel density estimate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(t_\mathrm{total})\propto \sum_{i=1}^k \exp\left(\frac{(t_\mathrm{total}-s_i)^2}{2\sigma^2}\right)&lt;/script&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think this is actually a really important point—average participant judgments can’t necessarily be used to say something about the knowledge of individuals. I do think, though, that starting at the computational level and working down can still be revealing. In this case, the computational level analysis reveals that at least in &lt;em&gt;aggregate&lt;/em&gt; people’s judgments reflect true statistical regularities in the world. It could have been the other way around, and a computational level analysis allows us to first look at the high-level picture. Given that, we can then ask questions like the one that Mozer et al. ask here: what is actually the reason why people’s judgments reflect regularities? Is it that each participant has accurate knowledge of the prior, or that knowledge across people is distributed according to the prior? Mozer et al. show that it is the latter.&lt;/p&gt;

&lt;p&gt;I would be interested to see how well an analysis based on mechanisms for sampling would do. For example, would a model that uses rejection sampling also give a consistent algorithmic-level account? I would hypothesize that it would give the same results. You could also measure response time to see if people take longer to respond in cases where they are unlikely to get samples from the prior that are greater than $t_\mathrm{total}$—i.e., they have to make more rejections, and so it takes them longer to respond.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 08:27:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Mozer2008.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Mozer2008.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Judgment under uncertainty: heuristics and biases</title>
        <description>&lt;p&gt;&lt;span id=&quot;Tversky1974&quot;&gt;Tversky, A., &amp;amp; Kahneman, D. (1974). Judgment under Uncertainty: Heuristics and Biases. &lt;i&gt;Science&lt;/i&gt;, &lt;i&gt;185&lt;/i&gt;(4157), 1124–1131. doi:10.1126/science.185.4157.1124&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Tversky &amp;amp; Kahneman give an overview of three commonly used heuristics that lead to systematic biases: representativeness, availability, and anchoring.&lt;/p&gt;

&lt;h2 id=&quot;representativeness&quot;&gt;Representativeness&lt;/h2&gt;

&lt;p&gt;The representativeness heuristic was discussed in &lt;a href=&quot;/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html&quot;&gt;Kahneman &amp;amp; Tversky&lt;/a&gt;, so I won’t go into too much detail about it here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insensitivity to prior probabilities&lt;/li&gt;
  &lt;li&gt;Insensitivity to sample size&lt;/li&gt;
  &lt;li&gt;Misconceptions of chance — e.g. HTHTTH is more representative than HHHTTT&lt;/li&gt;
  &lt;li&gt;Insensitivity to predictability&lt;/li&gt;
  &lt;li&gt;The illusion of validity — confidence is a function based more on degree of representativeness rather than accuracy&lt;/li&gt;
  &lt;li&gt;Misconceptions of regression&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;availability&quot;&gt;Availability&lt;/h2&gt;

&lt;p&gt;The availability heuristic states that people will estimate things like probabilities by seeing how many items from the distribution come to mind. Thus, things that come to mind more easily will be estimated to have higher probability, even if they don’t actually have higher probability.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Biases due to the retrievability of instances — e.g. things like fame or salience will make things more available, and thus be considered more numerous&lt;/li&gt;
  &lt;li&gt;Biases due to the effectiveness of a search set — e.g. judging whether more words start with ‘r’ or have ‘r’ as the third letter&lt;/li&gt;
  &lt;li&gt;Biases of imaginability — e.g. intuitively computing “10 choose $k$” for $2\leq k\leq 8$. People are better able to imagine disjoint sets for $k=2$ than $k=8$, so they say that “10 choose 2” is higher than “10 choose 8”, even though they are the same&lt;/li&gt;
  &lt;li&gt;Illusory correlations — overestimating the frequency of co-occurring events (e.g. believing that patients who exhibit suspiciousness draw characters with shifty eyes, because “suspiciousness” and “eyes” have a strong association, even if they might not predict anything true about the diagnosis)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;anchoring-and-adjustment&quot;&gt;Anchoring and Adjustment&lt;/h2&gt;

&lt;p&gt;The anchoring heuristic states that people estimate values by starting at one value (the &lt;em&gt;anchor&lt;/em&gt;) and adjusting either up or down. Usually, people fail to fully adjust, and thus their estimates are biased towards the anchor.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insufficient adjustments — e.g. in multiplying $8\times 7\times 6\times 5\times 4\times 3\times 2\times 1$ vs. multiplying $1\times 2\times 3\times 4\times 5\times 6\times 7\times 8$, people will report higher estimates for the first product and lower estimates for the second product&lt;/li&gt;
  &lt;li&gt;Biases in the evaluation of conjunctive and disjunctive events — e.g. overestimation that things with high probability will co-occur, underestimation that at least one thing with low probability will occur. The anchors in this case are the probabilities which are then insufficiently adjusted, leading to overestimates of high probability things and underestimates of low probability things.&lt;/li&gt;
  &lt;li&gt;Anchoring in the assessment of subjective probability distributions — e.g. starting at current (mean) estimate and adjusting upward to get to the 90th percentile&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Each of these heuristics is quite telling about the way that people think. Although they’ve been taken in the past that people are just “irrational”, I think they’re more interesting when thought about in the context of how people are actually processing information. The case is clearer in cases like the availability heuristic, for example: it seems plausible that whatever algorithm is used for retrieving things from memory will bias us when we try to objectively estimate the probabilities of things. Generally, the point of memory seems to be to provide us with information that is going to be &lt;em&gt;relevant&lt;/em&gt;, which means there will be an effect of both frequency and utility—the combination of which would manifest itself as “availability”.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 07:27:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Tversky1974.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Tversky1974.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
  </channel>
</rss>
