<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 07 Jan 2016 13:55:28 -0800</pubDate>
    <lastBuildDate>Thu, 07 Jan 2016 13:55:28 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Representational momentum</title>
        <description>&lt;p&gt;&lt;span id=&quot;Freyd1984&quot;&gt;Freyd, J. J., &amp;amp; Finke, R. A. (1984). Representational Momentum. &lt;i&gt;Journal Of Experimental Psychology: Learning, Memory, and Cognition&lt;/i&gt;, &lt;i&gt;10&lt;/i&gt;(1), 126–132. doi:10.1037/0278-7393.10.1.126&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Freyd &amp;amp; Finke present a number of experiments showing an effect of &lt;em&gt;representational momentum&lt;/em&gt; in which people’s memories for the position/rotation of an object are distorted in the direction of implied motion of the object.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, they showed participants a sequence of four images and asked them to determine whether the third and fourth images were the same or different. The first three images showed a rectangle in different orientations resulting from rotating it in a particular direction. The fourth image was either the same as the third, or rotated slightly forward or backwards from the third image (though not as much as the angular separation between the first three images). Freyd &amp;amp; Finke found that people were significantly faster and more accurate at judging the same (15.8% error) and reverse (6.4% error) stimuli, compared to the forward (43.9% error) stimuli.&lt;/p&gt;

&lt;p&gt;In Experiment 2, they controlled for the effect of the result being somehow due to “some configural property of the inducing displays, independent of their temporal order”. Thus, they swapped the first and second images in the presentation. This caused the representational momentum effect to disappear entirely.&lt;/p&gt;

&lt;p&gt;In Experiment 3, Freyd &amp;amp; Finke asked whether the representation momentum effect was due to sensory or cognitive processing. To test this, they increased the ISI (inter-stimulus interval) times to 500ms and 750ms (from 250ms). With these larger times, they still found the representational momentum effects, though they were less strong than at 250ms.&lt;/p&gt;

&lt;p&gt;They additionally ran another control study to see if it was just whether people were extrapolating forward to the next image in the sequence (after the first three images). Instead of having small perturbations to the rotation of the third image, they used the next image in the sequence. They did not find the representational momentum effect using these larger rotations.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;There seems to be something about people’s predictive processing of visual stimuli that causes them to misremember the location or pose of objects. I wonder if something like this could be explained by a system that, rather than storing the actual current perception, stores an expectation for the next perception. If there has recently been motion, then this would give rise to something like the representational momentum effect. It doesn’t quite fit with their experimental setup, though, which is that they showed each image for 250ms, then removed it for 250ms, then showed the next, etc. These long presentation times mean that apparent motion isn’t generated, so it doesn’t seem as if people are estimating the angular velocity and then propagating that forward—if they were, then presumably the last control experiment that Freyd &amp;amp; Finke found would have still found the effect. Perhaps one explanation for this is that the visual system is estimating velocity from the implied motion, but it only propagates it for a fraction of the time forward—i.e., it doesn’t estimate the ISI time and factor that in. If this were the case, then it should be possible to demonstrate that different angular velocities result in memory distortions that are either closer or further away from the true image.&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Jan 2016 05:54:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/representational%20momentum/2016/01/07/Freyd1984.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/representational%20momentum/2016/01/07/Freyd1984.html</guid>
        
        
        <category>Representational momentum</category>
        
      </item>
    
      <item>
        <title>Visual perception and interception of falling objects: a review of evidence for an internal model of gravity</title>
        <description>&lt;p&gt;&lt;span id=&quot;Zago2005&quot;&gt;Zago, M., &amp;amp; Lacquaniti, F. (2005). Visual perception and interception of falling objects: a review of evidence for an internal model of gravity. &lt;i&gt;Journal Of Neural Engineering&lt;/i&gt;, &lt;i&gt;2&lt;/i&gt;(3), S198–208. doi:10.1088/1741-2560/2/3/S04&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this article, Zago &amp;amp; Lacquaniti review evidence for the hypothesis that the brain has an internal model of gravity. They begin by layout out the computational problem, which is to solve for &lt;em&gt;time-to-contact&lt;/em&gt; (TTC). Specifically, if an object starts at height $h_0$, and follows the equation $h(t)=h_0-0.5gt^2$, then solving for TTC involves solving for $h(t)=0$.&lt;/p&gt;

&lt;p&gt;Next, Zago &amp;amp; Lacquaniti discuss evidence from visual perception that people take gravity into account. They summarize some work from Kim &amp;amp; Spelke (1992) suggesting that infants are sensitive to the downward acceleration of gravity. Adults are also sensitive to gravity in visual perception, with evidence that it affects the perception of causality and naturalness of motion, absolute distance and size of falling objects, and even biological motion (details aren’t given for these examples, though, just citations).&lt;/p&gt;

&lt;p&gt;Zago &amp;amp; Lacquaniti also give evidence that people take gravity into account when trying to catch a falling ball. For example, if a ball is dropped directly over a person’s hand, muscle activity can be detected just before the ball reaches the hand (with a delay accounting for processing of the motor system), and that the time this activity is detected is consistent even if the ball is dropped from different heights. The expected mass of the ball affected the magnitude of the muscle activity, but not the timing. Similar effects were shown even when people didn’t have visual access to the ball, though in these cases it was a reflex behavior, rather than anticipatory behavior. Still, the results indicate that “subjects are able to reach an internal estimate of the expected duration of fall even in the absence of vision, as demonstrated by the fact that they can easily detect randomly interspersed cases of inaccurate timing of the auditory cue” (pg. S200).&lt;/p&gt;

&lt;p&gt;However, Zago &amp;amp; Lacquaniti also discuss the fact that visual perception is remarkably bad at estimating or detecting changes in acceleration. They suggest that, rather than people maintaining an estimate of gravity that can change, gravity is an “ecological constraint” that cannot change. They present evidence that shows that astronauts (in 0g) still predict objects to fall in the same manner as they do on earth. Even after spending extended time in 0g, astronauts only adapted very slightly, and even then readjusted to 1g almost immediately upon returning to Earth. Similarly, experiments conduced on participants using computer simulated gravity revealed that people were consistently able to punch a ball moving under 1g but prematurely punched a ball moving under 0g. Interestingly, they also found that participants seemed to ignore acceleration when the task was less motor based, i.e., if they just had to click a button to intercept the ball, leading to higher performance in 0g than in 1g. Thus, it would appear that perceptual and motor task demands determine whether the mind’s internal estimate of gravity is recruited, or whether just visual cues are used.&lt;/p&gt;

&lt;p&gt;Based on computational results (see Algorithm), it seems that people have an internal model of gravity that is fixed at 1g. Some adaptation can occur by modifying other parameters in the model, but not the actual estimate of the gravitational acceleration. There are results that suggest that the model of gravity is calculated by the vestibular system, and that this system is recruited less for tasks involving unnatural motion.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Zago &amp;amp; Lacquaniti discuss possible computation models of how people’s estimate of gravity works. One hypothesis is that they have two separate models of gravity (one for 0g and one for 1g). Another hypothesis is that they have a single model of gravity, but change it’s parameters. This second hypothesis can additionally be broken down into two hypotheses about &lt;em&gt;which&lt;/em&gt; particular parameter is tuned. The model predicts:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{TTC}(t)=\frac{-\hat{v}(t)+\sqrt{\hat{v}(t)^2+2\hat{g}\hat{h}(t)}}{\hat{g}}&lt;/script&gt;

&lt;p&gt;where $\hat{v}$ and $\hat{h}$ are the visually estimated velocity and height, respectively, and where $\hat{g}$ is the estimate of gravitational acceleration. If $\lambda$ is then the amount of time it takes to execute the action, then the action should be triggered at time $\epsilon$, where $\mathrm{TTC}(\epsilon)=\lambda$. If this model is used assuming $\hat{g}=1g$, but the true gravitational acceleration is $0g$, then the resulting error will be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta=\frac{\hat{g}\lambda^2}{2v_0}&lt;/script&gt;

&lt;p&gt;This error could be reduced by either changing the value of $\hat{g}$ or the value of $\lambda$. If $\hat{g}$ is reduced, then this should affect responses under both 0g and 1g (because it is a variable in the TTC equation), but if $\lambda$ is reduced, then this should affect only 0g.&lt;/p&gt;

&lt;p&gt;On the other hand, if an entirely separate $0g$ model is learned, then it should predict:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{TTC}(t)=\frac{\hat{h}(t)}{\hat{v}(t)}&lt;/script&gt;

&lt;p&gt;Empirical results suggest that adaptation is consistent with the first TTC model that has a fixed value of $\hat{g}=1g$ but which modifies $\lambda$.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;It is not too surprising that gravity plays such a prominent role in our ability to reason about the world. As it is essentially an invariant in our environments, it is also not surprising that it seems to be largely invariant to adaptation as well. What’s interesting are the results that seem to show that sometimes the vestibular estimate of gravity is used, while other times it isn’t. Perhaps, when faced with unnatural motion tasks (e.g. -1g), our brains are consistently computing the error in predicting motion for objects. If there is consistent error, then perhaps it switches to not relying on the internal estimate of gravity, and relies solely on visual cues. If the motion is consistent with natural gravitational acceleration, then that estimate is used—though it’s also interesting that sometimes that estimate is used for visual perception while other times it only seems to be recruitable when people are engaged in motor action. I’m not entirely sure what the right explanation is for all of these results. When do we take gravity into account, and when do we not, and why?&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Jan 2016 04:18:24 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning/2016/01/07/Zago2005.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning/2016/01/07/Zago2005.html</guid>
        
        
        <category>Physical reasoning</category>
        
      </item>
    
      <item>
        <title>Physical imagery: kinematic versus dynamic models</title>
        <description>&lt;p&gt;(missing reference)&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Schwartz asks: is physical imagery based on kinematics or dynamics? Specifically, does it only rely on spatial information (kinematic model, KM), or does it also incorporate information about things like forces (dynamic model, DM)? Schwartz shows through a series of four experiments that physical imagery is consistent with the dynamics account.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, participants completed two tasks in one of two orders. One task was the “judge” task, which was to view two differently size glasses of water (with the same level of water) and to determine which glass needed to tilt farther for the water to spill out. The other task was the “tilt” task, in which participants physically rotated empty glasses with their eyes closed and imagined the water in the glasses. The hypothesis was that if people were using KM, if people performed the “judge” task before the “tilt” task, then their judgments should affect the outcome of the tilt (as the tilt would just be based on a spatial outcome). If people were using DM, then doing the “judge” task before the “tilt” task should interfere with people’s mental imagery (which based on previous work is accurate for tilting but inaccurate for judging). They found the results that were consistent with DM: when tilting first, people (correctly) tilted the thin glass further, but when judging first, they either tilted the wide glass further or tilted the two glasses the same. People were almost entirely incorrect in the explicit judgments, and judgments did not correspond to tilts.&lt;/p&gt;

&lt;p&gt;In Experiment 2, participant performed the “tilt” task but were told to either imagine that the liquid in the glass was water or molasses. The hypothesis was that if people are using KM, then the type of liquid should have no effect; if they are using DM, then they should tilt the molasses glasses further, because the molasses moves more slowly than water. The results were again consistent with the DM account. People tilted the thin glasses further than wide glasses (as expected) and also turned the molasses glasses further than the water glasses. Schwartz makes an important point in the discussion of this experiment:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;People rely on the temporal coordinations of physical imagery to allow inferences to emerge; they do not first decide what the inference should be and then adjust the timing of things to portray that inference. (pg. 449)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Experiment 3, Schwartz had people tilt glasses normally, or tilt them starting from a horizontal position. He also had people perform the tasks lying down. In addition to the tilting task, Schwartz had people rate the quality of their mental imagery, in order to gauge how well people were able to imagine the water in the glass. The idea was to see whether gravity would have an effect on people’s ability to perform the task. He found that, as before, people were able to perform the task when both they and the glass were upright. Interestingly, people could also perform the task if they were lying down, provided the glass was still upright with respect to gravity. They could not perform the task if the glass was horizontal, saying things like “the water began to pour out when I started to tilt the glass” (pg. 452). In terms of the quality of their imagery when the glass was sideways, it was typically high initially (before they started tilting), but image quality degraded as they began the glass tilt. These results also support the DM account.&lt;/p&gt;

&lt;p&gt;Experiment 4 looked at another manipulation to test how perceptual information affects imagery. In this experiment, Schwartz had regular glasses and weighted glasses, and hypothesized that people would turn the weighted cups less than the regular cups because the extra weight from the glass introduces a torque that increases as the amount of rotation increases. In particular, as the water level decreases, people should increasingly under rotate because they have to rotate further into the torque. As predicted, this is what Schwartz found. He also ran a control version in which he had people rotate the glasses to a specified rotation (without water, 45 degrees). People were able to perform this task nearly perfectly, suggesting that the results from the main experiment were not due to people’s inability to represent the angle of the glass. Rather, Schwartz suggests that the effect is due to a relationship between the rate of work exerted in turning the glass and rate of change of the water. As the class is turned, the rate of work increases, causing the water to change more quickly, and thus causing people to underrotate.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;These experiments indicate that dynamic information is clearly incorporated in our ability to visualize objects and our actions on those objects. Schwartz makes the argument that, contrary to other mental imagery accounts, we do not represent transformations by computing the target spatial orientation, but that we represent the control and apply that control until the orientation is achieved. I like this account, but I still don’t quite know how it fits into the mental &lt;em&gt;rotation&lt;/em&gt; account. It cannot be that people just randomly pick a direction of rotation, as then their response times would average out to be constant.&lt;/p&gt;

&lt;p&gt;Perhaps, as argued by &lt;a href=&quot;/quals/mental%20imagery/2015/12/31/Just1976.html&quot;&gt;Just and Carpenter&lt;/a&gt;, people do rely on some sort of feature matching in order to determine the direction of rotation—but not the final orientation. Then, they apply the relevant control in order to move the shape in that direction until they match the correct orientation. It’s still not clear to me exactly how you would tell if you’ve reached the correct orientation… I suppose if people only rotate one part of the shape, then that local piece would be easy to compare. Then, once the local rotation is found, people presumably know what the angle is and can rotate the rest of the shape to that angle, and don’t necessarily need to provide the control.&lt;/p&gt;

&lt;p&gt;The control account is very compelling, but I wonder if there are really some cases where we use purely visual imagery, and other cases where we use dynamic imagery. For example, in the mental rotation case I just described, could one component of that be using dynamic imagery, while another component just uses spatial imagery? Would it be possible to test for this? Do these two cases differ in important ways (i.e., does it really matter if we only use one or the other)? I would expect that it does. I have read a lot of stuff in robotics that is based on knowing the goal state and applying control to get there, but I have read less about simply applying control until some conditions are satisfied (i.e. it is not explicitly a goal state in terms of pose). I need to think more about how these two things are different (or if they are different at all). Perhaps the latter isn’t really actually that different—it’s just that some higher level planner is making the goal states be not very far away from the current state.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 14:33:50 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning/2016/01/06/Schwartz1999.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning/2016/01/06/Schwartz1999.html</guid>
        
        
        <category>Physical reasoning</category>
        
      </item>
    
      <item>
        <title>Representing statics as forces in equilibrium</title>
        <description>&lt;p&gt;&lt;span id=&quot;Freyd1988&quot;&gt;Freyd, J. J., Pantzer, T. M., &amp;amp; Cheng, J. L. (1988). Representing Statics as Forces in Equilibrium. &lt;i&gt;Journal Of Experimental Psychology: General&lt;/i&gt;, &lt;i&gt;117&lt;/i&gt;, 395–407. doi:dx.doi.org/10.1037/0096-3445.117.4.395&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Freyd et al. hypothesize that the mind represents dynamic forces, even for static scenes. In particular, they point out that in static scenes, it is not truly the case that there are no forces: it is just that forces are in equilibrium. They run a series of four experiments to test this hypothesis, finding that people’s memory for static objects is distorted in the direction that those objects would move if they were unsupported.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, Freyd et al. showed participants a scene with a plant either on a table or hanging from a hook for 250ms. The scene was removed for 250ms at which point an identical scene was showed for 250ms, except that the table or hook was removed. The scene was again removed for 250ms, and then a new scene was shown which was either (1) the same, (2) different, with the plant moved slightly upwards, or (3) different, with the plant moved slightly downwards. Participants had to determine whether the last scene was the same as the original one or not. They found that people were quite accurate at determining “same” trials (8-13% error) and worse at detecting “down” trials (57% error) than “up” trials (37% error).&lt;/p&gt;

&lt;p&gt;Experiment 2 was a control in which Freyd et al. never showed any form of support. The results showed basically no difference between the “down” trials (33%) vs. the “up” trials (34%). Interestingly, though, the error rate for the “same” trials was higher than in Experiment 1 (26%).&lt;/p&gt;

&lt;p&gt;Experiment 3 was similar to Experiments 1 and 2, except that they used a different display (a lock hanging from a hook) and tested different distances from the true position. This was done in order to get a more fine-grained estimate of the distortion in people’s memories. As in Experiments 1 and 2, they found distortions when there was support, and that the shift was positive (i.e. “down” trials had more error) when there was support and that it was close to zero when there was no support.&lt;/p&gt;

&lt;p&gt;In Experiment 4, Freyd et al. tested another new display, this time with a block resting on a spring. This allowed them to test for distortions in memory both in the upward and downward directions. There were two conditions: one in which there was initially no block (which would predict a downward shift, as the spring should compress), and one in which there was initially a block (which would predict an upward shift, as the spring should decompress). The results supported the predicted direction of memory distortions.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;These are a cool set of results, related to representational momentum, that indicate that people have strong perceptual expectations about the way that objects should move. In particular, I think the results of Experiment 4 are really striking: it’s not just that people expect things to move down due to gravity, but that they expect them to move in the way they actually would. I expect you would find similar results with objects that typically move up (e.g. balloons). I also wonder if you would find these effects if they depended on higher-level knowledge about object properties (such as mass). For example, in the spring experiment, if you knew the block was extremely light (and thus would not compress the spring), would the memory distortion still take place?&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 07:09:25 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning/2016/01/06/Freyd1988.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning/2016/01/06/Freyd1988.html</guid>
        
        
        <category>Physical reasoning</category>
        
      </item>
    
      <item>
        <title>The experience of force: the role of haptic experience of forces in visual perception of object motion and interactions, mental simulation, and motion-related judgments</title>
        <description>&lt;p&gt;&lt;span id=&quot;White2012a&quot;&gt;White, P. A. (2012). The experience of force: The role of haptic experience of forces in visual perception of object motion and interactions, mental simulation, and motion-related judgments. &lt;i&gt;Psychological Bulletin&lt;/i&gt;, &lt;i&gt;138&lt;/i&gt;(4), 589–615. doi:10.1037/a0025587&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, White proposes a theory of action and perception that is based on the notion of force. Specifically, he argues that during our interactions with the world, we perceive force from our haptic system (along with other sensory modalities), and these perceptions get stored in memory along with the relevant actions associated with them. Then, when we perceive new situations, we activate these stored representations which allows us to make predictions and judgments about motion and other factors.&lt;/p&gt;

&lt;p&gt;First, White discusses evidence for forward models of action in the motor system, as well as evidence for the role of mechanoreceptor feedback. What is sounds like he proposes is a sort of forward model like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[\mathbf{x}_{t+1}, \mathbf{s}_{t+1}] = f(\mathbf{x}_t,\mathbf{s}_t,\mathbf{u}_t)&lt;/script&gt;

&lt;p&gt;where $\mathbf{x}$ is the state of the system, $\mathbf{s}$ is the sensory information (e.g. from the haptic system), and $\mathbf{u}$ are the controls (forces) of the system. A prediction error for the sensory information (e.g. mechanoreceptor feedback) is also computed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\delta}_t=\mathbf{s}_t - \hat{\mathbf{s}}_t&lt;/script&gt;

&lt;p&gt;where $\mathbf{s}_t$ is the predicted sensory information and $\hat{\mathbf{s}}_t$ is the true sensory information. The feedback $\mathbf{\delta}_t$ is thus the error signal, which is going to be zero when our predictions of force are accurate. White also argues that perception of additional object properties (texture, rigidity, mass, etc.) are computed based on sensory information from mechanoreceptors. I’ll denote these properties as $\mathbf{\pi}$.&lt;/p&gt;

&lt;p&gt;All of these different sources of information are stored in long-term memory, roughly (it seems) in the form of tuples such as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{m}_t=[\mathbf{x}_t,\mathbf{\delta}_t,\mathbf{\pi}]&lt;/script&gt;

&lt;p&gt;White describes these as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A stored representation of an action on an object is a multimodal episodic trace combining haptic information such as the disposition and movement of the limbs during execution of the action, visual information about body movement and the associated motion of the object acted on, auditory information such as sounds elicited by contact between extremity and object, and in principle, information in any sensory modality. Internally available information such as the content of the forward model also forms part of the representation. (pg. 607)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Importantly, we store our &lt;em&gt;prediction error&lt;/em&gt; of sensory information, rather than the absolute sensory information itself.
These stored representations are activated by matching to similar perceptual stimuli (e.g. visual stimuli).&lt;/p&gt;

&lt;p&gt;White uses this formulation of stored representations to offer a unifying account for several lines of research:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The storing of sensory feedback, rather than direct sensory information, predicts that when making judgments about force in terms of one moving object acting on a stationary, we assign notions of force &lt;em&gt;from&lt;/em&gt; the moving object (because we do store $\mathbf{u}_t$) but not &lt;em&gt;to&lt;/em&gt; the moving object (because, for static objects, the sensory prediction error should be zero). If both objects are moving, however, we should assign a notion of force that the second object is applying to the first object because the sensory prediction error is nonzero. This explains, for example, Michottean launching effects.&lt;/li&gt;
  &lt;li&gt;To the extent that visual perception of motion matches stored representations corresponding to actions, we should perceive that motion as being internally caused. This extends to biological plausibility as well. Importantly, biologically generated motion has different velocity profiles than, for example, two nonbiological objects colliding—thus visual motion that matches the biological motion velocity profile should be interpreted as more biological.&lt;/li&gt;
  &lt;li&gt;Representational momentum&lt;/li&gt;
  &lt;li&gt;Perception of inanimate entities as intentional&lt;/li&gt;
  &lt;li&gt;Mental simulation&lt;/li&gt;
  &lt;li&gt;Perception of mass&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a surprisingly consistent and satisfying account of how perception arises from the combination of visual and haptic feedback. Assuming people do store information as something like $\mathbf{m}_t$ defined above, and they have access to forward an inverse models, it should be possible to reconstruct $\mathbf{u}_t$ (from both $\mathbf{x}_t$ and $\mathbf{x}_{t+1}$), which is consistent with White’s assertions. I am skeptical, though, that all we are doing is “storing” and “matching” representations. It is not at all clear to me how it would work to match the motion of a 2D ball (e.g. in the Michotte experiments) to the stored motion of ourselves. Additionally, it sounds like White is advocating for something like an exemplar model, but I find it much more likely that we use our experiences to build structured forward or inverse models. There may be multiple forward models (as suggested by Kawato) that are perhaps combined in certain ways, but give that there is evidence for some generalization (also described by Kawato), I would be very surprised if all that was going on was just storing and matching exemplars.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 04:47:29 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/06/White2012a.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/06/White2012a.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Prediction precedes control in motor learning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Flanagan2003&quot;&gt;Flanagan, R. R., Vetter, P., Johansson, R. S., &amp;amp; Wolpert, D. M. (2003). Prediction precedes control in motor learning. &lt;i&gt;Current Biology&lt;/i&gt;, &lt;i&gt;13&lt;/i&gt;(2), 146–150. doi:10.1016/S0960-9822(03)00007-1&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Flanagan et al. describe an experiment in which participants must grip an object with their index finger and thumb, and move it in a straight line to a target. The dynamics of the object were modified so that when they moved it in the horizontal plane, a proportional vertical force was applied to the object. Thus, to learn to move it in a straight line, participants had to adapt to the vertical force.&lt;/p&gt;

&lt;p&gt;Flanagan et al. found that participants took a long time (about 70 trials) before they were able to fully adjust their trajectories to be straight. However, they took much less time (about 10 trials) to adjust the force with which they gripped the object to match that of the corresponding load force. These results suggest that there are two internal models (one for the grip force, and one for the arm trajectory) that are being learned at separate rates. Specifically, Flanagan et al. suggest that in the first case, it is a forward kinematic model that is being learned, while in the second case, it is a inverse dynamics model that is being learned. This is consistent with the demands of the task: the novel dynamics of the object require learning a new mapping from desired trajectory to motor commands (the inverse model), but they do not require learning a new mapping for controlling the load force. Rather, the motor system needs only to predict the load force so that it can appropriately adjust for it.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This paper basically answers the question I ended with in &lt;a href=&quot;/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html&quot;&gt;Kawato’s review&lt;/a&gt;: learning operates independently in the forward and inverse models. Flanagan et al. suggest that, computationally, this may be able to be explained by something like &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/20/Nguyen-Tuong2011.html&quot;&gt;distal teacher learning&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 13:30:48 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Flanagan2003.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Flanagan2003.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Internal models for motor control and trajectory planning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kawato1999&quot;&gt;Kawato, M. (1999). Internal models for motor control and trajectory planning. &lt;i&gt;Current Opinions In Neurobiology&lt;/i&gt;, &lt;i&gt;9&lt;/i&gt;(6), 718–727. doi:10.1016/S0959-4388(99)00028-8&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this review article, Kawato discusses the role of internal models in motor control. He argues that both forward and inverse internal models are used in motor control. In particular:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Fast and coordinated arm movements cannot be executed solely under feedback control, since biological feedback loops are slow and have small gains. Thus, the internal model hypothesis proposes that the brain needs to acquire an inverse dynamics model of the object to be controlled through motor learning, after which motor control can be executed in a pure feedforward manner.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, Kawato gives an overview of the existence of internal models. One piece of evidence comes from perturbing the dynamics of participants’ motions. Initially, people make the wrong movements under these novel dynamics. However, they eventually adapt and can make the correct motion. If the new dynamics are removed, then they again make errors because they are now using an incorrect model of the inverse dynamics. Another piece of evidence comes from &lt;em&gt;grip-force—load-force coupling&lt;/em&gt;, which is the coupling of a grip force (e.g. thumb and index finger) and load force (e.g. weight of the object that is being held). When gripping an object like this, and moving one’s arm to a new location, the motor system must both determing the appropriate trajectory for the arm as well as the grip force needed to hold the object. The way this works is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The inverse model of the combined dynamics of the arm, hand, and object calculates the necessary motor commands from the desired trajectory of the arm. These commands are sent to the arm muscles as well as to the forward dynamics model as the efference copy. Then, the forward model can predict an arm trajectory that is slightly in the future. Given the predicted arm trajectory, the load force is calculated; then, by multiplying a friction coefficient and a safety factor, the necessary minimum level of grip force can be calculated.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next, Kawato discusses neurological evidence for internal models in the cerebellum. I’m not going to go into detail on this.&lt;/p&gt;

&lt;p&gt;Next, Kawato discusses the structure of internal models. Specifically, is it that internal models are just implemented as mappings between states and actions, or are they implemented using some sort of generalizable parameterization? To test this, the “generalization” paradigm is used, in which participants are trained to do a specific task under novel dynamics. They are then instructed to do another task, still under the novel dynamics, to see whether the dynamics have been fully generalized to new situations or not. The results are somewhere in the middle: some generalization occurs, but not perfect generalization.&lt;/p&gt;

&lt;p&gt;Finally, Kawato discusses how the motor system computes trajectories. There are apparently two competing models: kinematic models (e.g. the minimum jerk model) and dynamics models (e.g. the minimum torque-change mdoel). Kawato proposes the &lt;em&gt;minimum variance&lt;/em&gt; model as middle ground between these two models. In the minimum variance model, the objective function minimizes a kinematic value (the variance of the end pose); however, the variance is determined by motor commands, which are part of the dynamics. If this model is correct, then it gives another motivation for there being both be a kinematic internal model (i.e. a forward model) as well as a dynamics internal model (i.e. a inverse dynamics model)&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Based on this article, there is very strong evidence for internal models in the motor system. Moreover, there is evidence for &lt;em&gt;both&lt;/em&gt; forward kinematic models and inverse dynamics models which seem to work in tandem.&lt;/p&gt;

&lt;p&gt;I wonder how these models are learned. In particular, when people’s motions are perturbed in some of the studies that Kawato describes, is it that people are updating their forward models, or their dynamics models, or both? Does learning in one affect the other, or are they independent?&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 11:48:31 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Temporal and kinematic properties of motor behavior reflected in mentally simulated action</title>
        <description>&lt;p&gt;&lt;span id=&quot;Parsons1994&quot;&gt;Parsons, L. M. (1994). Temporal and kinematic properties of motor behavior reflected in mentally simulated action. &lt;i&gt;Journal Of Experimental Psychology: Human Perception and Performance&lt;/i&gt;, &lt;i&gt;20&lt;/i&gt;(4), 709–730. doi:10.1037/0096-1523.20.4.709&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Parsons asks the question: how similar are mental simulations of motor actions to actual motor actions themselves? In a series of five experiments, he demonstrates that they correspond quite closely—though they differ in a few revealing ways—supporting the hyothesis that mental simulations do operate on a detailed model of the body and utilize the same type of trajectory optimization/motion planning that the motor system does.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In all experiments, Parsons showed participants images of left and right hands from six cardinal perspectives and 12 orientations ($30^\circ$, $60^\circ$, $90^\circ$, $120^\circ$, and $150^\circ$; clockwise and counterclockwise).&lt;/p&gt;

&lt;p&gt;In Experiment 1, participants first performed a task in which they had to move their hand into the specified target position. They then performed another task in which they had to determine whether the specified target position depicted a right hand or a left hand (which was hypothesized to elicit mental simulation). Results showed that response times for the movement task and the left-right judgments were nearly identical, and that they increased as the distance between the original hand position and the target posture increased (in trajectory space). The cases where the RTs were less similar between the two tasks was for awkward and uncommon hand positions, in which case the response times for the left-right judgments tended to be longer (perhaps because people have a less good kinematic model of their hands in those positions).&lt;/p&gt;

&lt;p&gt;Experiment 2 was the same as Experiment 1, except that instead of making left-right judgments, participants were told to imagine moving their hand to the target position, thus explicitly engaging them in mental simulation. The results were pretty much the same as in Experiment 1, suggesting that participants in the left-right judgments were indeed using mental simulation.&lt;/p&gt;

&lt;p&gt;Experiment 3 was a control condition in which Parsons controlled for the time to perceive the target. Under this control, participants had RTs that were about 300ms faster on average, and which were especially faster for positions which were more awkward, suggesting that the awkward positions also take more time to perceive.&lt;/p&gt;

&lt;p&gt;Experiments 4 and 5 had participants make left-right judgments (Experiment 4) or move to the specified target position (Experiment 5) but additionally had participants begin with their hands in different initial positions. The idea was to test whether mental simulation began from a canonical body pose, or from the current body pose. Parsons found that the original position did influence RTs in a way that was similar both for the left-right judgments and for movements.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is such a cool set of experiments. What’s really remarkable about this is that participants can look at a schematized drawing of a hand, map it to their own hand (without looking at their own hand), and make the desired movement. Given the results that people take the same amount of time to make the right-left judgments, or just to explicitly perform the mental simulation, this is fairly strong evidence that people have a detailed mental model of their own body that they can use for motion planning and trajectory optimization.&lt;/p&gt;

&lt;p&gt;One might say that it is not necessarily the case that this mental model need be &lt;em&gt;kinematic&lt;/em&gt;—it could just be that people are determining the pose that they need to reach in configuration space, that they know their current pose in configuration space, and that then they compute some trajectory optimization and execute the resulting plan. But, it wouldn’t really make sense why people would need to do this for left-right judgments: if all it were was that people were trying to reach some point in configuration space, then they would presumably have the information already as to which hand it was. I suppose, though, that even if they have a kinematic model they wouldn’t necessarily &lt;em&gt;need&lt;/em&gt; to do a mental simulation in the left-right task if they’ve already mapped the stimulus onto their body. Or, perhaps, mapping the stimulus onto the body &lt;em&gt;is&lt;/em&gt; the mental simulation? It might be revealing to, instead of controlling for the perception time (as in Experiment 3), have people look at the stimulus and then indicate when they are ready to make the motion. If the perception itself is what is making use of the mental simulation, then that RT should be the same as the RT in the left-right judgments. But that would still just make things more puzzling: how is it that the mental simulation seems to match the actual movement if the point of the mental simulation is to determine how to make the movement?&lt;/p&gt;

&lt;p&gt;To be more precise, in robotics, it seems that the process is something like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Determine initial and goal states in configuration space.&lt;/li&gt;
  &lt;li&gt;Perform trajectory optimization to find a path from the start to the goal.&lt;/li&gt;
  &lt;li&gt;Execute the found trajectory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, one could say that a method of solving the right-left judgment would just be the first step of the above process: if you know the goal state, there is no need to actually compute and execute a trajectory. Perhaps the reason why we see something that looks like computation and execution of the trajectory is that we don’t deterministically compute the goal state. Instead, perhaps we form hypotheses about what the goal state is, and then attempt to compute multiple trajectories under these different hypotheses until we find one. If computing the trajectory is linear in the distance of the trajectory, then this should reflect the observed behavior.&lt;/p&gt;

&lt;p&gt;A further question I thought of relates our mental simulation of action to the mental simulation of the motion of objects. Trajectory optimization in robotics tends to usually be in terms of control (i.e., torques); if the same is true in humans, and our mental simulations reflect this, then does that extend to our mental simulation of other objects? That is, do we mentally simulate the &lt;em&gt;control&lt;/em&gt; of those objects (e.g. in mental simulation do we imagine that they are being rotated &lt;em&gt;by&lt;/em&gt; some entity) or just the kinematic motion of those objects? Presumably, it is at least sometimes the control (e.g. if you imagine grasping a coffee cup and taking a sip of coffee), but presumably at other times it is not (e.g. if you imagine a leaf falling from a tree). In the mental rotation case, my intuition is that it really could be either one. Understanding this distinction is going to be an important question, I think.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 08:32:04 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Parsons1994.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Parsons1994.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Kinematic mental simulations in abduction and deduction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Khemlani2013&quot;&gt;Khemlani, S. S., Mackiewicz, R., Bucciarelli, M., &amp;amp; Johnson-Laird, P. N. (2013). Kinematic mental simulations in abduction and deduction. &lt;i&gt;Proceedings Of the National Academy of Sciences of the United States of America&lt;/i&gt;, &lt;i&gt;110&lt;/i&gt;(42), 16766–71. doi:10.1073/pnas.1316275110&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Khemlani et al. conduct a series of experiments in which they have people solve programming-like problems, come up with algorithmic solutions to those problems, and execute existing algorithms. Their experiments operate in a domain that involves train tracks with a set of cars where the problems are to rearrange the cars by sliding them to different sections of the tracks. They also propose a model of how people solve these types of tasks, based on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Johnson-Laird2012.html&quot;&gt;Johnson-Laird’s model theory&lt;/a&gt; but with the extension to &lt;em&gt;kinematic&lt;/em&gt; mental models. Importantly, there are three assumptions that they make about the way in which people use mental models:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mental models are iconic (i.e., they have the same form as the thing they represent)&lt;/li&gt;
  &lt;li&gt;Kinematic mental models are based in time (i.e., the sequence of operations that they simulate are thus ordered in time)&lt;/li&gt;
  &lt;li&gt;Mental models can be schematic (i.e., not necessarily a visual mental image)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The kinematic mental models theory makes several predictions about people’s performance on programming-like tasks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Solutions that involve more steps, and steps that operate on more objects (operands), will take longer and be more prone to error. Thus a solution that takes 5 steps each operating on a single car should be faster than one that takes 7, but also faster than one that takes 5 steps each operating on two cars.&lt;/li&gt;
  &lt;li&gt;People should find it easier to generate algorithms that use while loops than algorithms that use for loops, because “naive individuals use simulations to abduce algorithms”. (It’s not really clear to me how this follows?)&lt;/li&gt;
  &lt;li&gt;People should find it more difficult to abduce algorithms that have higher Kolmogorov complexity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Experiment 1 (“problem solving”) had participants solve rearrangement problems. They were able to actually move the cars on the track using a computer interface. Khemlani et al. found that participants made more moves when their model made more moves, and that participants also made more moves as the number of operands increased. They found similar results with response times.&lt;/p&gt;

&lt;p&gt;Experiment 2 (“abduction”) had participants come up with an algorithm to solve each problem (i.e. a description of how to solve the problem, in English). They for each type of problem, they had to solve it for 8 cars, and then for an unspecified number of cars. Participants were close to ceiling in coming up with algorithms for the 8 car problems, but varied on the problems that had an unspecified number of cars (where, as predicted, solutions with a higher Kolgomorov complexity had a lower success rate). Participants also used while loops more frequently.&lt;/p&gt;

&lt;p&gt;Experiment 3 (“deduction”) gave participants a description of the procedure, the intial state of the trains, and asked them to determine what the end result would be after executing the procedure. They attempted to control for amount of information in the descriptions, by making sure that each description was the same length (number of words). They again found that people’s success was correlated with the algorithm’s Kolmogorov complexity.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Creation of an algorithm (abduction) involves three steps: first computing solutions to two specific problems, then recovering the loop that must be performed, and then converting the structure of the solution into a verbal description. The specific solutions are solved using a “partial means-ends analysis”, in which the problem is broken down into subgoals, where the first goal is to get the rightmost car on the right track, and so on. The way their model recovers the loop is to first find repeated sequences of at least two moves in both of the solutions to the specific problems, and then either:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the case of a for loop, solve a system of linear equations based on the solutions to the two specific problems.&lt;/li&gt;
  &lt;li&gt;In the case of a while loop, determine the condition that needs to be satisified for the while loop to halt.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To compute Kolmogorov complexity of an algorithm, they simply take the number of characters in the LISP program and multiplied by the number of bits for each character.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a really cool exploration of how people reason about more structured plans such as algorithms. It would be really interesting to see if you coded people’s algorithms in Experiment 2 into actual code, how well they corresponded to the algorithms produced by mAbducer, rather than just comparing the success rate. That is, are people actually coming up with solutions like those predicted by the model theory?&lt;/p&gt;

&lt;p&gt;I also wonder how well an approach based on some type of grammar would work (e.g. something like Kevin Ellis’ paper from NIPS 2015, “Unsupervised Learning by Program Synthesis”). I also wonder how their mAbducer program compares to typical approaches to program induction in general. Are there key differences that the model theory predicts that general program induction algorithms would not predict?&lt;/p&gt;

&lt;p&gt;Khemlani et al. use a very specific notion of “simulation” in this paper, which is essentially the simulation of a computer program: the sequential application of known rules beginning with a given initial state. There are other types of computations that they assume, too (e.g. that are used to solve the problems in the first place), but once specific instances of the programs have been solved, simulations of those programs are used to abduce a general solution to the problem. Simulations of the general solution are then used to perform deduction on new problems.&lt;/p&gt;

&lt;p&gt;One potential issue with the third experiment is that participants didn’t actually come up with the solutions themselves. I wonder if there is something important about the way that people form their mental models: it might be that you can’t just give them a description of the program, but that they actually need to abduce it themselves. If this were the case, I wonder if the accuracies at solving the deduction problems would be higher.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 06:06:28 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/05/Khemlani2013.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/05/Khemlani2013.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Inference with mental models</title>
        <description>&lt;p&gt;&lt;span id=&quot;Johnson-Laird2012&quot;&gt;Johnson-Laird, P. N. (2012). Inference with Mental Models. In &lt;i&gt;The Oxford Handbook of Thinking and Reasoning&lt;/i&gt; (pp. 134–145). doi:10.1093/oxfordhb/9780199734689.001.0001&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this chapter, Johnson-Laird describes the theory of mental models:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Perception yields models of the world that lies outside us. An understanding of discourse yields models of the world that the speaker describes for us. And thinking, which enables us to anticipate the world and to choose a course of action, relies on internal manipulations of these mental models. The present chapter is about this theory, which it refers to as the &lt;em&gt;model&lt;/em&gt; theory, and about its experimental corroborations. (pg. 134)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Johnson-Laird first defines what a mental model is. In particular, it follows three principles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mental models are iconic; that is, they have the same structure of the thing the model represents.&lt;/li&gt;
  &lt;li&gt;Each mental model is the simplest way of describing a possibility (and thus implicitly incorporates other irrelevant possibilities).&lt;/li&gt;
  &lt;li&gt;Mental models only represent what is possible, and not what is impossible.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;deduction&quot;&gt;Deduction&lt;/h2&gt;

&lt;p&gt;Next, Johnson-Laird describes how model theory applies to deductive reasoning, and gives empirical evidence for several predictions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fewer and simpler models require less time to process.&lt;/li&gt;
  &lt;li&gt;People will overlook certain models of premises (e.g. because mental models encode implicit information) and thus produce certain errors.&lt;/li&gt;
  &lt;li&gt;People can detect invalid inferences by coming up with counterexamples. Johnson-Laird emphasizes that there are two types of invalid inferences: first, that the conclusion is inconsistent with the premises (people seem to just be able to detect this inconsistency), and second, the conclusion is consistent with the premises but does not follow from them (people come up with a counterexample).&lt;/li&gt;
  &lt;li&gt;People confidently make “illusory” inferences which are invalid. In particular, “when they think about the truth of one assertion, they fail to think about the consequences of the falsity of other assertions”.&lt;/li&gt;
  &lt;li&gt;People who have never performed these sorts of deductions will develop their own strategies for doing so automatically.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;probabilistic-reasoning&quot;&gt;Probabilistic reasoning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;intentional reasoning&lt;/strong&gt;: people use heuristics to infer the probability of an event from evidence&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;extensional reasoning&lt;/strong&gt;: people infer the probability of an event based on different ways the event might occur&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;principle of equiprobability&lt;/strong&gt;: mental models are uniformly equally probable (unless there is reason to believe otherwise)&lt;/p&gt;

&lt;h2 id=&quot;induction&quot;&gt;Induction&lt;/h2&gt;

&lt;p&gt;Johnson-Laird argues that there are two systems of inductive reasoning: one which is &lt;em&gt;intuitive&lt;/em&gt; (system 1) and which has no access to working memory (and thus can only make fast, but potentially erroneous inferences), and one which is &lt;em&gt;explicit&lt;/em&gt; (system 2) and which is “slow, voluntary, and conscious”. For intuitive inductions, they can be affected by &lt;em&gt;modulation&lt;/em&gt;, which is the idea that “the meanings of clauses, coreferential links between them, general knowledge, and knowledge of context, can modulate the core meanings of sentential connectives” (pg. 146). This can affect things like property relations (e.g. “if the dish is kidney beans, then its basis is beans” vs. “if the dish is made of meat, then it can be Portugese stew”, in which case not-A and B is ok in the first case but not the second) and also temporal relations (e.g. “if she put the book on the shelf, then it fell off”).&lt;/p&gt;

&lt;h2 id=&quot;abduction&quot;&gt;Abduction&lt;/h2&gt;

&lt;p&gt;Abduction is the idea of going beyond just inference (i.e. concluding something beyond the information given) to actually inferring a theory or explanation for the conclusions. For example, when asked “If the trigger is pulled, then the pistol will fire. The trigger is pulled, but the pistol does not fire. Why not?”, people seem to augment their mental models in order to find a set of premises that does make the conclusions valid, such as that there were no bullets in the pistol.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I spent a long time thinking about the illusory inferences, and I’m dissatisfied with the explanation. Here is an example of one, where only one of the propositions is true:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If there is a king then there is an ace or else if there isn’t a king then there is an ace.
There is a king.
What follows?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;People invariably say “there is an ace”, but this is technically incorrect. The reason is that the way this is supposed to be parsed is something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;P1: king --&amp;gt; ace
P2: not king --&amp;gt; ace
P1 xor P2
king
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Because only one of P1 or P2 can be true, that means if P1 were false then &lt;code class=&quot;highlighter-rouge&quot;&gt;king --&amp;gt; not ace&lt;/code&gt; and therefore “there is an ace” is not a valid conclusion. Johnson-Laird claims this is based on how people construct their mental models, but I think it has more to do with the language of the scenario. When I write it out as I did just above, the answer is much more obvious—it took me a long time to realize why it was obvious because I was actually parsing the entire situation incorrectly. Perhaps it is just my programming background, but I intuitively parsed this statement into a program like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;king = True
if king:
    ace = True
elif not king:
    ace = True
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Clearly, at the end of execution, &lt;code class=&quot;highlighter-rouge&quot;&gt;ace&lt;/code&gt; will be true. I think the effect would be much lessened if instead the problem were given like:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There are two rules, only one of which is true: (1) a king implies an ace, or (2) lack of a king implies an ace.
There is a king.
What follows?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An informal test of $n=1$ suggests this is a better phrasing, as they correctly determined that there is nothing you can deduce here. Looking it up, I see that Johnson-Laird did test for exactly this in &lt;a href=&quot;http://mentalmodels.princeton.edu/papers/1999illusory.pdf&quot;&gt;Experiment 2 of this paper&lt;/a&gt;. They did find that more people got the correct answer when they changed the language (25% vs 0%), though the majority still made the error.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 13:28:36 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Johnson-Laird2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Johnson-Laird2012.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
  </channel>
</rss>
