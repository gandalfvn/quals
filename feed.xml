<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 06 Jan 2016 23:20:18 -0800</pubDate>
    <lastBuildDate>Wed, 06 Jan 2016 23:20:18 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Physical imagery: kinematic versus dynamic models</title>
        <description>&lt;p&gt;(missing reference)&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Schwartz asks: is physical imagery based on kinematics or dynamics? Specifically, does it only rely on spatial information (kinematic model, KM), or does it also incorporate information about things like forces (dynamic model, DM)? Schwartz shows through a series of four experiments that physical imagery is consistent with the dynamics account.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, participants completed two tasks in one of two orders. One task was the “judge” task, which was to view two differently size glasses of water (with the same level of water) and to determine which glass needed to tilt farther for the water to spill out. The other task was the “tilt” task, in which participants physically rotated empty glasses with their eyes closed and imagined the water in the glasses. The hypothesis was that if people were using KM, if people performed the “judge” task before the “tilt” task, then their judgments should affect the outcome of the tilt (as the tilt would just be based on a spatial outcome). If people were using DM, then doing the “judge” task before the “tilt” task should interfere with people’s mental imagery (which based on previous work is accurate for tilting but inaccurate for judging). They found the results that were consistent with DM: when tilting first, people (correctly) tilted the thin glass further, but when judging first, they either tilted the wide glass further or tilted the two glasses the same. People were almost entirely incorrect in the explicit judgments, and judgments did not correspond to tilts.&lt;/p&gt;

&lt;p&gt;In Experiment 2, participant performed the “tilt” task but were told to either imagine that the liquid in the glass was water or molasses. The hypothesis was that if people are using KM, then the type of liquid should have no effect; if they are using DM, then they should tilt the molasses glasses further, because the molasses moves more slowly than water. The results were again consistent with the DM account. People tilted the thin glasses further than wide glasses (as expected) and also turned the molasses glasses further than the water glasses. Schwartz makes an important point in the discussion of this experiment:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;People rely on the temporal coordinations of physical imagery to allow inferences to emerge; they do not first decide what the inference should be and then adjust the timing of things to portray that inference. (pg. 449)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In Experiment 3, Schwartz had people tilt glasses normally, or tilt them starting from a horizontal position. He also had people perform the tasks lying down. In addition to the tilting task, Schwartz had people rate the quality of their mental imagery, in order to gauge how well people were able to imagine the water in the glass. The idea was to see whether gravity would have an effect on people’s ability to perform the task. He found that, as before, people were able to perform the task when both they and the glass were upright. Interestingly, people could also perform the task if they were lying down, provided the glass was still upright with respect to gravity. They could not perform the task if the glass was horizontal, saying things like “the water began to pour out when I started to tilt the glass” (pg. 452). In terms of the quality of their imagery when the glass was sideways, it was typically high initially (before they started tilting), but image quality degraded as they began the glass tilt. These results also support the DM account.&lt;/p&gt;

&lt;p&gt;Experiment 4 looked at another manipulation to test how perceptual information affects imagery. In this experiment, Schwartz had regular glasses and weighted glasses, and hypothesized that people would turn the weighted cups less than the regular cups because the extra weight from the glass introduces a torque that increases as the amount of rotation increases. In particular, as the water level decreases, people should increasingly under rotate because they have to rotate further into the torque. As predicted, this is what Schwartz found. He also ran a control version in which he had people rotate the glasses to a specified rotation (without water, 45 degrees). People were able to perform this task nearly perfectly, suggesting that the results from the main experiment were not due to people’s inability to represent the angle of the glass. Rather, Schwartz suggests that the effect is due to a relationship between the rate of work exerted in turning the glass and rate of change of the water. As the class is turned, the rate of work increases, causing the water to change more quickly, and thus causing people to underrotate.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;These experiments indicate that dynamic information is clearly incorporated in our ability to visualize objects and our actions on those objects. Schwartz makes the argument that, contrary to other mental imagery accounts, we do not represent transformations by computing the target spatial orientation, but that we represent the control and apply that control until the orientation is achieved. I like this account, but I still don’t quite know how it fits into the mental &lt;em&gt;rotation&lt;/em&gt; account. It cannot be that people just randomly pick a direction of rotation, as then their response times would average out to be constant.&lt;/p&gt;

&lt;p&gt;Perhaps, as argued by &lt;a href=&quot;/quals/mental%20imagery/2015/12/31/Just1976.html&quot;&gt;Just and Carpenter&lt;/a&gt;, people do rely on some sort of feature matching in order to determine the direction of rotation—but not the final orientation. Then, they apply the relevant control in order to move the shape in that direction until they match the correct orientation. It’s still not clear to me exactly how you would tell if you’ve reached the correct orientation… I suppose if people only rotate one part of the shape, then that local piece would be easy to compare. Then, once the local rotation is found, people presumably know what the angle is and can rotate the rest of the shape to that angle, and don’t necessarily need to provide the control.&lt;/p&gt;

&lt;p&gt;The control account is very compelling, but I wonder if there are really some cases where we use purely visual imagery, and other cases where we use dynamic imagery. For example, in the mental rotation case I just described, could one component of that be using dynamic imagery, while another component just uses spatial imagery? Would it be possible to test for this? Do these two cases differ in important ways (i.e., does it really matter if we only use one or the other)? I would expect that it does. I have read a lot of stuff in robotics that is based on knowing the goal state and applying control to get there, but I have read less about simply applying control until some conditions are satisfied (i.e. it is not explicitly a goal state in terms of pose). I need to think more about how these two things are different (or if they are different at all). Perhaps the latter isn’t really actually that different—it’s just that some higher level planner is making the goal states be not very far away from the current state.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 14:33:50 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning/2016/01/06/Schwartz1999.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning/2016/01/06/Schwartz1999.html</guid>
        
        
        <category>Physical reasoning</category>
        
      </item>
    
      <item>
        <title>Representing statics as forces in equilibrium</title>
        <description>&lt;p&gt;&lt;span id=&quot;Freyd1988&quot;&gt;Freyd, J. J., Pantzer, T. M., &amp;amp; Cheng, J. L. (1988). Representing Statics as Forces in Equilibrium. &lt;i&gt;Journal Of Experimental Psychology: General&lt;/i&gt;, &lt;i&gt;117&lt;/i&gt;, 395–407. doi:dx.doi.org/10.1037/0096-3445.117.4.395&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Freyd et al. hypothesize that the mind represents dynamic forces, even for static scenes. In particular, they point out that in static scenes, it is not truly the case that there are no forces: it is just that forces are in equilibrium. They run a series of four experiments to test this hypothesis, finding that people’s memory for static objects is distorted in the direction that those objects would move if they were unsupported.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, Freyd et al. showed participants a scene with a plant either on a table or hanging from a hook for 250ms. The scene was removed for 250ms at which point an identical scene was showed for 250ms, except that the table or hook was removed. The scene was again removed for 250ms, and then a new scene was shown which was either (1) the same, (2) different, with the plant moved slightly upwards, or (3) different, with the plant moved slightly downwards. Participants had to determine whether the last scene was the same as the original one or not. They found that people were quite accurate at determining “same” trials (8-13% error) and worse at detecting “down” trials (57% error) than “up” trials (37% error).&lt;/p&gt;

&lt;p&gt;Experiment 2 was a control in which Freyd et al. never showed any form of support. The results showed basically no difference between the “down” trials (33%) vs. the “up” trials (34%). Interestingly, though, the error rate for the “same” trials was higher than in Experiment 1 (26%).&lt;/p&gt;

&lt;p&gt;Experiment 3 was similar to Experiments 1 and 2, except that they used a different display (a lock hanging from a hook) and tested different distances from the true position. This was done in order to get a more fine-grained estimate of the distortion in people’s memories. As in Experiments 1 and 2, they found distortions when there was support, and that the shift was positive (i.e. “down” trials had more error) when there was support and that it was close to zero when there was no support.&lt;/p&gt;

&lt;p&gt;In Experiment 4, Freyd et al. tested another new display, this time with a block resting on a spring. This allowed them to test for distortions in memory both in the upward and downward directions. There were two conditions: one in which there was initially no block (which would predict a downward shift, as the spring should compress), and one in which there was initially a block (which would predict an upward shift, as the spring should decompress). The results supported the predicted direction of memory distortions.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;These are a cool set of results, related to representational momentum, that indicate that people have strong perceptual expectations about the way that objects should move. In particular, I think the results of Experiment 4 are really striking: it’s not just that people expect things to move down due to gravity, but that they expect them to move in the way they actually would. I expect you would find similar results with objects that typically move up (e.g. balloons). I also wonder if you would find these effects if they depended on higher-level knowledge about object properties (such as mass). For example, in the spring experiment, if you knew the block was extremely light (and thus would not compress the spring), would the memory distortion still take place?&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 07:09:25 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning/2016/01/06/Freyd1988.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning/2016/01/06/Freyd1988.html</guid>
        
        
        <category>Physical reasoning</category>
        
      </item>
    
      <item>
        <title>The experience of force: the role of haptic experience of forces in visual perception of object motion and interactions, mental simulation, and motion-related judgments</title>
        <description>&lt;p&gt;&lt;span id=&quot;White2012a&quot;&gt;White, P. A. (2012). The experience of force: The role of haptic experience of forces in visual perception of object motion and interactions, mental simulation, and motion-related judgments. &lt;i&gt;Psychological Bulletin&lt;/i&gt;, &lt;i&gt;138&lt;/i&gt;(4), 589–615. doi:10.1037/a0025587&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, White proposes a theory of action and perception that is based on the notion of force. Specifically, he argues that during our interactions with the world, we perceive force from our haptic system (along with other sensory modalities), and these perceptions get stored in memory along with the relevant actions associated with them. Then, when we perceive new situations, we activate these stored representations which allows us to make predictions and judgments about motion and other factors.&lt;/p&gt;

&lt;p&gt;First, White discusses evidence for forward models of action in the motor system, as well as evidence for the role of mechanoreceptor feedback. What is sounds like he proposes is a sort of forward model like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[\mathbf{x}_{t+1}, \mathbf{s}_{t+1}] = f(\mathbf{x}_t,\mathbf{s}_t,\mathbf{u}_t)&lt;/script&gt;

&lt;p&gt;where $\mathbf{x}$ is the state of the system, $\mathbf{s}$ is the sensory information (e.g. from the haptic system), and $\mathbf{u}$ are the controls (forces) of the system. A prediction error for the sensory information (e.g. mechanoreceptor feedback) is also computed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\delta}_t=\mathbf{s}_t - \hat{\mathbf{s}}_t&lt;/script&gt;

&lt;p&gt;where $\mathbf{s}_t$ is the predicted sensory information and $\hat{\mathbf{s}}_t$ is the true sensory information. The feedback $\mathbf{\delta}_t$ is thus the error signal, which is going to be zero when our predictions of force are accurate. White also argues that perception of additional object properties (texture, rigidity, mass, etc.) are computed based on sensory information from mechanoreceptors. I’ll denote these properties as $\mathbf{\pi}$.&lt;/p&gt;

&lt;p&gt;All of these different sources of information are stored in long-term memory, roughly (it seems) in the form of tuples such as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{m}_t=[\mathbf{x}_t,\mathbf{\delta}_t,\mathbf{\pi}]&lt;/script&gt;

&lt;p&gt;White describes these as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A stored representation of an action on an object is a multimodal episodic trace combining haptic information such as the disposition and movement of the limbs during execution of the action, visual information about body movement and the associated motion of the object acted on, auditory information such as sounds elicited by contact between extremity and object, and in principle, information in any sensory modality. Internally available information such as the content of the forward model also forms part of the representation. (pg. 607)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Importantly, we store our &lt;em&gt;prediction error&lt;/em&gt; of sensory information, rather than the absolute sensory information itself.
These stored representations are activated by matching to similar perceptual stimuli (e.g. visual stimuli).&lt;/p&gt;

&lt;p&gt;White uses this formulation of stored representations to offer a unifying account for several lines of research:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The storing of sensory feedback, rather than direct sensory information, predicts that when making judgments about force in terms of one moving object acting on a stationary, we assign notions of force &lt;em&gt;from&lt;/em&gt; the moving object (because we do store $\mathbf{u}_t$) but not &lt;em&gt;to&lt;/em&gt; the moving object (because, for static objects, the sensory prediction error should be zero). If both objects are moving, however, we should assign a notion of force that the second object is applying to the first object because the sensory prediction error is nonzero. This explains, for example, Michottean launching effects.&lt;/li&gt;
  &lt;li&gt;To the extent that visual perception of motion matches stored representations corresponding to actions, we should perceive that motion as being internally caused. This extends to biological plausibility as well. Importantly, biologically generated motion has different velocity profiles than, for example, two nonbiological objects colliding—thus visual motion that matches the biological motion velocity profile should be interpreted as more biological.&lt;/li&gt;
  &lt;li&gt;Representational momentum&lt;/li&gt;
  &lt;li&gt;Perception of inanimate entities as intentional&lt;/li&gt;
  &lt;li&gt;Mental simulation&lt;/li&gt;
  &lt;li&gt;Perception of mass&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a surprisingly consistent and satisfying account of how perception arises from the combination of visual and haptic feedback. Assuming people do store information as something like $\mathbf{m}_t$ defined above, and they have access to forward an inverse models, it should be possible to reconstruct $\mathbf{u}_t$ (from both $\mathbf{x}_t$ and $\mathbf{x}_{t+1}$), which is consistent with White’s assertions. I am skeptical, though, that all we are doing is “storing” and “matching” representations. It is not at all clear to me how it would work to match the motion of a 2D ball (e.g. in the Michotte experiments) to the stored motion of ourselves. Additionally, it sounds like White is advocating for something like an exemplar model, but I find it much more likely that we use our experiences to build structured forward or inverse models. There may be multiple forward models (as suggested by Kawato) that are perhaps combined in certain ways, but give that there is evidence for some generalization (also described by Kawato), I would be very surprised if all that was going on was just storing and matching exemplars.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 04:47:29 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/06/White2012a.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/06/White2012a.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Prediction precedes control in motor learning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Flanagan2003&quot;&gt;Flanagan, R. R., Vetter, P., Johansson, R. S., &amp;amp; Wolpert, D. M. (2003). Prediction precedes control in motor learning. &lt;i&gt;Current Biology&lt;/i&gt;, &lt;i&gt;13&lt;/i&gt;(2), 146–150. doi:10.1016/S0960-9822(03)00007-1&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Flanagan et al. describe an experiment in which participants must grip an object with their index finger and thumb, and move it in a straight line to a target. The dynamics of the object were modified so that when they moved it in the horizontal plane, a proportional vertical force was applied to the object. Thus, to learn to move it in a straight line, participants had to adapt to the vertical force.&lt;/p&gt;

&lt;p&gt;Flanagan et al. found that participants took a long time (about 70 trials) before they were able to fully adjust their trajectories to be straight. However, they took much less time (about 10 trials) to adjust the force with which they gripped the object to match that of the corresponding load force. These results suggest that there are two internal models (one for the grip force, and one for the arm trajectory) that are being learned at separate rates. Specifically, Flanagan et al. suggest that in the first case, it is a forward kinematic model that is being learned, while in the second case, it is a inverse dynamics model that is being learned. This is consistent with the demands of the task: the novel dynamics of the object require learning a new mapping from desired trajectory to motor commands (the inverse model), but they do not require learning a new mapping for controlling the load force. Rather, the motor system needs only to predict the load force so that it can appropriately adjust for it.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This paper basically answers the question I ended with in &lt;a href=&quot;/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html&quot;&gt;Kawato’s review&lt;/a&gt;: learning operates independently in the forward and inverse models. Flanagan et al. suggest that, computationally, this may be able to be explained by something like &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/20/Nguyen-Tuong2011.html&quot;&gt;distal teacher learning&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 13:30:48 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Flanagan2003.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Flanagan2003.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Internal models for motor control and trajectory planning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kawato1999&quot;&gt;Kawato, M. (1999). Internal models for motor control and trajectory planning. &lt;i&gt;Current Opinions In Neurobiology&lt;/i&gt;, &lt;i&gt;9&lt;/i&gt;(6), 718–727. doi:10.1016/S0959-4388(99)00028-8&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this review article, Kawato discusses the role of internal models in motor control. He argues that both forward and inverse internal models are used in motor control. In particular:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Fast and coordinated arm movements cannot be executed solely under feedback control, since biological feedback loops are slow and have small gains. Thus, the internal model hypothesis proposes that the brain needs to acquire an inverse dynamics model of the object to be controlled through motor learning, after which motor control can be executed in a pure feedforward manner.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, Kawato gives an overview of the existence of internal models. One piece of evidence comes from perturbing the dynamics of participants’ motions. Initially, people make the wrong movements under these novel dynamics. However, they eventually adapt and can make the correct motion. If the new dynamics are removed, then they again make errors because they are now using an incorrect model of the inverse dynamics. Another piece of evidence comes from &lt;em&gt;grip-force—load-force coupling&lt;/em&gt;, which is the coupling of a grip force (e.g. thumb and index finger) and load force (e.g. weight of the object that is being held). When gripping an object like this, and moving one’s arm to a new location, the motor system must both determing the appropriate trajectory for the arm as well as the grip force needed to hold the object. The way this works is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The inverse model of the combined dynamics of the arm, hand, and object calculates the necessary motor commands from the desired trajectory of the arm. These commands are sent to the arm muscles as well as to the forward dynamics model as the efference copy. Then, the forward model can predict an arm trajectory that is slightly in the future. Given the predicted arm trajectory, the load force is calculated; then, by multiplying a friction coefficient and a safety factor, the necessary minimum level of grip force can be calculated.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next, Kawato discusses neurological evidence for internal models in the cerebellum. I’m not going to go into detail on this.&lt;/p&gt;

&lt;p&gt;Next, Kawato discusses the structure of internal models. Specifically, is it that internal models are just implemented as mappings between states and actions, or are they implemented using some sort of generalizable parameterization? To test this, the “generalization” paradigm is used, in which participants are trained to do a specific task under novel dynamics. They are then instructed to do another task, still under the novel dynamics, to see whether the dynamics have been fully generalized to new situations or not. The results are somewhere in the middle: some generalization occurs, but not perfect generalization.&lt;/p&gt;

&lt;p&gt;Finally, Kawato discusses how the motor system computes trajectories. There are apparently two competing models: kinematic models (e.g. the minimum jerk model) and dynamics models (e.g. the minimum torque-change mdoel). Kawato proposes the &lt;em&gt;minimum variance&lt;/em&gt; model as middle ground between these two models. In the minimum variance model, the objective function minimizes a kinematic value (the variance of the end pose); however, the variance is determined by motor commands, which are part of the dynamics. If this model is correct, then it gives another motivation for there being both be a kinematic internal model (i.e. a forward model) as well as a dynamics internal model (i.e. a inverse dynamics model)&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Based on this article, there is very strong evidence for internal models in the motor system. Moreover, there is evidence for &lt;em&gt;both&lt;/em&gt; forward kinematic models and inverse dynamics models which seem to work in tandem.&lt;/p&gt;

&lt;p&gt;I wonder how these models are learned. In particular, when people’s motions are perturbed in some of the studies that Kawato describes, is it that people are updating their forward models, or their dynamics models, or both? Does learning in one affect the other, or are they independent?&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 11:48:31 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Temporal and kinematic properties of motor behavior reflected in mentally simulated action</title>
        <description>&lt;p&gt;&lt;span id=&quot;Parsons1994&quot;&gt;Parsons, L. M. (1994). Temporal and kinematic properties of motor behavior reflected in mentally simulated action. &lt;i&gt;Journal Of Experimental Psychology: Human Perception and Performance&lt;/i&gt;, &lt;i&gt;20&lt;/i&gt;(4), 709–730. doi:10.1037/0096-1523.20.4.709&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Parsons asks the question: how similar are mental simulations of motor actions to actual motor actions themselves? In a series of five experiments, he demonstrates that they correspond quite closely—though they differ in a few revealing ways—supporting the hyothesis that mental simulations do operate on a detailed model of the body and utilize the same type of trajectory optimization/motion planning that the motor system does.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In all experiments, Parsons showed participants images of left and right hands from six cardinal perspectives and 12 orientations ($30^\circ$, $60^\circ$, $90^\circ$, $120^\circ$, and $150^\circ$; clockwise and counterclockwise).&lt;/p&gt;

&lt;p&gt;In Experiment 1, participants first performed a task in which they had to move their hand into the specified target position. They then performed another task in which they had to determine whether the specified target position depicted a right hand or a left hand (which was hypothesized to elicit mental simulation). Results showed that response times for the movement task and the left-right judgments were nearly identical, and that they increased as the distance between the original hand position and the target posture increased (in trajectory space). The cases where the RTs were less similar between the two tasks was for awkward and uncommon hand positions, in which case the response times for the left-right judgments tended to be longer (perhaps because people have a less good kinematic model of their hands in those positions).&lt;/p&gt;

&lt;p&gt;Experiment 2 was the same as Experiment 1, except that instead of making left-right judgments, participants were told to imagine moving their hand to the target position, thus explicitly engaging them in mental simulation. The results were pretty much the same as in Experiment 1, suggesting that participants in the left-right judgments were indeed using mental simulation.&lt;/p&gt;

&lt;p&gt;Experiment 3 was a control condition in which Parsons controlled for the time to perceive the target. Under this control, participants had RTs that were about 300ms faster on average, and which were especially faster for positions which were more awkward, suggesting that the awkward positions also take more time to perceive.&lt;/p&gt;

&lt;p&gt;Experiments 4 and 5 had participants make left-right judgments (Experiment 4) or move to the specified target position (Experiment 5) but additionally had participants begin with their hands in different initial positions. The idea was to test whether mental simulation began from a canonical body pose, or from the current body pose. Parsons found that the original position did influence RTs in a way that was similar both for the left-right judgments and for movements.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is such a cool set of experiments. What’s really remarkable about this is that participants can look at a schematized drawing of a hand, map it to their own hand (without looking at their own hand), and make the desired movement. Given the results that people take the same amount of time to make the right-left judgments, or just to explicitly perform the mental simulation, this is fairly strong evidence that people have a detailed mental model of their own body that they can use for motion planning and trajectory optimization.&lt;/p&gt;

&lt;p&gt;One might say that it is not necessarily the case that this mental model need be &lt;em&gt;kinematic&lt;/em&gt;—it could just be that people are determining the pose that they need to reach in configuration space, that they know their current pose in configuration space, and that then they compute some trajectory optimization and execute the resulting plan. But, it wouldn’t really make sense why people would need to do this for left-right judgments: if all it were was that people were trying to reach some point in configuration space, then they would presumably have the information already as to which hand it was. I suppose, though, that even if they have a kinematic model they wouldn’t necessarily &lt;em&gt;need&lt;/em&gt; to do a mental simulation in the left-right task if they’ve already mapped the stimulus onto their body. Or, perhaps, mapping the stimulus onto the body &lt;em&gt;is&lt;/em&gt; the mental simulation? It might be revealing to, instead of controlling for the perception time (as in Experiment 3), have people look at the stimulus and then indicate when they are ready to make the motion. If the perception itself is what is making use of the mental simulation, then that RT should be the same as the RT in the left-right judgments. But that would still just make things more puzzling: how is it that the mental simulation seems to match the actual movement if the point of the mental simulation is to determine how to make the movement?&lt;/p&gt;

&lt;p&gt;To be more precise, in robotics, it seems that the process is something like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Determine initial and goal states in configuration space.&lt;/li&gt;
  &lt;li&gt;Perform trajectory optimization to find a path from the start to the goal.&lt;/li&gt;
  &lt;li&gt;Execute the found trajectory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, one could say that a method of solving the right-left judgment would just be the first step of the above process: if you know the goal state, there is no need to actually compute and execute a trajectory. Perhaps the reason why we see something that looks like computation and execution of the trajectory is that we don’t deterministically compute the goal state. Instead, perhaps we form hypotheses about what the goal state is, and then attempt to compute multiple trajectories under these different hypotheses until we find one. If computing the trajectory is linear in the distance of the trajectory, then this should reflect the observed behavior.&lt;/p&gt;

&lt;p&gt;A further question I thought of relates our mental simulation of action to the mental simulation of the motion of objects. Trajectory optimization in robotics tends to usually be in terms of control (i.e., torques); if the same is true in humans, and our mental simulations reflect this, then does that extend to our mental simulation of other objects? That is, do we mentally simulate the &lt;em&gt;control&lt;/em&gt; of those objects (e.g. in mental simulation do we imagine that they are being rotated &lt;em&gt;by&lt;/em&gt; some entity) or just the kinematic motion of those objects? Presumably, it is at least sometimes the control (e.g. if you imagine grasping a coffee cup and taking a sip of coffee), but presumably at other times it is not (e.g. if you imagine a leaf falling from a tree). In the mental rotation case, my intuition is that it really could be either one. Understanding this distinction is going to be an important question, I think.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 08:32:04 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Parsons1994.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Parsons1994.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Kinematic mental simulations in abduction and deduction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Khemlani2013&quot;&gt;Khemlani, S. S., Mackiewicz, R., Bucciarelli, M., &amp;amp; Johnson-Laird, P. N. (2013). Kinematic mental simulations in abduction and deduction. &lt;i&gt;Proceedings Of the National Academy of Sciences of the United States of America&lt;/i&gt;, &lt;i&gt;110&lt;/i&gt;(42), 16766–71. doi:10.1073/pnas.1316275110&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Khemlani et al. conduct a series of experiments in which they have people solve programming-like problems, come up with algorithmic solutions to those problems, and execute existing algorithms. Their experiments operate in a domain that involves train tracks with a set of cars where the problems are to rearrange the cars by sliding them to different sections of the tracks. They also propose a model of how people solve these types of tasks, based on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Johnson-Laird2012.html&quot;&gt;Johnson-Laird’s model theory&lt;/a&gt; but with the extension to &lt;em&gt;kinematic&lt;/em&gt; mental models. Importantly, there are three assumptions that they make about the way in which people use mental models:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mental models are iconic (i.e., they have the same form as the thing they represent)&lt;/li&gt;
  &lt;li&gt;Kinematic mental models are based in time (i.e., the sequence of operations that they simulate are thus ordered in time)&lt;/li&gt;
  &lt;li&gt;Mental models can be schematic (i.e., not necessarily a visual mental image)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The kinematic mental models theory makes several predictions about people’s performance on programming-like tasks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Solutions that involve more steps, and steps that operate on more objects (operands), will take longer and be more prone to error. Thus a solution that takes 5 steps each operating on a single car should be faster than one that takes 7, but also faster than one that takes 5 steps each operating on two cars.&lt;/li&gt;
  &lt;li&gt;People should find it easier to generate algorithms that use while loops than algorithms that use for loops, because “naive individuals use simulations to abduce algorithms”. (It’s not really clear to me how this follows?)&lt;/li&gt;
  &lt;li&gt;People should find it more difficult to abduce algorithms that have higher Kolmogorov complexity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Experiment 1 (“problem solving”) had participants solve rearrangement problems. They were able to actually move the cars on the track using a computer interface. Khemlani et al. found that participants made more moves when their model made more moves, and that participants also made more moves as the number of operands increased. They found similar results with response times.&lt;/p&gt;

&lt;p&gt;Experiment 2 (“abduction”) had participants come up with an algorithm to solve each problem (i.e. a description of how to solve the problem, in English). They for each type of problem, they had to solve it for 8 cars, and then for an unspecified number of cars. Participants were close to ceiling in coming up with algorithms for the 8 car problems, but varied on the problems that had an unspecified number of cars (where, as predicted, solutions with a higher Kolgomorov complexity had a lower success rate). Participants also used while loops more frequently.&lt;/p&gt;

&lt;p&gt;Experiment 3 (“deduction”) gave participants a description of the procedure, the intial state of the trains, and asked them to determine what the end result would be after executing the procedure. They attempted to control for amount of information in the descriptions, by making sure that each description was the same length (number of words). They again found that people’s success was correlated with the algorithm’s Kolmogorov complexity.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Creation of an algorithm (abduction) involves three steps: first computing solutions to two specific problems, then recovering the loop that must be performed, and then converting the structure of the solution into a verbal description. The specific solutions are solved using a “partial means-ends analysis”, in which the problem is broken down into subgoals, where the first goal is to get the rightmost car on the right track, and so on. The way their model recovers the loop is to first find repeated sequences of at least two moves in both of the solutions to the specific problems, and then either:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the case of a for loop, solve a system of linear equations based on the solutions to the two specific problems.&lt;/li&gt;
  &lt;li&gt;In the case of a while loop, determine the condition that needs to be satisified for the while loop to halt.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To compute Kolmogorov complexity of an algorithm, they simply take the number of characters in the LISP program and multiplied by the number of bits for each character.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a really cool exploration of how people reason about more structured plans such as algorithms. It would be really interesting to see if you coded people’s algorithms in Experiment 2 into actual code, how well they corresponded to the algorithms produced by mAbducer, rather than just comparing the success rate. That is, are people actually coming up with solutions like those predicted by the model theory?&lt;/p&gt;

&lt;p&gt;I also wonder how well an approach based on some type of grammar would work (e.g. something like Kevin Ellis’ paper from NIPS 2015, “Unsupervised Learning by Program Synthesis”). I also wonder how their mAbducer program compares to typical approaches to program induction in general. Are there key differences that the model theory predicts that general program induction algorithms would not predict?&lt;/p&gt;

&lt;p&gt;Khemlani et al. use a very specific notion of “simulation” in this paper, which is essentially the simulation of a computer program: the sequential application of known rules beginning with a given initial state. There are other types of computations that they assume, too (e.g. that are used to solve the problems in the first place), but once specific instances of the programs have been solved, simulations of those programs are used to abduce a general solution to the problem. Simulations of the general solution are then used to perform deduction on new problems.&lt;/p&gt;

&lt;p&gt;One potential issue with the third experiment is that participants didn’t actually come up with the solutions themselves. I wonder if there is something important about the way that people form their mental models: it might be that you can’t just give them a description of the program, but that they actually need to abduce it themselves. If this were the case, I wonder if the accuracies at solving the deduction problems would be higher.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 06:06:28 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/05/Khemlani2013.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/05/Khemlani2013.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Inference with mental models</title>
        <description>&lt;p&gt;&lt;span id=&quot;Johnson-Laird2012&quot;&gt;Johnson-Laird, P. N. (2012). Inference with Mental Models. In &lt;i&gt;The Oxford Handbook of Thinking and Reasoning&lt;/i&gt; (pp. 134–145). doi:10.1093/oxfordhb/9780199734689.001.0001&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this chapter, Johnson-Laird describes the theory of mental models:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Perception yields models of the world that lies outside us. An understanding of discourse yields models of the world that the speaker describes for us. And thinking, which enables us to anticipate the world and to choose a course of action, relies on internal manipulations of these mental models. The present chapter is about this theory, which it refers to as the &lt;em&gt;model&lt;/em&gt; theory, and about its experimental corroborations. (pg. 134)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Johnson-Laird first defines what a mental model is. In particular, it follows three principles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mental models are iconic; that is, they have the same structure of the thing the model represents.&lt;/li&gt;
  &lt;li&gt;Each mental model is the simplest way of describing a possibility (and thus implicitly incorporates other irrelevant possibilities).&lt;/li&gt;
  &lt;li&gt;Mental models only represent what is possible, and not what is impossible.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;deduction&quot;&gt;Deduction&lt;/h2&gt;

&lt;p&gt;Next, Johnson-Laird describes how model theory applies to deductive reasoning, and gives empirical evidence for several predictions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fewer and simpler models require less time to process.&lt;/li&gt;
  &lt;li&gt;People will overlook certain models of premises (e.g. because mental models encode implicit information) and thus produce certain errors.&lt;/li&gt;
  &lt;li&gt;People can detect invalid inferences by coming up with counterexamples. Johnson-Laird emphasizes that there are two types of invalid inferences: first, that the conclusion is inconsistent with the premises (people seem to just be able to detect this inconsistency), and second, the conclusion is consistent with the premises but does not follow from them (people come up with a counterexample).&lt;/li&gt;
  &lt;li&gt;People confidently make “illusory” inferences which are invalid. In particular, “when they think about the truth of one assertion, they fail to think about the consequences of the falsity of other assertions”.&lt;/li&gt;
  &lt;li&gt;People who have never performed these sorts of deductions will develop their own strategies for doing so automatically.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;probabilistic-reasoning&quot;&gt;Probabilistic reasoning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;intentional reasoning&lt;/strong&gt;: people use heuristics to infer the probability of an event from evidence&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;extensional reasoning&lt;/strong&gt;: people infer the probability of an event based on different ways the event might occur&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;principle of equiprobability&lt;/strong&gt;: mental models are uniformly equally probable (unless there is reason to believe otherwise)&lt;/p&gt;

&lt;h2 id=&quot;induction&quot;&gt;Induction&lt;/h2&gt;

&lt;p&gt;Johnson-Laird argues that there are two systems of inductive reasoning: one which is &lt;em&gt;intuitive&lt;/em&gt; (system 1) and which has no access to working memory (and thus can only make fast, but potentially erroneous inferences), and one which is &lt;em&gt;explicit&lt;/em&gt; (system 2) and which is “slow, voluntary, and conscious”. For intuitive inductions, they can be affected by &lt;em&gt;modulation&lt;/em&gt;, which is the idea that “the meanings of clauses, coreferential links between them, general knowledge, and knowledge of context, can modulate the core meanings of sentential connectives” (pg. 146). This can affect things like property relations (e.g. “if the dish is kidney beans, then its basis is beans” vs. “if the dish is made of meat, then it can be Portugese stew”, in which case not-A and B is ok in the first case but not the second) and also temporal relations (e.g. “if she put the book on the shelf, then it fell off”).&lt;/p&gt;

&lt;h2 id=&quot;abduction&quot;&gt;Abduction&lt;/h2&gt;

&lt;p&gt;Abduction is the idea of going beyond just inference (i.e. concluding something beyond the information given) to actually inferring a theory or explanation for the conclusions. For example, when asked “If the trigger is pulled, then the pistol will fire. The trigger is pulled, but the pistol does not fire. Why not?”, people seem to augment their mental models in order to find a set of premises that does make the conclusions valid, such as that there were no bullets in the pistol.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I spent a long time thinking about the illusory inferences, and I’m dissatisfied with the explanation. Here is an example of one, where only one of the propositions is true:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If there is a king then there is an ace or else if there isn’t a king then there is an ace.
There is a king.
What follows?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;People invariably say “there is an ace”, but this is technically incorrect. The reason is that the way this is supposed to be parsed is something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;P1: king --&amp;gt; ace
P2: not king --&amp;gt; ace
P1 xor P2
king
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Because only one of P1 or P2 can be true, that means if P1 were false then &lt;code class=&quot;highlighter-rouge&quot;&gt;king --&amp;gt; not ace&lt;/code&gt; and therefore “there is an ace” is not a valid conclusion. Johnson-Laird claims this is based on how people construct their mental models, but I think it has more to do with the language of the scenario. When I write it out as I did just above, the answer is much more obvious—it took me a long time to realize why it was obvious because I was actually parsing the entire situation incorrectly. Perhaps it is just my programming background, but I intuitively parsed this statement into a program like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;king = True
if king:
    ace = True
elif not king:
    ace = True
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Clearly, at the end of execution, &lt;code class=&quot;highlighter-rouge&quot;&gt;ace&lt;/code&gt; will be true. I think the effect would be much lessened if instead the problem were given like:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There are two rules, only one of which is true: (1) a king implies an ace, or (2) lack of a king implies an ace.
There is a king.
What follows?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An informal test of $n=1$ suggests this is a better phrasing, as they correctly determined that there is nothing you can deduce here. Looking it up, I see that Johnson-Laird did test for exactly this in &lt;a href=&quot;http://mentalmodels.princeton.edu/papers/1999illusory.pdf&quot;&gt;Experiment 2 of this paper&lt;/a&gt;. They did find that more people got the correct answer when they changed the language (25% vs 0%), though the majority still made the error.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 13:28:36 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Johnson-Laird2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Johnson-Laird2012.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Qualitative modeling</title>
        <description>&lt;p&gt;&lt;span id=&quot;Forbus2011&quot;&gt;Forbus, K. D. (2011). Qualitative modeling. &lt;i&gt;Wiley Interdisciplinary Reviews: Cognitive Science&lt;/i&gt;, &lt;i&gt;2&lt;/i&gt;(4), 374–391. doi:10.1002/wcs.115&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this article, Forbus summarizes the state of the field of qualitative reasoning, explaining how qualitative reasoning works and what it can be applied to. He begins by outlining three principles that are core to qualitative modeling:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Discretization&lt;/em&gt; – qualitative representations are almost always discrete.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Relevance&lt;/em&gt; – the manner in which the discretization is chosen (an in general how the model is set up) depends on the relevance of different aspects of the situation that is being modeled.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ambiguity&lt;/em&gt; – predictions made by qualitative models are ambiguous and there may be multiple possible predictions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Next, Forbus describes what a qualitative representation is. I won’t go into detail on this, since it’s mostly covered by my notes on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Kuipers1986.html&quot;&gt;Kuipers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Given the definitions for qualitative representations, Forbus describes the qualitative mathematical operations that can be applied to them. One of &lt;em&gt;qualitative proportionality&lt;/em&gt;, which is that “all else being equal, if $B$ increases, then $A$ will increase, and if $B$ decreases, then $A$ will decrease” (pg. 4). To get around the fact that not all functions are monotonic, Forbus describes to options. First, functions can be broken up into &lt;em&gt;model fragments&lt;/em&gt; where monotonicity holds within a given fragment. Second, a &lt;em&gt;compositional modeling language&lt;/em&gt; can be used instead. The notation used in the explanation of the compositional modeling language isn’t explained, though, so I’m not entirely clear on how it works. Same thing goes for &lt;em&gt;confluences&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Forbus moves on to describe the pieces of a qualitative model, which include &lt;em&gt;processes&lt;/em&gt; (e.g., heat flow), &lt;em&gt;components&lt;/em&gt; (individual discrete parts that can be combined), and &lt;em&gt;fields&lt;/em&gt; (a division of space in qualitatively distinct regions where some qualitative parameter is constant). These pieces, represented as model fragments, are assembed in the &lt;em&gt;compositional modeling&lt;/em&gt; methodology to form a &lt;em&gt;domain theory&lt;/em&gt;. There are also &lt;em&gt;modeling assumptions&lt;/em&gt; which represent choices that need to be made depending on their relevance to the situation (for example, whether a thermal object should be considered a regular thermal object or just a temperature source).&lt;/p&gt;

&lt;p&gt;The qualitative model can then be used in a qualitative simulation. I also won’t go into details here as I already wrote about it in my notes on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Kuipers1986.html&quot;&gt;Kuipers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Forbus discusses how qualitative modeling can be used to model causality and spatial reasoning. For spatial reasoning, there are a few considerations: topology (which can be represented using a region connection calculus), direction, position, and shape. Space itself also needs to be decomposed into discrete regions and edges; this can be done with something called &lt;em&gt;place vocabularies&lt;/em&gt; (e.g. this is what was used by the FROB system, see &lt;a href=&quot;/quals/mental%20models/2015/12/19/Gentner1983.html&quot;&gt;Chapter 4 of Gentner&lt;/a&gt;).&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The term “simulation” when used in “qualitative simulation” does seem to mean something along the lines of simulation of a physical process—though because it is discrete and approximate, it is perhaps closer to something like the simulations used in Monte-Carlo Tree Search. It would be interesting to see how qualitative simulation would do in certain reinforcement learning settings when combined with something like MCTS. For example, in playing Atari games, maybe something like qualitative simulation would be useful for efficiently determining high-level actions a player could take (rather than the low level actions of left/right/jump/etc).&lt;/p&gt;

&lt;p&gt;One thing that is not clear to me, though, is how qualitative modeling fits into scenarios where there is uncertainty about the environment. In one sense, it’s great at doing that, because you don’t necessarily need to numerically specify every parameter. In another sense, though, it doesn’t seem like qualitative modeling would work well in an uncertain case where, for example, it might be &lt;em&gt;possible&lt;/em&gt; for a situation to unfold in a certain way (depending on the initial conditions), but highly unlikely. It doesn’t seem like qualitative simulation is set up for being able to express the magnitude of uncertainty or quality.&lt;/p&gt;

&lt;p&gt;Also, despite their suggestion to the contrary, qualitative models suffer from many of the same issues that are discussed by &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Davis.html&quot;&gt;Davis &amp;amp; Marcus&lt;/a&gt; as physical simulation does. Importantly, qualitative modeling doesn’t free you from having to make hard choices about &lt;em&gt;how&lt;/em&gt; to set the simulation up, something that Forbus explicitly notes in this paper. You still have to make the relevant decisions about which properties are important, how to do the discretization, etc.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 08:58:54 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Forbus2011.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Forbus2011.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Qualitative simulation</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kuipers1986&quot;&gt;Kuipers, B. (1986). Qualitative Simulation. &lt;i&gt;Artificial Intelligence&lt;/i&gt;, &lt;i&gt;29&lt;/i&gt;(3), 289–338. doi:10.1016/0004-3702(86)90073-1&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Kuipers gives an overview of what qualitative simulation is, how it can be used, and what some its limitations are. He focuses specifically on the QSIM system, which is similar to other qualitative reasoning systems (e.g. from de Kleer, Forbus) but which also has a few differences. Regardless of the specific system, the point of qualitative reasoning is to break down continuous sytems into discrete analogues. Given a set of constraints that the continuous system must follow, along with an initial qualitative state description, qualitative reasoning determines how the system must evolve without necessarily specifying any precise values. For example, qualitative reasoning can be used to infer that if a ball is thrown upwards, it must eventually come down again (but the specific position and velocity of the ball need not be specified).&lt;/p&gt;

&lt;p&gt;Kuipers shows that the QSIM method will always produce the true behavior of the system that it describes, but it may additionally produce false behaviors if the proper constraints aren’t given (e.g. conservation of energy).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;qualitative-behavior&quot;&gt;Qualitative behavior&lt;/h2&gt;

&lt;p&gt;The qualitative description of a continuous and differentiable function $f:[a,b]\rightarrow\mathbb{R}^*$ relies on a set of &lt;em&gt;landmark values&lt;/em&gt;, which include 0, $f(a)$, $f(b)$, and $f(t)$ where $t$ is a critical point of the function. A &lt;em&gt;distinguished time point&lt;/em&gt; is the corresponding $t$ for a landmark value.&lt;/p&gt;

&lt;p&gt;Then, the &lt;em&gt;qualitative state&lt;/em&gt; of the function at a particular point in time is $\mathrm{QS}(f,t)=[\mathrm{qval},\mathrm{qdir}]$, where $\mathrm{qval}$ is either a landmark value or the interval between landmark values, and where $\mathrm{qdir}$ is $\mathrm{sgn}(f^\prime(t))$. The &lt;em&gt;qualitative behavior&lt;/em&gt; of a function is the sequence $[\mathrm{QS}(f,t_0),\mathrm{QS}(f,t_0,t_1),\mathrm{QS}(f,t_1),\ldots{},\mathrm{QS}(f,t_{n-1},t_n),\mathrm{QS}(f,t_n)]$, where each $t_i$ is a distinguished time point.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;qualitative state transition&lt;/em&gt; is either a &lt;em&gt;P-transition&lt;/em&gt;, which is the transition from a distinguished time point to an interval between distinguished time points: $\mathrm{QS}(f,t_i)\Rightarrow \mathrm{QS}(f, t_i, t_{i+1})$. Similarly, a &lt;em&gt;I-transition&lt;/em&gt; is the other way around: $\mathrm{QS}(f,t_{i-1},t_i)\Rightarrow\mathrm{QS}(f,t_i)$.&lt;/p&gt;

&lt;h2 id=&quot;qualitative-structure&quot;&gt;Qualitative structure&lt;/h2&gt;

&lt;p&gt;Constraints can be imposed on either a single function, or on a pair of functions, for example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ADD}(f, g, h)$ is defined as $f(t)+g(t)=h(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{MULT}(f, g, h)$ is defined as $f(t)\cdot{}g(t)=h(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{MINUS}(f, g)$ is defined as $f(t)=-g(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{DERIV}(f, g)$ is defined as $f^\prime(t)=g(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{M}^+(f, g)$ is defined as $f(t)=H(g(t))$ where $H^\prime(x)&amp;gt;0$. This rougly says that $f$ and $g$ are directly proportional&lt;/li&gt;
  &lt;li&gt;$\mathrm{M}^-(f, g)$ is defined as $f(t)=H(g(t))$ where $H^\prime(x)&amp;lt;0$. This roughly says that $f$ and $g$ are inversely proportional.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that these constraints only need to hold for $t$ within the domain of $f$ and $g$ (i.e., so you could have $x=\cos\theta$ on $\theta\in(0,\pi)$ and then $\mathrm{M}^-(\theta, x)$ will be true for that specific range of $\theta$).&lt;/p&gt;

&lt;h2 id=&quot;qualitative-simulation&quot;&gt;Qualitative simulation&lt;/h2&gt;

&lt;p&gt;The qualitative simulation takes as input the functions, constraints, landmark values, upper and lower range limits, an initial time point $t_0$, and initial qualitative values for the functions.&lt;/p&gt;

&lt;p&gt;The qualitative simulation returns as output the qualitative behavior descriptions for the given functions, which include: the distinguished time points, landmark values (which may include additional ones not given as input), and qualitative state descriptions for each distinguished time point and interal between time points.&lt;/p&gt;

&lt;p&gt;The way that the simulation works for a single state is, roughly:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;For each function, determine the P-transitions and I-transitions for the current qualitative state&lt;/li&gt;
  &lt;li&gt;For each constraint, form combinations of “transition tuples” for each of the functions involved in the constraint, and filter out those which violate the constraint&lt;/li&gt;
  &lt;li&gt;For each constraint, filter our transition tuples for which there are no transition tuples for adjacent constraints that have the same transition for the shared parameter (e.g. $(I4,I9)$ for $\mathrm{DERIV}(Y,V)$ would be filtered out if there is no corresponding tuple beginning with $I9$ for $\mathrm{DERIV}(V, A)$).&lt;/li&gt;
  &lt;li&gt;Generate possible global interpretations based on the remaining tuples&lt;/li&gt;
  &lt;li&gt;Check which interpretations constitute either a state that has been visited before (cylce) or if the function diverges (goes to infinity). These are leaves of the seach tree; others are next possible states.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This was a really nice, detailed explanation of how (one particular version of) qualitative simulation and reasoning works. I think it’s cool that you can show that this type of qualitative reasoning will produce the right answer (even if it might also produce other answers). It seems like the big challenge is—as Kuipers points out in the discussion—is determining what the constraints on the system are in the first place. This is somewhat related to the question of how you set up a physical simulation in the first place (i.e. object positions, velocities), though in this case the choices that need to be made are the functional constraints, rather than specifics about the object. Certaintly in qualitative simulation it seems that it is easier to specify information about the state of the object: the hard part is in determining what information to specify about the function.&lt;/p&gt;

&lt;p&gt;Also, this seems to apply specifically to continuous systems than can be described by a differential equation, though, so I’m interested in understanding better how this approach works with things that are not continuous (for example, colliding objects) or situations where there are additional types of constraints, such as inequality or equality constraints. I expect some of the next readings will give me a better sense of this.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 04:25:40 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Kuipers1986.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Kuipers1986.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
  </channel>
</rss>
