<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 30 Dec 2015 16:07:17 -0500</pubDate>
    <lastBuildDate>Wed, 30 Dec 2015 16:07:17 -0500</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Stabilizing novel objects by learning to predict tactile slip</title>
        <description>&lt;p&gt;&lt;span id=&quot;Veiga2015&quot;&gt;Veiga, F., van Hoof, H., Peters, J., &amp;amp; Hermans, T. (2015). Stabilizing Novel Objects by Learning to Predict Tactile Slip. &lt;i&gt;Proceedings Of the IEEE/RSJ Conference on Intelligent Robots and Systems&lt;/i&gt;. Retrieved from http://www.ausy.tu-darmstadt.de/uploads/Site/EditPublication/IROS2015veiga.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Veiga et al. compare a variety of supervised machine learning methods for learning to detect and predict tactile slip. They compare both SVMs and random forest classifiers and use features that come from a tactile sensor based on the human finger. More specifically, they have three different ways of constructing features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Single element features – based only on the current timestep, $\phi(\mathbf{x}_{1:t})=\mathbf{x}_t$&lt;/li&gt;
  &lt;li&gt;Delta features – based on the current timestep, as well as the change from the last timestep, $\phi(\mathbf{x}_{1:t})=[\mathbf{x}_t,\Delta\mathbf{x}_t]$&lt;/li&gt;
  &lt;li&gt;Time window features – based on a window of previous time steps, $\phi(\mathbf{x}_{1:t})=\mathbf{x}_{t-\tau:t}$, where $\tau$ is the size of the time window&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The random forest classifiers tend to do the best, particularly with the delta features.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;In this paper, Veiga et al. use purely discriminative methods to detect and predict slip based on features extracted from a tactile sensor. This means that their method works independently of object-specific features and thus does not take into account anything about the actual dynamics of the object. In this case, this approach actually makes a lot of sense to me—it feels like it would be overkill to need to have a full dynamics model of how the object behaves just to determine if it is slipping. That said, I do wonder if having &lt;em&gt;some&lt;/em&gt; approximate knowledge of the object dynamics (e.g. curvature, overall mass, distribution of mass, etc.) would help, particularly in having the controller adjust once it has detected that the object is slipping.&lt;/p&gt;

&lt;p&gt;I also wonder to what extent these learned classifiers will generalize to greater changes in mass/shape/friction—e.g. will the same classifier work for a very heavy object as opposed to a very light object? Or a very smooth object vs. a rough object? And, will it generalize to other types of scenarios where slip might occur (e.g. holding an object with two hands, as opposed to up against a wall; and also, the location where the object is being held).&lt;/p&gt;

&lt;p&gt;This paper also made me think a bit about what constitutes simulation. In this paper, Veiga et al. are able to predict several timesteps into the future whether slip &lt;em&gt;will&lt;/em&gt; occur. Even though they don’t have a dynamics model of how slip changes over time, for example, does this type of prediction still count as “simulation”? I’m tempted to say no, not unless they can predict at each timestep &lt;em&gt;how much&lt;/em&gt; slip is going to occur—e.g. in the next timestep, there will be a small amount of slip, in the following timestep, it will increase by so much, etc. I don’t think this type of prediction would be particularly hard to do, though, if they could perhaps attach sensors to their objects in order to more precisely quantify what slip is (in this paper, they labeled parts of the video as “slip” or “not slip” just by human visual judgments).&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Dec 2015 10:48:29 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Veiga2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Veiga2015.html</guid>
        
        
        <category>Physical reasoning without dynamics models</category>
        
      </item>
    
      <item>
        <title>Learning force-based manipulations of deformable objects from multiple demonstrations</title>
        <description>&lt;p&gt;&lt;span id=&quot;Lee2015&quot;&gt;Lee, A. X., Lu, H., Gupta, A., Levine, S., &amp;amp; Abbeel, P. (2015). Learning Force-Based Manipulation of Deformable Objects from Multiple Demonstrations. &lt;i&gt;Proceedings Of the IEEE International Conference on Robotics and Automation&lt;/i&gt;. doi:10.1109/ICRA.2015.7138997&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper builds on &lt;a href=&quot;/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/29/Schulman2013a.html&quot;&gt;Schulman et al.&lt;/a&gt; by not only warping the pose of the robot’s trajectory, but also the force that is applied to the objects. This makes for a more robust and generalizable scheme for manipulating deformable objects (such as rope).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;The algorithm has several steps:&lt;/p&gt;

&lt;h2 id=&quot;step-1&quot;&gt;Step 1&lt;/h2&gt;

&lt;p&gt;Perform scene registration, where the points from the test scene are mapped to the points from the demonstrated scene in order to come up with a function $f$ that transforms between the two.&lt;/p&gt;

&lt;h2 id=&quot;step-2&quot;&gt;Step 2&lt;/h2&gt;

&lt;p&gt;The trajectories from demonstrated scenes are mapped to the new scene using $f$: the poses are given by $\tilde{\mathbf{Q}}^d=f(\hat{\mathbf{Q}}^d)$ and the forces are given by $\tilde{\mathbf{U}}^d=\frac{df^d}{d\mathbf{p}}(\hat{\mathbf{Q}}^d)\hat{\mathbf{U}}^d$.&lt;/p&gt;

&lt;h2 id=&quot;step-3&quot;&gt;Step 3&lt;/h2&gt;

&lt;p&gt;The trajectories are time-aligned using “dynamic time warping” such that all demonstrated trajectories follow the same time course. This yields $\mathbf{Q}^d$ and $\mathbf{U}^d$.&lt;/p&gt;

&lt;h2 id=&quot;step-4&quot;&gt;Step 4&lt;/h2&gt;

&lt;p&gt;To extract the final trajectory based on the time-aligned demonstrated trajectories, Lee et al. use a Gaussian model of the force:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{u}^d_t\vert \mathbf{q}_t^d,\mathbf{\dot{q}}_t^d)=\mathcal{N}(\mathbf{K}_{pt}(\mathbf{\bar{q}}_t-\mathbf{q}_t^d)+\mathbf{K}_{vt}(\dot{\bar{\mathbf{q}}}_t-\dot{\mathbf{q}}_t^d)+\bar{\mathbf{u}}_t; \mathbf{C}_t)&lt;/script&gt;

&lt;p&gt;Here, $\bar{\mathbf{q}}_t$, $\dot{\bar{\mathbf{q}}}_t$, and $\bar{\mathbf{u}}_t$ are the positions, velocities, and forces of the desired trajectory and are computed by fitting a joint Gaussian distribution to $[\mathbf{q}_t^d,\dot{\mathbf{q}}_t^d,\mathbf{u}_t^d]$ at each time step and then taking the mean of that distribution. The variables $\mathbf{K}_{pt}$ and $\mathbf{K}_{vt}$ are the position and velocity gains and are computed from the covariances of the fitted joint Gaussians: $\mathbf{K}_{pt}=-\Sigma_{\mathbf{uq},t}\Sigma^{-1}_{\mathbf{qq},t}$ and $\mathbf{K}_{vt}=-\Sigma_{\mathbf{u\dot{q}},t}\Sigma^{-1}_{\mathbf{\dot{q}q},t}$ (see Section 8.1.3 of the Matrix Cookbook for a better intuition of where this comes from).&lt;/p&gt;

&lt;p&gt;Rather than directly fitting the covariances $\Sigma_t$, they use a inverse-Wishart prior on the covariance in order to encourage the gains to be non-negative. They additionally include samples from nearby time points which encourages the estimate to vary smoothly over time.&lt;/p&gt;

&lt;h2 id=&quot;step-5&quot;&gt;Step 5&lt;/h2&gt;

&lt;p&gt;Lee et al. now optimize the robot joint angles $\Theta=[\theta_1,\ldots{},\theta_T]^\top$ to match the desired trajectory (as computed in the previous step). Then, they convert the gains into joint-space gains $\mathbf{K}_{pt}^\theta$ and $\mathbf{K}_{vt}^\theta$ and compute the torque as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{f}_t=\mathbf{K}_{pt}^\theta(\theta_t-\theta_\mathrm{obs})+\mathbf{K}_{vt}^\theta(\dot{\theta}_t-\dot{\theta}_\mathrm{obs})+\mathbf{J}(\theta)^\top\bar{\mathbf{u}}_t&lt;/script&gt;

&lt;p&gt;where $\mathbf{J}(\theta)$ is the Jacobian and where $\theta_\mathrm{obs}$ and $\dot{\theta}_\mathrm{obs}$ are the observed joint angles and velocities.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This approach is a significant improvement over previous work, which was unable to generalize to situations in which the applied force was more important than the exact position (e.g. folding a larger towel than the one in the demonstrations requires applying the same force, but moving the arms out further in order to apply that force). The core idea is still the same, however: without having any task-specific or physical knowledge, map from demonstrated trajectories (including forces) to new situations.&lt;/p&gt;

&lt;p&gt;I think it’s really interesting that they are able to take force into account and trade off between force and position without actually having knowledge of the task; I’m surprised it works as well as it does! The intuition behind why it works is that when there is variance in the demonstrations, that implies that absolute position is less important, and that something else is more important—for example, force. Thus, where there is more variance in position, the force should be considered more strongly, and when there is low variance, position should be considered more strongly. I do wonder though whether this property always holds true. For example, if you were trying to unscrew a cap from a jar, and different demonstrations had caps that were stuck more or less tightly, I &lt;em&gt;think&lt;/em&gt; that the position of the gripper would end up still being about the same—it would be the force that differs. In that case, this wouldn’t work (I think) because it would move the gripper in the appropriate way, but not apply the right force; and in fact, having demonstrations of the force wouldn’t necessarily be sufficient either—to accomplish this task, you would have to have some notion of the position of the jar and the cap and how much the cap is moving, etc.&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Dec 2015 05:52:02 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Lee2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Lee2015.html</guid>
        
        
        <category>Physical reasoning without dynamics models</category>
        
      </item>
    
      <item>
        <title>Learning from demonstrations through the use of non-rigid registration</title>
        <description>&lt;p&gt;&lt;span id=&quot;Schulman2013a&quot;&gt;Schulman, J., Ho, J., Lee, C., &amp;amp; Abbeel, P. (2013). Learning from Demonstrations Through the Use of Non-Rigid Registration. &lt;i&gt;Proceedings Of the 16th International Symposium on Robotics Research&lt;/i&gt;. Retrieved from http://www.cs.berkeley.edu/ pabbeel/papers/SchulmanHoLeeAbbeel_ISRR2013.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Schulman et al. describe a way to learn from demonstration for performing tasks with deformable objects (e.g., rope tying). The core idea behind their method is essentially to compute a geometric transformation $\mathbf{f}$ which maps from one state to another state, and which can be applied to the demonstrated trajectory to produce a new trajectory. This approach, combined with a nearest-neighbors selection policy, works well for transforming given trajectories to new trajectories as long as the nearest neighbor is close enough.&lt;/p&gt;

&lt;p&gt;Their approach rests on the following property (the &lt;em&gt;cost function invariance&lt;/em&gt;):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\mathrm{State},\mathrm{Traj})=L(\mathbf{f}(\mathrm{State}),\mathbf{f}(\mathrm{Traj}))&lt;/script&gt;

&lt;p&gt;where $L$ is the loss function. Using this property, the following approximation follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
L(\mathrm{State}_\mathrm{test},\mathbf{f}(\mathrm{Traj}_\mathrm{train}))&amp;\approx L(\mathbf{f}(\mathrm{State}_\mathrm{train}),\mathbf{f}(\mathrm{Traj}_\mathrm{train}))\\
&amp;= L(\mathrm{State}_\mathrm{train},\mathrm{Traj}_\mathrm{train})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;An alternate intuition for why this works is that of &lt;em&gt;dynamics invariance&lt;/em&gt;, in which the “dynamics of the system are approximately invariant—more properly, &lt;em&gt;covariant&lt;/em&gt;—under sufficiently smooth coordinate transformations”. This is true if and only if:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{f}(\Pi_\mathrm{Traj}(\mathrm{State}^t))=\Pi_{\mathbf{f}(\mathrm{Traj})}(f(\mathrm{State}^t))&lt;/script&gt;

&lt;p&gt;where $\Pi_\mathrm{Traj}(\mathrm{State})$ applies trajectory $\mathrm{Traj}$ to state $\mathrm{State}$.&lt;/p&gt;

&lt;p&gt;Of course, there are cases where this will not hold. The example they give in the paper is a rigid transformation where the “absolution orientation of the robot’s end-effector matters”. However, based on their results, it seems that for a wide range of states and trajectories it does hold well enough to generalize to new states.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a clever use of nearest neighbors to generalize from examples without needing to have any knowledge of the domain (besides being able to perceive the position/topology of the relevant object, e.g. the rope). However, I wonder how far this will really generalize, even with many examples. For example, if you had to tie a rope around another object, I don’t think this would work as well—and this intuition seems to be correct, based on their experimental results (1/10 successes for the clove hitch, which involves tying a rope around a pole). I would expect it to be even harder if the task involved tying a rope around a nonrigid object. There is simply so much noise in the system that without some model of how the objects behave, you are going to quickly run into scenarios that you don’t have examples for.&lt;/p&gt;

&lt;p&gt;I also think you would probably quickly run into scenarios where the transformation between states isn’t smooth. For example, if the task were to place a block on a stack of blocks and have the tower remain stable, the stability of the tower doesn’t vary smoothly as a function of the positions of the blocks. So even if you had encountered a very similar tower in the past, it’s entirely possible that even a slight difference between that tower and the current one could result in nonlinearities—e.g. in the first tower you need to place the block in position A to keep the tower stable, but position A in the second tower will always be unstable.&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Dec 2015 14:18:08 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/29/Schulman2013a.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/29/Schulman2013a.html</guid>
        
        
        <category>Physical reasoning without dynamics models</category>
        
      </item>
    
      <item>
        <title>Model-based Bayesian reinforcement learning in large structured domains</title>
        <description>&lt;p&gt;&lt;span id=&quot;Ross2008&quot;&gt;Ross, S., &amp;amp; Pineau, J. (2008). Model-based Bayesian Reinforcement Learning in Large Structured Domains. &lt;i&gt;Proceedings Of the 24th Conference in Uncertainty in Artificial Intelligence&lt;/i&gt;, 476–483. Retrieved from http://arxiv.org/abs/1206.3281&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Ross &amp;amp; Pineau propose a method for performing Bayesian reinforcement learning when both the structure and parameters of the transition function are unknown. That is, they need to infer $P(G,\theta_G\vert D)$ where $G$ is the graph structure, $\theta_G$ are the parameters of the graph, and $D$ is the observed data. Then, given this posterior, they need to plan optimally with respect to it. Ross &amp;amp; Pineau show how this can be done by casting the problem as a POMDP (where the unobserved states consist of the original state, the graph structure, and the graph parameters) and then suggest an algorithm for planning in continuous, high-dimensional POMDPs such as this that uses particle filtering with resampling. They show that their method rapidly recovers the correct structure in simple cases and recovers incorrect but approximate structure in more complex cases, and that it is significantly faster and more accurate than a MDP which does full joint inference over transitions (i.e. does not take into account structure).&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I am not going to go into the nitty-gritty details of the math in this paper—I think the important takeaway is that by doing Bayesian RL, we can maintain not only a distribution over what we think the transition function is, but we can incorporate structured prior knowledge into the inference procedure. So rather than assuming what is essentially a fully connected transition graph and estimating the parameters of each edge, we can keep open the possibility that the transition function has some form of higher-level structure. I think this is particularly important because it could potentially allow you to do inference about transitions or rewards from states that are different but are actually similar according to some high level structure. Actually, I think this is probably especially important for reward structures, even more so than transition structures—for example if the reward structure is based on something like the graph structures discussed by &lt;a href=&quot;/quals/probabilistic%20models%20of%20cognition/2015/11/08/Kemp2008.html&quot;&gt;Kemp &amp;amp; Tenenbaum&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Dec 2015 07:16:31 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/planning%20and%20decision%20making/2015/12/29/Ross2008.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/planning%20and%20decision%20making/2015/12/29/Ross2008.html</guid>
        
        
        <category>Planning and decision making</category>
        
      </item>
    
      <item>
        <title>The scope and limits of simulation in automated reasoning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Davis&quot;&gt;Davis, E., &amp;amp; Marcus, G. F. The Scope and Limits of Simulation in Automated Reasoning. &lt;i&gt;Artificial Intelligence&lt;/i&gt;. Retrieved from http://www.cs.nyu.edu/faculty/davise/papers/SimulationSubmitAIJ.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Davis &amp;amp; Marcus summarize some of the challenges that they seen in using simulation for automated reasoning (and in particular physical reasoning). The twelve challenges that they discuss are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Choosing an appropriate model&lt;/strong&gt;. For example, what kind of cutting instrument should be used for firewood? (Not scissors.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Discretizing time&lt;/strong&gt;. There are various issues with this, for example having objects pass through each other, running into instability due to inappropriate discretizations, or violating constrainst such as conservation of matter.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Discontinuous dynamics&lt;/strong&gt;. Small perturbations can lead to large changes in dynamics (e.g. rolling a die, chaotic systems).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Choosing an idealization&lt;/strong&gt;. What representation should be used in the simulation? e.g., should a pendulum be represented by a point mass moving on a circle, by a rigid shape constrained to be a certain distance from the origin, by a rigid shape on a (deformable) string, etc.?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Drawing “easy” inferences&lt;/strong&gt;. Some types of inferences are “easy” but simulation is not well-suited to them. For example:
    &lt;ul&gt;
      &lt;li&gt;Invariance under irrelevant changes (if a jar fits on a shelf, and you fill it, it will still fit on the shelf)&lt;/li&gt;
      &lt;li&gt;Invariance under changes of scale (e.g. kinematics is the same regardless of the size of the objects)&lt;/li&gt;
      &lt;li&gt;Approximation (e.g. containers of similar sizes will hold similar amounts of liquid)&lt;/li&gt;
      &lt;li&gt;Ordering on a relevant dimension (e.g. if a toy fits in a box, then it will fit in a larger box)&lt;/li&gt;
      &lt;li&gt;Decomposition (e.g. if there are two independent subsystems, they can be reasoned about independently)&lt;/li&gt;
      &lt;li&gt;Rules of thumb (e.g. you spill coffee in your office –&amp;gt; it won’t end up in another office)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incorporating extra-physical information&lt;/strong&gt;. Sometimes information that is non-physical can still be useful in predicting physical systems (e.g. general accuracy of a baseball pitcher)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incomplete information&lt;/strong&gt;. You don’t always have fully specified information about the system, such as occluded geometry, unobservable properties (like mass, friction), unknown dynamics, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Irrelevant information&lt;/strong&gt;. How should the automated system determine what information is irrelevant and should be excluded? This is something that is typically done by hand (by the modeller) but in the general case needs to be handled by the system.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Range of scales&lt;/strong&gt;. How can simulation scale from very small scales (e.g. everyday objects) to very large scales (e.g. stars)?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tasks other than prediction&lt;/strong&gt;. There are other types of tasks that simulation does not deal well with:
    &lt;ul&gt;
      &lt;li&gt;Interpolation&lt;/li&gt;
      &lt;li&gt;Planning&lt;/li&gt;
      &lt;li&gt;Inferring object shape&lt;/li&gt;
      &lt;li&gt;Inferring physical properties&lt;/li&gt;
      &lt;li&gt;Design of systems&lt;/li&gt;
      &lt;li&gt;Comparative analysis (determining how a modification to a system would affect the solution)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The frame problem&lt;/strong&gt;. Reasoning separately about things that are independent.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Checking simulations with common sense&lt;/strong&gt;. If simulation is the only reasoning mechanism, then there is nothing to check whether the simulation is correct.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I certainly agree with Davis &amp;amp; Marcus that simulation isn’t the only form of reasoning, and I think there are very interesting questions regarding how we trade off between different forms of reasoning. Any system that uses simulation almost certainly needs to incorporate the following choices:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Determine what type of reasoning to use, based on features of the problem.&lt;/li&gt;
  &lt;li&gt;If simulation is appropriate, determine how to set up the simulation and how to evaluate/interpret the results of the simulation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I would argue that the majority of Davis &amp;amp; Marcus’ objections are just reformulations of the question of how to address these two choices. Points 1, 2, 4, 7, 8, 9, 11, and 12 are instances of “how to set up an appropriate simulation and interpret it?”. Points 5, 6, and 10 are instances of “should simulation be used, or something else?”. The only remaining point is Point 3, which doesn’t seem like a problem to me. Reasoning about the world is inherently uncertain, and any reasoning system needs to be able to take that type of uncertainty into account. Simulation needs to be precise in one sense (that things need to be fully specified), but it does not need to be &lt;em&gt;accurate&lt;/em&gt;, and the specification need not necessarily be precise, either.&lt;/p&gt;

&lt;p&gt;I do think that the questions of whether to use simulation or something else, and then &lt;em&gt;how&lt;/em&gt; to use simulation, are incredibly important and extremely difficult questions. Also, the fact that I’ve boiled 12 points down into 2 doesn’t mean that those two choices aren’t &lt;em&gt;incredibly&lt;/em&gt; difficult and don’t have multiple subquestions. But I don’t think that the fact that simulation is a difficult topic means that we shouldn’t use it or research its capabilities and the extent of its utility. It seems to me that simulation—and generative models more broadly—are really powerful because they’re so flexible and can apply in so many ways to so many types of scenarios. The question is how to take full advantage of that flexbility.&lt;/p&gt;
</description>
        <pubDate>Mon, 28 Dec 2015 11:54:26 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Davis.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Davis.html</guid>
        
        
        <category>Physical reasoning with dynamics models</category>
        
      </item>
    
      <item>
        <title>Model-based reinforcement learning with parametrized physical models and optimism-driven exploration</title>
        <description>&lt;p&gt;&lt;span id=&quot;Xie2015&quot;&gt;Xie, C., Patil, S., Moldovan, T., Levine, S., &amp;amp; Abbeel, P. (2015). Model-based Reinforcement Learning with Parametrized Physical Models and Optimism-Driven Exploration. &lt;i&gt;ArXiv Preprint ArXiv:1509.06824v1 [Cs.LG]&lt;/i&gt;. Retrieved from http://arxiv.org/abs/1509.06824&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Xie et al. use a linear approximation to physical dynamics using dynamics features in order to perform optimism-based exploration for model-based reinforcement learning. For certain dynamical systems, such as a pendulum or robot arm, this approach works remarkably well because the dynamics can be factored into a linear least-squares model where certain features are known (e.g. joint angles) and others must be learned (e.g. link weights or lengths) as part of the RL process. This can either be done by hand or with the help of libraries like &lt;a href=&quot;https://github.com/cdsousa/sympybotics&quot;&gt;SymPyBotics&lt;/a&gt; (which looks super cool!).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;There are a few components to this algorithm, which I will describe separately.&lt;/p&gt;

&lt;h2 id=&quot;optimism-based-exploration&quot;&gt;Optimism-based exploration&lt;/h2&gt;

&lt;p&gt;The idea behind optimism-based exploration is related to the idea of having slack variables in an optimization. Essentially, you have some estimate of the dynamics, and you can act greedily with respect to those dynamics:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\ddot{q}}_t=\hat{f}(\mathbf{q}_t,\mathbf{\dot{q}}_t,\tau_t)&lt;/script&gt;

&lt;p&gt;where $\hat{f}$ are the forward dynamics. To make it optimism-based:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\ddot{q}}_t=\hat{f}(\mathbf{q}_t,\mathbf{\dot{q}}_t,\tau_t)+\mathbf{xi}_t=\mathbf{\tilde{f}}(\mathbf{q}_t,\mathbf{\dot{q}}_t,\tau_t,\mathbf{xi}_t)&lt;/script&gt;

&lt;p&gt;where $\mathbf{\xi}_t$ is a slack variable. This variable is constrained based on the amount of uncertainty in the estimate of the dynamics $\hat{f}$. Rather than explicitly estimating this uncertainty, Xie et al. simply include a penalty term of the form $\frac{1}{m}\lVert\mathbf{\xi}_t\rVert^2$ where $m$ is proportional to the number of samples.&lt;/p&gt;

&lt;h2 id=&quot;linear-least-squares-model&quot;&gt;Linear least squares model&lt;/h2&gt;

&lt;p&gt;The equations of motion can be factored into a linear equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\mathbf{q},\mathbf{\dot{q}}, \mathbf{\ddot{q}})\cdot{}\Delta=\mathbf{\tau}&lt;/script&gt;

&lt;p&gt;where $\Delta$ are the system parameters to be estimated and $H$ are the physical features (e.g. joint angles). Given observations of $[\mathbf{q},\mathbf{\dot{q}}, \mathbf{\ddot{q}}]$, $\Delta$ can be estimated using least-squares regression, thus giving the forward dynamics model ($\mathbf{\tilde{f}}(\mathbf{q}_t,\mathbf{\dot{q}}_t,\tau_t,\mathbf{xi}_t)$) that can then be used in model-predictive control.&lt;/p&gt;

&lt;h2 id=&quot;model-predictive-control&quot;&gt;Model-predictive control&lt;/h2&gt;

&lt;p&gt;The MPC algorithm used by Xie et al. is the same as that used by &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Kitaev2015.html&quot;&gt;Kitaev et al.&lt;/a&gt; (iterative linear quadratic regulator). Briefly:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The algorithm iteratively computes first order expansions of the dynamics and second order expansions of the cost around the current trajectory, and then analytically computes the sequence of optimal controls with respect to this approximation. This sequence of controls is then executed to obtain a new trajectory, and the process repeats until convergence or for a fixed number of iterations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;By using prior knowledge of the &lt;em&gt;structure&lt;/em&gt; of dynamics, it is possible to quickly learn the correct model parameters needed to use model-predictive control. This is really powerful, because model-predictive control can be much more efficient and accurate than a model-free method, especially for systems with complex dynamics (e.g. a 7 DOF robot arm). If some information about the dynamics of the system can be specified (e.g. the number of robot links and their joint angles) then the rest can be quickly inferred because the really hard part (the structure) is already present – the parameters of that structure just need to be learned.&lt;/p&gt;
</description>
        <pubDate>Mon, 28 Dec 2015 10:06:19 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Xie2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Xie2015.html</guid>
        
        
        <category>Physical reasoning with dynamics models</category>
        
      </item>
    
      <item>
        <title>Physics-based trajectory optimization for grasping in cluttered environments</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kitaev2015&quot;&gt;Kitaev, N., Mordatch, I., Patil, S., &amp;amp; Abbeel, P. (2015). Physics-Based Trajectory Optimization for Grasping in Cluttered Environments. &lt;i&gt;Proceedings Of the IEEE International Conference on Robotics and Automation&lt;/i&gt;. Retrieved from http://www.eecs.berkeley.edu/ pabbeel/papers/2015-ICRA-clutter.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kitaev et al. use rollouts from a physics engine in order to compute the gradient of the object, which takes into account physical dynamics. They compare two different methods: a baseline which only uses straight trajectories, and their physics-based method. They find that the physics-based method is able to produce behavior that is similar to that exhibited by people (e.g. weaving through cluttered objects) and which avoids catastrophic physical events.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;baseline-method&quot;&gt;Baseline method&lt;/h2&gt;

&lt;p&gt;First, the baseline approach finds a straight line trajectory at some particular approach angle $\alpha$. For any particular value of $\alpha$, the trajectory $\mathbf{q}_\alpha$ is computed using sequental quadratic programming to minimize an objective function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\mathbf{q}^{1,\ldots{},\Delta}} \sum_t \sum_k w_k c_k^t (\mathbf{q}^t,\mathbf{q}^{t-1})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathrm{s.t.}\ \ &amp; \mathbf{p}_\mathrm{gripper}(\mathbf{q}^\Delta)=\mathbf{p}_\mathrm{gripper}(\mathbf{q}_\alpha^\Delta)\\
&amp; R_\mathrm{gripper}(\mathbf{q}^\Delta)=R_\mathrm{gripper}(\mathbf{q}_\alpha^\Delta)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mathbf{p}_\mathrm{gripper}$ is the position of the gripper and $R_\mathrm{gripper}$ is the rotation of the gripper. The cost terms are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$c_\mathrm{static\underline{\ }hull}$ – penalizes penetration between the robot and static obstacles if they are closer than some distance $d_\mathrm{safe}$.&lt;/li&gt;
  &lt;li&gt;$c_\mathrm{vel}$ – penalizes the robot’s velocity to ensure smooth trajectories&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Trajectories are computed for each value of $\alpha$ and then physics simulations are used to evaluate each $\mathbf{q}_\alpha$ (criteria described below). The $\alpha$ corresponding to the best trajectory is chosen.&lt;/p&gt;

&lt;h2 id=&quot;physics-based-method&quot;&gt;Physics-based method&lt;/h2&gt;

&lt;p&gt;The physics-based method also takes into account the physical dynamics of the static objects – not just that of the robot. The objective is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp; \min_{\mathbf{q}^{1,\ldots{},\Delta}} \sum_t \sum_k \sum_i w_k c_k^t (\mathbf{q}^t,\mathbf{q}^{t-1},\mathbf{x}_i^t)\\
&amp; \mathrm{s.t.}\ \ \mathcal{X}^t=\phi(\mathcal{X}^{t-1},\mathbf{q}^t),\ \mathcal{X}^1\ \mathrm{is\ fixed}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\phi$ is the physical dynamics. They solve this optimization by picking an initial trajectory and rolling out physics for that trajectory. Then, they linearize the dynamics around that trajectory (which they can do by taking the derivative of the dynamics using a smooth contact model), and optimize that approximation.&lt;/p&gt;

&lt;p&gt;The cost terms are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$c_\mathrm{static}$ – similar to $c_\mathrm{static\underline{\ }hull}$, except that it checks collision at each time step rather than using the swept-out convex hull between timesteps&lt;/li&gt;
  &lt;li&gt;$c_\mathrm{acc}$ – similar to $c_\mathrm{vel}$, but ensures smooth acceleration rather than velocity&lt;/li&gt;
  &lt;li&gt;$c_\mathrm{force}$ – penalizes contact forces to prevent kinematically impossible trajectories&lt;/li&gt;
  &lt;li&gt;$c_\mathrm{motion}$ – penalizes static objects falling off the table&lt;/li&gt;
  &lt;li&gt;$c_\mathrm{upright}$ – penalizes static objects tipping over&lt;/li&gt;
  &lt;li&gt;$c_\mathrm{grasp}$ – encourages the gripper to be close to the target object&lt;/li&gt;
  &lt;li&gt;$c_\mathrm{grp\underline{\ }horiz}$ – enforces that the gripper must be horizontal&lt;/li&gt;
  &lt;li&gt;$c_\mathrm{grp\underline{\ }open}$ – enforces that the gripper must be open on the last timestep&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(there are a few other cost terms that are used in pre- and post-processing steps, but I am not going to go into the details of those here).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Kitaev et al. evaluate their approach on “shelf” and “refridgerator” scenes with varying numbers of objects, and they define success as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Success&lt;/em&gt; – the robot grasps the target object without knocking over any other objects&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Partial success&lt;/em&gt; – the robot grasps the target object and some of the other objects are tipped over (but not knocked off the shelf)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Failure&lt;/em&gt; – any other behavior, including failure to grasp the object, knocking objects off the shelf, and kinematic failures (e.g. attempting to move through a static object).&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The problem that Kitaev et al. are solving here is that sometimes when reaching for an object, there will be other objects in the way. Traditionally, trajectory optimization has just dealt with situations where this isn’t an issue by avoiding obstacles entirely, but this isn’t always an option. Instead, some situations call for actually reasoning about the dynamics of objects and executing a trajectory that does collide with those objects, but that does so in an intelligent way such that they are less likely to fall over or cause other objects to fall over.&lt;/p&gt;

&lt;p&gt;The approach that they take in this paper is to include a term for linearized approximate dynamics in the objective function. They make the simplifying assumption that the dynamics are smooth (e.g. rather than having a step function saying “in contact” and “not in contact”, there is some smooth function that varies between the two).&lt;/p&gt;

&lt;p&gt;It’s not clear to me how well this approach would work in practice, though. Their experiments show improvement over the baseline, but the baseline seems to be pretty simple and doesn’t include cost terms that might be helpful to it (e.g. by allowing the gripper to rotate and remain closed until the end). For the most crowded cases, these things probably wouldn’t help the baseline much, but for intermediate cases a better baseline would probably be one that allows the gripper to move around rather than enforcing a straight line. Additionally, would the smooth contact model work well in scenarios in which objects need to remain stacked, for example? Especially in the case of the fridge scenario, there are often objects stacked on top of each other, and it would be important to represent this in the simulation. I’m not sure if their smooth contact model would be stable for stacked objects, though.&lt;/p&gt;

&lt;p&gt;That said, it is really cool that they seem to find realistic behavior emerging from the physics-based approach – for example, the “snaking” behavior of the gripper in very crowded scenes to push objects to the side, rather than forward. And, it is interesting to see this as yet another type of approach that can be used to incorporate physical knowledge into reasoning about the world.&lt;/p&gt;
</description>
        <pubDate>Mon, 28 Dec 2015 07:17:48 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Kitaev2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Kitaev2015.html</guid>
        
        
        <category>Physical reasoning with dynamics models</category>
        
      </item>
    
      <item>
        <title>Detecting potential falling objects by inferring human action and natural disturbance</title>
        <description>&lt;p&gt;&lt;span id=&quot;Zheng2014&quot;&gt;Zheng, B., Zhao, Y., Yu, J. C., Ikeuchi, K., &amp;amp; Zhu, S.-C. (2014). Detecting Potential Falling Objects by Inferring Human Action and Natural Disturbance. &lt;i&gt;Proceedings Of the IEEE International Conference on Robotics and Automation&lt;/i&gt;. doi:10.1109/ICRA.2014.6907351&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Zheng et al. outline a system for inferring how likely objects are to fall, based on various disturbance models (e.g. human motion, earthquake, wind) and work/energy calculations. They compare their model’s predictions to human judgments and find that they correspond fairly well (at least based on rank orderings).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;First, they define the “energy barrier” to be the work required to move an object from it’s current resting position $x_0$ (in an energy minima) to an unstable equilibrium $\tilde{x}$. Then, the “falling risk” of an object is defined to be the amount of change in energy between the unstable equilibrium to a smaller local minima $x_{0^\prime}$.&lt;/p&gt;

&lt;p&gt;Second, they define a “disturbance field” to be a probability distribution over forces in the scene:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Earthquake – horizontal forces distributed uniformly at random&lt;/li&gt;
  &lt;li&gt;Wind – forces in a particular direction distributed uniformly at random&lt;/li&gt;
  &lt;li&gt;Human action – they calculate this by convolving two different distributions. First, they calculate the areas that humans are most likely to be in, based on computing shortest paths between random pairs of points in the room and then computing how likely it is for someone to cross a particular point across all shortest paths. Second, they calculate the regions of space that a human’s arms or legs are most likely to move through, by using a 3D motion capture (they don’t say, though, what motions they used in the motion capture – just walking?). By convolving these two fields, they end up with a probability distribution for how likely it is for motion to occur at particular places in space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They then calculate the expected falling risk as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(a,\mathbf{x}_0)=\int p(W,\mathbf{x}_0)R(a, \mathbf{x}_0, W)\ \mathrm{d}W&lt;/script&gt;

&lt;p&gt;where $p(W,\mathbf{x}_0)$ is given by the disturbance field and $R(a, \mathbf{x}_0, W)$ is the maximum energy that the object can release when it is moved out of its energy barrier by work $W$. This is related to, for example, how high up the object is – there will be a higher risk if the object is high up because it can fall further.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;It is a really interesting idea to estimate how likely it is for things to fall based on computing a possible disturbance field. I wonder, though, how similar this is to how people estimate things. I definitely think we probably need something more efficient and robust than running simulations for every single object in the room (a la what we did with the tower simulations), but I also don’t believe that people would estimate how likely a place is to be walked in. Perhaps an approach that would be more similar to human behavior would be to include this type of calculate into the loss function. That is, for a given path, you estimate where movement is likely to occur &lt;em&gt;just for that path&lt;/em&gt; and then you can estimate the expected risk for objects near the path. Then you don’t need to consider the whole scene at once. This would essentially just take into account the local human disturbance field rather than the global one.&lt;/p&gt;
</description>
        <pubDate>Mon, 21 Dec 2015 10:09:10 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/21/Zheng2014.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/21/Zheng2014.html</guid>
        
        
        <category>Physical reasoning with dynamics models</category>
        
      </item>
    
      <item>
        <title>Tracking deformable objects with point clouds</title>
        <description>&lt;p&gt;&lt;span id=&quot;Schulman2013b&quot;&gt;Schulman, J., Lee, A. X., Ho, J., &amp;amp; Abbeel, P. (2013). Tracking deformable objects with point clouds. &lt;i&gt;Proceedings Of the IEEE International Conference on Robotics and Automation&lt;/i&gt;. doi:10.1109/ICRA.2013.6630714&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this work, Schulman et al. propose an algorithm for tracking deformable objects (e.g. rope, cloth, sponges) using point clouds and a variation of the EM algorithm augmented with a real-time physics engine. They show that their algorithm can run in real-time and is fairly accurate provided there are not too many occlusions from external sources (e.g. human hands). Here is a video demonstrating their algorithm:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=3ps7gb_oxxI&quot;&gt;https://www.youtube.com/watch?v=3ps7gb_oxxI&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Schulman et al. define a probabilistic generative model for point tracking, which include the following variables:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathbf{m}_{1:K}={\mathbf{m}_1, \mathbf{m}_2, \ldots{}, \mathbf{m}_K}$ are a set of $K$ points in 3D space which define the (estimated) configuration/model of the object.&lt;/li&gt;
  &lt;li&gt;$\mathbf{c}_{1:N}={\mathbf{c}_1, \mathbf{c}_2, \ldots{}, \mathbf{c}_N}$ are a set of $N$ points generated from the sensor hardware (i.e. the point cloud).&lt;/li&gt;
  &lt;li&gt;$z_{kn}$ is an indicator variable that is 1 if the object surface nearest to $\mathbf{m}_{k}$ generated observation $\mathbf{c}_n$.&lt;/li&gt;
  &lt;li&gt;$z_{K+1,n}$ is an indicator variable that is 1 if $\mathbf{c}_n$ is a point generated by noise.&lt;/li&gt;
  &lt;li&gt;$v_k$ is an indicator variable that is 1 if $\mathbf{m}_k$ is visible from the camera&lt;/li&gt;
  &lt;li&gt;$\Sigma_k=\mathrm{diag}((\sigma^x)^2, (\sigma^y)^2, (\sigma^z)^2)$ is a diagonal covariance matrix that parameterizes the noise in observations of points in the point cloud: $\mathbf{c}_n\sim \mathcal{N}(\mathbf{m}_k, \Sigma_k)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I won’t go too much into the details of the densities in this model, except to note that they additionally place a prior on the model points:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{m}_{1:K})=e^{-\frac{1}{\eta} V_0(\mathbf{m}_{1:K})}&lt;/script&gt;

&lt;p&gt;where $V_0(\mathbf{m}_{1:K})$ is the potential energy of the tracked object, which is infinite in infeasible states (e.g. interpenetration). This prior encourages the object model to stay in low-energy states—e.g. stationary (or moving slowly), not being bent too much, etc.&lt;/p&gt;

&lt;p&gt;The inference procedure is a variation on the EM algorithm. First, in the E-step, they compute a lower bound on the log-probability:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\log p(\mathbf{m}_{1:K},\mathbf{c}_{1:N})&amp;=\log p(\mathbf{m}_{1:K}) + \log p(\mathbf{c}_{1:N}\vert \mathbf{m}_{1:K})\\
&amp;= V_0(\mathbf{m}_{1:K})+\sum_n\sum_k \alpha_{nk}\log\mathcal{N}(\mathbf{c}_n; \mathbf{m}_k, \Sigma_k)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\alpha_{nk}=\log p(z_{kn}=1 \vert \mathbf{m}_{1:K}, \mathbf{c}_{1:N})$. Then, in the M-step, they use a physics simulation in order to maximize the lower bound on the log probability. By just running it forward, it would just converge to the prior (a low-energy state), so to avoid this, they introduce external forces into the simulation engine at each point mass equal to the negative gradient of the log likelihood $p(\mathbf{c}_{1:N}\vert \mathbf{m}_{1:K})$. They also include a damping force to keep the total energy at a constant level.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a really clever use of a physics engine to compute a maximization over a posterior distribution. Essentially, the key idea is that the physics engine will naturally converge to the prior (a low-energy state); so, to make it converge to the posterior (i.e., take into accound observations), they introduce external forces into the physics engine proportional to the gradient of the log-likelihood. More broadly, this is an interesting and novel way to maintain a physically realistic model of an object (including the object’s dynamics) based on observations.&lt;/p&gt;

&lt;p&gt;I expect this would actually work well with rigid body dynamics as well, though perhaps in the presence of many collisions, it wouldn’t – it would be interesting to test it out in those cases.
Additionally, it would be interesting to see how this could be extended to multiple object tracking. For example, could you use this method to track the objects in our block towers? I suspect that would be more difficult, as (I think) you would need to add an additional level of hierarchy to the probabilistic generative model to account for different objects. Or, perhaps it would work to just say there is one big object with multiple components that are independent? I’m not sure.&lt;/p&gt;
</description>
        <pubDate>Mon, 21 Dec 2015 07:02:32 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/21/Schulman2013b.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/21/Schulman2013b.html</guid>
        
        
        <category>Physical reasoning with dynamics models</category>
        
      </item>
    
      <item>
        <title>Model learning for robot control: a survey</title>
        <description>&lt;p&gt;&lt;span id=&quot;Nguyen-Tuong2011&quot;&gt;Nguyen-Tuong, D., &amp;amp; Peters, J. (2011). Model Learning for Robot Control: A Survey. &lt;i&gt;Cognitive Processing&lt;/i&gt;, &lt;i&gt;12&lt;/i&gt;, 319–340. doi:10.1007/s10339-011-0404-1&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As this is a review paper, I will just give a summary and takeaways, and not include methods/algorithm.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This was a really nice survey paper of how dynamics models have been used in robotics (and in particular in control). First, Nguyen-Tuong &amp;amp; Peters distinguish between different model types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Forward models – predict $s_{t+1}$ from $s_t$ and $a_t$&lt;/li&gt;
  &lt;li&gt;Inverse models – infer $a_t$ from $s_t$ and $s_{t+1}$&lt;/li&gt;
  &lt;li&gt;Mixed models – a combination of forward models and inverse models, which helps to constrain inverse models which are frequently ill-defined (i.e. there is no unique solution)&lt;/li&gt;
  &lt;li&gt;Multi-step prediction models – predict the state for the next $k$ timesteps. This is more than just applying the forward model $k$ times, as this means errors accumulate. Instead there are “autoregressive” methods that use past predicted values to predict future values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Second, Nguyen-Tuong &amp;amp; Peters discuss different architectures for learning these different types of models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Direct modeling – learn a direct mapping between inputs and outputs. This can be used to learn all the types of models described above, though it can only be used to model inverse and mixed models if they are invertible.&lt;/li&gt;
  &lt;li&gt;Indirect modeling – use error from a feedback controller to optimize the model. This can be used for learning inverse and mixed model types.&lt;/li&gt;
  &lt;li&gt;Distal teacher – first learn a forward model, and then use the learned forward model to generate errors for use in learning an inverse model. This is mainly used for modeling mixed models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Third, Nguyen-Tuong &amp;amp; Peters discuss challenges for model-based learning, which are (copied directly from Table 2, pg. 13):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data challenges: high dimensionality, smoothness, richness of data, noise, outliers, redundant data, missing data&lt;/li&gt;
  &lt;li&gt;Algorithmic constraints: incremental updates, real-time, online learning, efficiency, large data sets, prior knowledge, sparse data&lt;/li&gt;
  &lt;li&gt;Real-world challenges: safety, robustness, generalization, interaction, stability, uncertainty in the environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fourth, Nguyen-Tuong &amp;amp; Peters given an overview of how some of the above challenges can be addressed by machine learning techniques. They break the approaches down into two main categories: global learning techniques (which learn the entire global function) and local techniques (which approximate local functions around a fixed point). They give a lot of examples but don’t go much into the details, so I will need to take a further look at some of their referenced papers.&lt;/p&gt;

&lt;p&gt;Finally, the authors present three case studies as examples of how predictive model-based control has been successfully applied. Their first case study, on “simulation-based optimization”, refers to using forward models for controlling a helicopter. In particular, they reference one particular paper (Ng et al.) in which they use explicit rollouts of the simulator in order to learn the appropriate control policy. Their second case study involves “approximation-based inverse dynamics control”, in which inverse dynamics must be learned (rather than pre-programmed) because for most robotic platforms, rigid-body physics is not applicable. Their third case study focuses on “learning operational space control”, which is the idea of learning a control policy in task space, rather than in e.g. position space.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;There are a few interesting things that I noted while reading this article. First, the traditional notion of discriminative/generative models kind of breaks down in this discussion. Nguyen-Tuong &amp;amp; Peters are talking in general about learning forward (i.e. generative models), yet they discuss ways to learn these models which resemble discriminative (i.e., direct learning) or generative (i.e., distal teacher learning) approaches. So, I suppose if you are learning a hierarchical model, you could have different parts be generative or discriminative—the whole thing doesn’t have to be one way or the other.&lt;/p&gt;

&lt;p&gt;Nguyen-Tuong &amp;amp; Peters also discuss how the combination of forward and inverse models (i.e. mixed models) is useful because “information encoded in the forward model can help to resolve the non-uniqueness, i.e., the ill-posedness, of the inverse model” (pg. 7). This is not something I’d thought about before in that particular way—I’ve thought for a long time that it’s useful to have a generative model and then learn a discriminative model from that, but I hadn’t thought about the possibility that the generative model could constrain the discriminative model when the function it’s trying to learn isn’t one-to-one.&lt;/p&gt;

&lt;p&gt;I’m interested in reading more about the distal-teacher learning technique, which sounds to me like something that could potentially be used when learning from mental simulations. Though, Nguyen-Tuong &amp;amp; Peters say that “distal teacher learning has several potential disadvantages, such as learning stability, numerical stability, and error accumulation” (pg. 13). They don’t cite sources for that, unfortunately. But in general I would like to understand better the strengths and weaknesses of that approach.&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Dec 2015 18:51:14 -0500</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/20/Nguyen-Tuong2011.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/20/Nguyen-Tuong2011.html</guid>
        
        
        <category>Physical reasoning with dynamics models</category>
        
      </item>
    
  </channel>
</rss>
