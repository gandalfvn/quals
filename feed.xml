<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 01 Jan 2016 18:01:06 -0800</pubDate>
    <lastBuildDate>Fri, 01 Jan 2016 18:01:06 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Are things that are hard to physically move also hard to imaging moving?</title>
        <description>&lt;p&gt;&lt;span id=&quot;Flusberg2011&quot;&gt;Flusberg, S. J., &amp;amp; Boroditsky, L. (2011). Are things that are hard to physically move also hard to imagine moving? &lt;i&gt;Psychonomic Bulletin And Review&lt;/i&gt;, &lt;i&gt;18&lt;/i&gt;(1), 158–164. doi:10.3758/s13423-010-0024-2&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Flusberg &amp;amp; Boroditsky begin with the following important question (pg. 158-159):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;On the one hand, there appear to be tight links among perception, action and mental imagery, both in behavior and in the underlying neural circuitry… On the other hand, what often seems to separate our imagination from the everyday world is that it appears free from the constraints imposed by the physical environment. Indeed, people often imagine actions or events that they have never seen or never could experience themselves… to what extent do our experiences with objects in the real world impinge on our ability to represent and manipulate those objects in mental imagery?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To get at this question, they perform the classic mental rotation task but prime participants first by having them physically rotate the objects. The key manipulation was that some of the objects were more difficult to physically rotate than the others (denoted by color). Flusberg &amp;amp; Boroditsky found that participants had longer response times when using &lt;em&gt;motor imagery&lt;/em&gt; after performing the physical rotations (whereas there was no effect when using &lt;em&gt;visual imagery&lt;/em&gt;), and also that there was an effect of the angle of rotation on the difficulty of rotation (i.e. the slope for the more difficult objects was higher than that for the less difficult objects, which is what would be expected from manually performing the rotations). However, they only saw this effect after excluding some of the participants post-hoc.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think this is a cool study, though I would have liked to see a more in depth evaluation—for example, more angles of rotation, and more than two levels of difficulty. I also think it would have been better to have participants do the mental rotation task on entirely different objects than the ones they physically rotated, because it’s not clear to me whether they measured an effect of &lt;em&gt;prior knowledge&lt;/em&gt; (i.e. abstract knowledge about the difficulty of manipulating the object) or an effect of &lt;em&gt;prior experience&lt;/em&gt; (i.e. they are just replaying an experience they had). I would be inclined to believe they actually are measuring prior knowledge (as they claim), but I would like to know for sure.&lt;/p&gt;

&lt;p&gt;Methodology aside, the fact that there is a difference between motor imagery and visual imagery is quite interesting. I want to know &lt;em&gt;why&lt;/em&gt; this difference exists, though; I’m not convinced there is actually a hard line that can be drawn between the two. Typically, tasks like mental rotation do still involve some amount of motor activation in the brain, meaning that motor imagery isn’t entirely separate from visual imagery. Perhaps the distinction shouldn’t be between motor/visual, but just based on the mechanism by which the object is moving (or, put another way, how the simulated scene is constructed). If you are asked to imagine moving it yourself, then it makes sense that you would imagine what the effects of your motor actions are. If you are asked to imagine the object itself moving, then it perhaps you are imagining (for example) the object sitting on a rotating disk. It would be interesting to try to come up with a task that would require the “visual imagery” condition to still engage in some sort of motor imagery, but motor imager that is irrelevant to how difficult the object is to move. Would that cause an effect of difficulty to appear in the visual imagery condition? I hypothesize that it wouldn’t, which would imply that the distinction isn’t between motor and visual imagery but in terms of how the simulation is constructed, somehow.&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Jan 2016 09:29:15 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Flusberg2011.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Flusberg2011.html</guid>
        
        
        <category>Mental imagery</category>
        
      </item>
    
      <item>
        <title>The emulation theory of representation: motor control, imagery, and perception</title>
        <description>&lt;p&gt;&lt;span id=&quot;Grush2004&quot;&gt;Grush, R. (2004). The emulation theory of representation: motor control, imagery, and perception. &lt;i&gt;The Behavioral And Brain Sciences&lt;/i&gt;, &lt;i&gt;27&lt;/i&gt;(3), 377–96; discussion 396–442. doi:10.1017/S0140525X04000093&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Wow, this was such a cool paper to read! Grush lays out the “emulation” theory of motor control/perception/imagery/etc. which is very similar to the theory that I have been thinking of (but that I haven’t been able to explain nearly as eloquently). It is what he calls a “pseudo-closed-loop control” scheme, inspired by the ideas behind Kalman filtering. The core ideas from the Kalman filter are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;There is some process that is created by a combination of a driving force as well as the internal process mechanism (and some noise in that mechanism). Measurements are made of the state of this process, but they are also corrupted by noise.&lt;/li&gt;
  &lt;li&gt;There is an internal model (emulator) of the process mechanisms. Using this internal model plus the driving force, the next state of the system is computed.&lt;/li&gt;
  &lt;li&gt;A measurement is made of the simulated state, and it is compared to the measurement that was made from the true process. The difference between these two measurements is combined with the inverse of the observation in order to compute a residual correction.&lt;/li&gt;
  &lt;li&gt;Some proportion of correction is applied to the estimated state in order to produce a corrected estimate of the measurement.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In terms of cognition, the process is something like the state of a limb, or a representation of a scene. The driving force is the motor commands that are issued to change the state of the system. So, the hypothesis is essentially that during motor control or perception, the mind uses an internal model (the emulator) to keep track of a better estimate of the state of the system. But, by having an emulator, it can also be used for other things (such as mental imagery). In the case of imagery, the emulator itself can be used to make predictions about the future state of the system (without applying corrections).&lt;/p&gt;

&lt;h2 id=&quot;motor-imagery&quot;&gt;Motor imagery&lt;/h2&gt;

&lt;p&gt;Grush makes the distinction between &lt;em&gt;emulation theory&lt;/em&gt; and &lt;em&gt;simulation theory&lt;/em&gt;, where simulation theory is the idea that motor imagery comes from simulating motor commands, but not actually executing those controls. Emulation theory includes that aspect, but also includes the idea of having a separate model that emulates the state of the system. To quote from the paper:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;The differenceis that the simulation theory does not posit anything corresponding to an emulator&lt;/em&gt;; as far as I can tell, the simulator theory is conceived against the backdrop of closed-loop control, and motor imagery hypothesized to be the free-spinning of the controller (motor centers) when disengaged from the plant (body). In the emulation theory, by contrast, imagery is not produced by the mere free-spinning opertion of the efferent motor areas, but by the efferent motor areas driving an emulator of the musculoskeletal system. (pg. 382)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;visual-imagery&quot;&gt;Visual imagery&lt;/h2&gt;

&lt;p&gt;In discussing how emulation theory applies to perception and visual imagery, Grush makes the distinction between &lt;em&gt;modal emulators&lt;/em&gt; and &lt;em&gt;amodal emulators&lt;/em&gt;. A modal emulator is an emulator which mimics one of the senses (e.g., by representing the state as pixels). An amodal emulator, by contrast, represents the state using a more structured abstract or symbolic representation (e.g., the underlying geometry, pose, and kinematics of objects and where the agent is in relation to them). The amodal emulator is clearly more flexible and encodes much more information than modal emulators; however, the modal emulators may be better at predicting information that is specific to a particular sensory domain. Thus, Grush proposes that the mind probably makes use of an amodal emulator as well as multiple modal emulators for vision, motor control, audition, etc.&lt;/p&gt;

&lt;h2 id=&quot;sensation-and-perception&quot;&gt;Sensation and perception&lt;/h2&gt;

&lt;p&gt;Grush goes as far as to say that modal emulators are really used in &lt;em&gt;sensation&lt;/em&gt; (i.e. receiving and processing sensory signals) while a particular amodal emulator, the &lt;em&gt;environment emulator&lt;/em&gt;, is what really produces our &lt;em&gt;perception&lt;/em&gt; (the interpretation of our sensations). The environment emulator is object-centric, and presumably is what allows us to do things like predict physical events.&lt;/p&gt;

&lt;p&gt;There are two points that Grush makes here that are, in my opinion, very important. The first is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… the current scheme, exactly because it treats perception as one aspect of an integrated information processing strategy, sheds light on the nature of perception itself. In the first place, the scheme highlights the extent to which the outcome of the perceptual process, the state estimate embodied in the emulator, is tuned to sensorimotor requirements. The emulator represents objects and the environment &lt;em&gt;as things engaged with&lt;/em&gt; in certain ways as opposed to how they are considered apart from their role in the organism’s environmental engagements. (pg. 393)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think this is an important point because I think the way we reason about the world—and the objects in it—is precisely because we have to interact in the world. The point of having an ability for mental simulation (emulation) isn’t because it’s useful for higher level problem solving or creativity, but because it’s essential for planning and predicting what is going to happen.&lt;/p&gt;

&lt;p&gt;The second important point is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another shift in emphasis suggested by this account is that perception is shown to be not a matter of starting with materials provided in sensation and filling in blanks until a completed percept is available. Rather, completed percepts of the environment are the starting point, in that the emulator always has a potentially self-contained environment emulator estimate up and running… The role played by sensation is to constrain the configuration and evolution of this representation. In motto form, &lt;em&gt;perception is a controlled hallucination process&lt;/em&gt;. (pg. 393)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I love the characterization of perception as a controlled hallucination process. This is also exactly in line with a Bayesian interpretation: the prior is the internal model of how the world works (the hallucination), and the likelihood constrains the prior based on actual observations of the real world.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think the emulation theory proposed by Grush is exactly the right way to think about perception and action. However, it is also a very high-level schematic of how to approach the problem: there are many questions that aren’t answered. For example:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How are the internal models learned? Here, perspectives from robotics might be particularly useful. For example, something like the “distal teaching” approach discussed by &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/20/Nguyen-Tuong2011.html&quot;&gt;Nguyen-Tuong &amp;amp; Peters&lt;/a&gt; for training a mixed model (forward plus inverse models) could be used to learn new forward models and their inverses.&lt;/li&gt;
  &lt;li&gt;The emulation theory as discussed here just takes as input to the controller some high level goal, which may or may not be produced by using the emulator to make further predictions. What is the tradeoff between using the emulator for tracking the current state of the system vs. doing planning? Or are there multiple emulators used?&lt;/li&gt;
  &lt;li&gt;How does the controller decide what control signal to produce? Again, this is something that could potentially engage the emulator, which means there is also potentially a tradeoff for how the emulator gets used. For both this point and the previous point, work in robotics may provide some insight.&lt;/li&gt;
  &lt;li&gt;The Kalman filtering approach described by Grush assumes that the forward dynamics are the same for both the real and imagined process. What if they aren’t? What sorts of implications does this model mismatch have?&lt;/li&gt;
  &lt;li&gt;If such emulators are used for imagery, how is the initial state for them constructed in the first place?&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 01 Jan 2016 07:46:02 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Grush2004.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Grush2004.html</guid>
        
        
        <category>Mental imagery</category>
        
      </item>
    
      <item>
        <title>Explorations of a creative visual synthesis in mental imagery</title>
        <description>&lt;p&gt;&lt;span id=&quot;Finke1988&quot;&gt;Finke, R. A., &amp;amp; Slayton, K. (1988). Explorations of creative visual synthesis in mental imagery. &lt;i&gt;Memory And Cognition&lt;/i&gt;, &lt;i&gt;16&lt;/i&gt;(3), 252–257. doi:10.3758/BF03197758&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Finke and Slayton asked participants to use mental imagery to imagine objects composed out of a given set of basic geometric parts. They found that in almost 40% of trials, participants were able to come up with a recognizible arrangement of the parts, demonstrating that mental imagery isn’t just used as a way of encoding visual information, but that it is an active process of simulation that people can use to discover new things.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;On each trial, participants saw three geometric forms (out of: circle, square, rectangle, triangle, horizontal line, vertical line, D, C, L, T, J, 8, X, V, P) and were told to combine them into a recognizable form. They could scale and rotate the parts, but not skew or otherwise distort them. Participants had 2 minutes to come up with a response, at the end of which they were to write down the name of the form they came up with (if they were able to come up with one) and then to draw out the form. Once they had drawn the form they couldn’t change their answer (to ensure that they were actually using mental imagery).&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think these result are really cool, and illustrate how powerful mental imagery really can be: it’s not just some epiphenomenon, but a true generative process that can be used to run a certain type of simulation.&lt;/p&gt;

&lt;p&gt;I also think this paper is interesting because it presents a uniquely difficult challenge for AI systems or computational models. How do you model the synthesis of something entirely new? I would argue that it’s not truly a new synthesis, because we have strong prior knowledge about object categories and prototypes. One potential way to approach this would be to do something like Google’s inceptionism. First, train a neural network to recognize various categories of images. Second, generate an input image from the parts used in this experiment. Then, “enhance” that image to maximize the network’s objective function, except that rather than modifying the image on the pixel level, modify it by changing the position, rotation, and scale of the parts. I wonder if doing this would actually produce something recognizable—I’m not sure if having such schematized line drawings would work, but it would be a cool thing to try.&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Jan 2016 03:20:28 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Finke1988.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Finke1988.html</guid>
        
        
        <category>Mental imagery</category>
        
      </item>
    
      <item>
        <title>Aspects of a cognitive neuroscience of mental imagery</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kosslyn1988&quot;&gt;Kosslyn, S. M. (1988). Aspect of a Cognitive Neuroscience of Mental Imagery. &lt;i&gt;Science&lt;/i&gt;, &lt;i&gt;240&lt;/i&gt;(4859), 1621–1626. Retrieved from http://www.jstor.org/stable/1701012&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Kosslyn presents neuroscientific evidence for how mental images are generated in the brain. He argues there are two separate systems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The first system encodes object &lt;em&gt;shape&lt;/em&gt;, which may be either a low-resolution image of the entire object or higher-resolution images of object parts&lt;/li&gt;
  &lt;li&gt;The second system encodes object &lt;em&gt;location&lt;/em&gt;, which may be either represented categorically (e.g. using symbolic/linguistic terms, such as “next to” or “inside of”) or via absolute coordinates.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These two systems interact so that when a mental image is generated, “the patterns in the images are built up by activating parts individually and that parts are imaged in roughly the order in which they are typically drawn” (pg. 1622).&lt;/p&gt;

&lt;p&gt;In particular, Kosslyn hypothesizes (and provides evidence for) the left cerebral hemisphere being better at arranging object parts according to categorical relations, while the right cerebral hemisphere is better at arranging object parts according to absolute locations. Both hemispheres are equally good at generating whole objects (that are not broken down into individual parts).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Kosslyn reports results both from split-brain patients as well as undergraduates. Both types of experiments rely on a technique called &lt;em&gt;lateralization&lt;/em&gt;, in which the stimulus is presented in only the left or right half of the visual field for a brief period of time (150ms; short enough such that participants could not move their eyes to move the stimulus into the other part of the visual field). In the case of the split-brain patients, there is no communication between the halves of the brain, so only one half of the brain will receive the visual information. In the case of the undergraduates with intact corpus callosi, Kosslyn measured reaction time (i.e. something presented to the right side of the brain that requires the left side of the brain for processing should elicit slower reaction times than if the same thing were directly presented to the left side of the brain).&lt;/p&gt;

&lt;p&gt;For the split-brain patient J.W., one task was to observe a lowercase probe and then determine whether the uppercase version of the letter had any curved lines. For the left hemisphere, J.W. had perfect performance; for the right hemisphere, performance was significantly reduced. This result suggests that the left hemisphere is involved in generating images that are composed of parts (e.g. strokes). To further supplement this finding, they ran another experiment that involved judging dimensions of holistic object (e.g., if an object like a belt buckle was wider than it was tall). In this case, both hemispheres performed equally well.&lt;/p&gt;

&lt;p&gt;For the undergraduates, the task was to view a lowercase letter in a grid. There were probe points in some of the grid cells, and participants had to determine wither the uppercase version of the letter would cover the probe points. As predicted, responses were faster when the probe was presented to the left hemisphere rather than the right hempisphere. If instead of a grid the letter was presented in just a box, participants were forced to remember the absolute position of the letter parts (rather than their symbolic relations) and in this case, the left hemisphere performed worse tha the right hemisphere.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a collection of really elegant results showing how mental images are stored and produced in the brain. It would be interesting to come up with a model that efficiently trades off between the different forms of representation to solve these tasks. Such a model should store representations at different levels of granularity (Joe Austerweil has some work related to this, which I want to remember to reread now) and should additionally store representations both using symbolic relations as well as absolute positions. This is actually pretty similar to Brenden Lake’s work on motor program induction—I’m not sure about the representation of where the parts go (I will have to go back and read the details more carefully) but I think it is symbolic/categorical. So it would be interesting to try to augment that sort of model with the capability for also absolute positioning, and different granularities of the parts.&lt;/p&gt;

&lt;p&gt;I also wonder how these types of representations interact with manipulation of the images. For example, if I say “imagine a F lying on its side”, do people imagine an upright F, and the rotate it holistically? Or do they imagine one part of the F, rotate it, and then attach the other parts to it? Or do they imagine all the parts rotated independently, and then attach them to each other? Subjectively, I feel like I first generate the image, and then rotate it, but I’m not sure subjective reports are necessarily reliable. Based on the work from &lt;a href=&quot;/quals/mental%20imagery/2015/12/31/Just1976.html&quot;&gt;Just &amp;amp; Carpenter&lt;/a&gt;, the answer seems more likely to be that one part is rotated, and then the others are “attached”.&lt;/p&gt;
</description>
        <pubDate>Thu, 31 Dec 2015 09:41:38 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20imagery/2015/12/31/Kosslyn1988.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20imagery/2015/12/31/Kosslyn1988.html</guid>
        
        
        <category>Mental imagery</category>
        
      </item>
    
      <item>
        <title>Eye fixations and cognitive processes</title>
        <description>&lt;p&gt;&lt;span id=&quot;Just1976&quot;&gt;Just, M. A., &amp;amp; Carpenter, P. A. (1976). Eye fixations and cognitive processes. &lt;i&gt;Cognitive Psychology&lt;/i&gt;, &lt;i&gt;8&lt;/i&gt;, 441–480. doi:10.1016/0010-0285(76)90015-3&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Just &amp;amp; Carpenter explore three different tasks that rely on high-level cognitive processing, and recorded the eye movements of participants while performing these tasks. Their main hypothesis is that participants fixate on a region that is involved in whatever computation is currently taking place in the mind. I’m only going to go through the first of their experiments, however, as it is the most related to mental imagery and I feel the other two experiments are somewhat less revealing. Regarding the first mental rotation experiment, they ask three questions:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;How does the subject know which parts of the figure are to be rotated into each other?&lt;/li&gt;
    &lt;li&gt;How does the subject know how far to rotate one of the objects?&lt;/li&gt;
    &lt;li&gt;Once the required rotation has been performed, how does the subject know whether the two figures represent the same object or not?&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Participants performed essentially the same task as that from &lt;a href=&quot;/quals/mental%20imagery/2015/12/31/Shepard1971.html&quot;&gt;Shepard &amp;amp; Metzler&lt;/a&gt;, though with far less data (120 trials vs 1600) and also fewer subjects (3 vs 8). Just &amp;amp; Carpenter find similar results, though not identical, results (i.e. monotonically increasing response times).&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Just &amp;amp; Carpenter propose the following model of cognitive processing:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, participants engage in a “search” procedure, in which they look back and forth between the two images to find a corresponding segment of the objects.&lt;/li&gt;
  &lt;li&gt;Second, participants engage in the “transform and compare” procedure, in which they mentally rotate the images toward each other in increments of $50^\circ$ until the images are within $25^\circ$ of each other.&lt;/li&gt;
  &lt;li&gt;Third, participants engage in the “confirmation” procedure, in which they check that the other segments in the images also align. There are two proposed ways that they do this: either by executing a transformation on an additional segment of the stimulus, or by checking that the relations between the segments and the central part of the image are the same.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think it makes sense that people would only actually focus on a single portion of the image at a time, rather than holistically transforming the entire image. I feel though that Just &amp;amp; Carpenter haven’t answered their first question, which is how exactly the “search” procedure works. They do bring up an interesting point regarding this, which is that one possible reason for the increase in response time is that participants pick one segment in one image, and then try to find that segment in the other image, first looking at the same location in the second image—so by design, it will take them longer to find the corresponding segment if it is further away (as it would be due to rotation).&lt;/p&gt;

&lt;p&gt;I wonder how this procedure works with other types of objects, that don’t necessarily have such discrete “segments” as the stimuli used in this experiment. I suppose that this question might not actually be that relevant for realistic stimuli, though. If we use something like mental rotation to e.g. identify similar objects in the world, then we probably first rely on high-information, high-level features (color, texture, overall shape). Only if those features are absent (as is the case with the Shepard &amp;amp; Metzler stimuli) would one need to do something more detailed like a mental rotation.&lt;/p&gt;

&lt;p&gt;One takeaway from all of this, I feel, is that mental rotation isn’t really all that useful/important in and of itself—I would hypothesize that we are not able to do rotations for the sake of doing rotations, but that we need that sort of capability in order to reason about how objects interact with each other or how they should be manipulated (e.g. how should I rotate this object so that it fits in the drawer?). So under a rational analysis the question isn’t, “how does mental rotation work?”, but rather, “what problem is mental rotation solving?”. It would be nice to be able to come up with a realistic task (or set of tasks), for example, in which mental rotation is clearly the right solution, and then see how people do it, and then compare models that do or do not use mental rotation to solve the task. Maybe some sort of bin-packing problem? I am sure someone has studied that sort of thing in cognitive science; I should remember to look up some of that research.&lt;/p&gt;
</description>
        <pubDate>Thu, 31 Dec 2015 02:25:12 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20imagery/2015/12/31/Just1976.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20imagery/2015/12/31/Just1976.html</guid>
        
        
        <category>Mental imagery</category>
        
      </item>
    
      <item>
        <title>Mental rotation of three-dimensional objects</title>
        <description>&lt;p&gt;&lt;span id=&quot;Shepard1971&quot;&gt;Shepard, R. N., &amp;amp; Metzler, J. (1971). Mental Rotation of Three-Dimensional Objects. &lt;i&gt;Science&lt;/i&gt;, &lt;i&gt;171&lt;/i&gt;(3972), 701–703. doi:10.1126/science.171.3972.701&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Shepard and Metzler showed participants 2D line drawings of 3D objects which differed by either a rotation (the “same” pairs) or a rotation and a reflection (the “different” pairs). They found that as the angle of rotation increased for the “same” pairs, participants’ response times increased linearly. The conclusion (based both on this response time data, as well as experience reports) was that participants were performing a “mental rotation” of the images in order to compare them.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Shepard &amp;amp; Metzler had 8 participants who each judged 1600 (!) pairs of line drawings. 800 of the pairs were “picture plane” rotations, while the other 800 were “depth” rotations. Each of these sets of stimuli were further broken down into 400 “same” and 400 “different” pairs. These 400 pairs were composed of 10 unique stimuli repeated twice at each one of ten angles from $0^\circ$ to $180^\circ$ in $20^\circ$ increments (side note: there seems to be another factor of 2 missing here; I’m not sure where that comes from?). Participants had as much time to respond as they wanted, and they could look at the images as long as they wanted, though they were instructed to respond as quickly as possible while still being accurate.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I still think this is such a cool result, though I am still unsatisfied by the rotation explanation. One quote from the paper is potentially illuminating, though:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the postexperimental interview, the subjects typically reported that they attempted to rotate one end of one object into congruence with the corresponding end of the other object; they discovered that the two objects were &lt;em&gt;different&lt;/em&gt; when, after this “rotation,” the two free ends still remained noncongruent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This point is going to elaborated on further in the next paper I am going to read, by Just &amp;amp; Carpenter.&lt;/p&gt;
</description>
        <pubDate>Thu, 31 Dec 2015 00:46:36 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20imagery/2015/12/31/Shepard1971.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20imagery/2015/12/31/Shepard1971.html</guid>
        
        
        <category>Mental imagery</category>
        
      </item>
    
      <item>
        <title>Robust Adaptive Markov Decision Processes</title>
        <description>&lt;p&gt;&lt;span id=&quot;Bertuccelli2012&quot;&gt;Bertuccelli, L. F., Bethke, B., &amp;amp; How, J. P. (2012). Robust adaptive Markov decision processes: Planning with model uncertainty. &lt;i&gt;IEEE Control Systems Magazine&lt;/i&gt;. doi:10.1109/MCS.2012.2205478&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Bertuccelli et al. describe the &lt;em&gt;Robust Adaptive Markov Decision Process&lt;/em&gt; (RAMDP), which is a type of MDP that takes into account uncertainty in the transition dynamics. This is somewhat similar to the BAMDP (see e.g. &lt;a href=&quot;/quals/planning%20and%20decision%20making/2015/12/19/Guez2013.html&quot;&gt;Guez et al.&lt;/a&gt;), which maintains a posterior distribution over the transition dynamics and/or reward function, and acts in a Bayes-optimal manner to maximize reward according to the posterior (i.e. by marginalizing over the uncertain dynamics). In contrast, the RAMDP maximizes a lower bound on the reward:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_R^*(i_0)=\min_{\mathcal{A}^\mu}\max_\mu \mathbb{E}[J_\mu(i_0)]&lt;/script&gt;

&lt;p&gt;where $\mathcal{A}$ is the transition model, $\mu$ is the policy, $J_R$ is the robust objective function, $J_\mu$ is the objective function under policy $\mu$, and $i$ is the state.&lt;/p&gt;

&lt;p&gt;To compute the minimum over transition models, they first define a feasible uncertainty set which, in this case, they choose to be the Dirichlet distribution. They then use a “scenario-based method”, which seems to just be sampling, over which to compute the minimum. Rather than taking a Monte-Carlo sampling approach, they compute &lt;em&gt;sigma points&lt;/em&gt;, which are optimal sampling locations based on the mean and variance of the Dirichlet distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{Y}_0&amp;=\bar{p}\\
\mathcal{Y}_i&amp;=\bar{p}+\beta_i(\Sigma^{1/2})_i\ \ \mathrm{for\ all}\ i\leq N\\
\mathcal{Y}_i&amp;=\bar{p}-\beta_i(\Sigma^{1/2})_i\ \ \mathrm{for\ all}\ N+1\leq i &lt;2N
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\beta$ is “a tuning parameter that reflects the level of conservatism desired” (i.e. the range of the credible region), and $\bar{p}$ and $\Sigma$ are the mean and covariance of the Dirichlet distribution.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;It’s not entirely clear to me in terms of theoretical guarantees whether BAMDP or RAMDP is better. My intuition is that BAMDP is probably more flexible, as it could potentially be used with complex structured priors. I suppose you could technically do the same for RAMDPs as well, though you would need to reformulate how to perform efficient sampling (since the approach taken by Bertuccelli et al. makes a strong assumption about the Dirichlet distribution). It seems that BAMDPs may be a little bit more agnostic of the assumptions put into the prior, particularly when used with something like MCTS. That said, I suspect that RAMDP probably provides better worst-case guarantees given that it’s explicitly optimizing for the worst case. But, these are just my intuitions; I’m not sure how true that is in practice or how much the worst case matters in non-adversarial settings.&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Dec 2015 23:54:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2015/12/30/Bertuccelli2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2015/12/30/Bertuccelli2012.html</guid>
        
        
        <category>Planning under uncertain dynamics</category>
        
      </item>
    
      <item>
        <title>Model-free probabilistic movement primitives for physical interaction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Paraschos2015&quot;&gt;Paraschos, A., Rueckert, E., Peters, J., &amp;amp; Neumann, G. (2015). Model-Free Probabilistic Movement Primitives for Physical Interaction. &lt;i&gt;Proceedings Of the IEEE/RSJ Conference on Intelligent Robots and Systems&lt;/i&gt;. Retrieved from http://www.ausy.tu-darmstadt.de/uploads/Team/PubAlexParaschos/Paraschos_IROS_2015.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this work (and in previous work), Paraschos et al. present a framework for learning “probabilistic motion primitives” (ProMPs), which is essentially a way of learning a distribution over trajectories. In this paper, they explicitly encode the trajectory distributions to not just be over states, but also over controls (forces/torques) and sensory information. That is, they define a trajectory as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbf{y}_t&amp;=[q_t,\dot{q}_t,u_t]^\top\\
\mathbf{\tau}&amp;=[\mathbf{y}_1, \ldots{}, \mathbf{y}_T]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;They then define the trajectory in terms of basis functions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{\tau}\vert w)=\prod_t \mathcal{N}(\mathbf{y}_t\vert \mathbf{\Phi}_t \mathbf{w},\mathbf{\Sigma_y})&lt;/script&gt;

&lt;p&gt;and learn the distribution over trajectories in terms of a hierarchical Bayesian model, marginalizing out the weights:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{\tau};\mathbf{\theta})=\int p(\mathbf{\tau}\vert \mathbf{w})p(\mathbf{w};\mathbf{\theta})\ \mathrm{d}\mathbf{w}&lt;/script&gt;

&lt;p&gt;Paraschos et al. assume that pretty much everything is Gaussian, which allows them to derive nice analytical solutions for everything, including the controller $p(\mathbf{u}_t\vert \tilde{\mathbf{y}}_t)$ (where $\tilde{\mathbf{y}}_t$ is the observable state of the system).&lt;/p&gt;

&lt;p&gt;As described in their previous paper (NIPS ‘13), this type of system provides a lot of nice properties: it is easy to condition on a particular aspect of a trajectory; to combine multiple motion primitives by multiplying the distributions; to blend multiple primitives according to an activation function; etc. By also including the control information, they are not only able to produce the desired trajectory in these scenarios, but the controls to achieve that trajectory as well. This is a somewhat similar approach to &lt;a href=&quot;/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Lee2015.html&quot;&gt;Lee et al.&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a cool paper that really demonstrates the power of maintaining a full generative model. It brings up an interesting distinction between model-based control and generative models—in this paper, Paraschos et al. explicitly are not using a model-based approach for controlling the robot (i.e., there is no dynamics model of the robot or the object), but they do estimate a full joint distribution over pose, velocity, force, and sensor data. With enough data, such a joint distribution could be essentially analogous to learning a model for control; in that case, the model is just not explicitly represented. You could even simulate forward by conditioning on the previous state.&lt;/p&gt;

&lt;p&gt;As with all of these papers that I’ve been reading, though, it seems like these approaches that rely solely on demonstrations won’t be able to generalize to novel objects or actions. I suppose that people do have tons of experience; after only a few weeks after birth babies will have already been exposed to thousands of trajectories (of course, those are not expert demonstrations). I wonder how well this sort of approach would work with some type of self-supervised learning: i.e., you learn the joint distribution over states and actions, but the examples you have are self-generated and are not as reliable as expert demonstrations. I also wonder if you could combine this with something like &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Xie2015.html&quot;&gt;Xie et al.&lt;/a&gt;, where instead of learning model-free controls, you do learn an explicit model, but you learn the model online along with the rest of the distribution.&lt;/p&gt;

</description>
        <pubDate>Wed, 30 Dec 2015 09:34:01 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Paraschos2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Paraschos2015.html</guid>
        
        
        <category>Physical reasoning without dynamics models</category>
        
      </item>
    
      <item>
        <title>Stabilizing novel objects by learning to predict tactile slip</title>
        <description>&lt;p&gt;&lt;span id=&quot;Veiga2015&quot;&gt;Veiga, F., van Hoof, H., Peters, J., &amp;amp; Hermans, T. (2015). Stabilizing Novel Objects by Learning to Predict Tactile Slip. &lt;i&gt;Proceedings Of the IEEE/RSJ Conference on Intelligent Robots and Systems&lt;/i&gt;. Retrieved from http://www.ausy.tu-darmstadt.de/uploads/Site/EditPublication/IROS2015veiga.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Veiga et al. compare a variety of supervised machine learning methods for learning to detect and predict tactile slip. They compare both SVMs and random forest classifiers and use features that come from a tactile sensor based on the human finger. More specifically, they have three different ways of constructing features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Single element features – based only on the current timestep, $\phi(\mathbf{x}_{1:t})=\mathbf{x}_t$&lt;/li&gt;
  &lt;li&gt;Delta features – based on the current timestep, as well as the change from the last timestep, $\phi(\mathbf{x}_{1:t})=[\mathbf{x}_t,\Delta\mathbf{x}_t]$&lt;/li&gt;
  &lt;li&gt;Time window features – based on a window of previous time steps, $\phi(\mathbf{x}_{1:t})=\mathbf{x}_{t-\tau:t}$, where $\tau$ is the size of the time window&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The random forest classifiers tend to do the best, particularly with the delta features.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;In this paper, Veiga et al. use purely discriminative methods to detect and predict slip based on features extracted from a tactile sensor. This means that their method works independently of object-specific features and thus does not take into account anything about the actual dynamics of the object. In this case, this approach actually makes a lot of sense to me—it feels like it would be overkill to need to have a full dynamics model of how the object behaves just to determine if it is slipping. That said, I do wonder if having &lt;em&gt;some&lt;/em&gt; approximate knowledge of the object dynamics (e.g. curvature, overall mass, distribution of mass, etc.) would help, particularly in having the controller adjust once it has detected that the object is slipping.&lt;/p&gt;

&lt;p&gt;I also wonder to what extent these learned classifiers will generalize to greater changes in mass/shape/friction—e.g. will the same classifier work for a very heavy object as opposed to a very light object? Or a very smooth object vs. a rough object? And, will it generalize to other types of scenarios where slip might occur (e.g. holding an object with two hands, as opposed to up against a wall; and also, the location where the object is being held).&lt;/p&gt;

&lt;p&gt;This paper also made me think a bit about what constitutes simulation. In this paper, Veiga et al. are able to predict several timesteps into the future whether slip &lt;em&gt;will&lt;/em&gt; occur. Even though they don’t have a dynamics model of how slip changes over time, for example, does this type of prediction still count as “simulation”? I’m tempted to say no, not unless they can predict at each timestep &lt;em&gt;how much&lt;/em&gt; slip is going to occur—e.g. in the next timestep, there will be a small amount of slip, in the following timestep, it will increase by so much, etc. I don’t think this type of prediction would be particularly hard to do, though, if they could perhaps attach sensors to their objects in order to more precisely quantify what slip is (in this paper, they labeled parts of the video as “slip” or “not slip” just by human visual judgments).&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Dec 2015 07:48:29 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Veiga2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Veiga2015.html</guid>
        
        
        <category>Physical reasoning without dynamics models</category>
        
      </item>
    
      <item>
        <title>Learning force-based manipulations of deformable objects from multiple demonstrations</title>
        <description>&lt;p&gt;&lt;span id=&quot;Lee2015&quot;&gt;Lee, A. X., Lu, H., Gupta, A., Levine, S., &amp;amp; Abbeel, P. (2015). Learning Force-Based Manipulation of Deformable Objects from Multiple Demonstrations. &lt;i&gt;Proceedings Of the IEEE International Conference on Robotics and Automation&lt;/i&gt;. doi:10.1109/ICRA.2015.7138997&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper builds on &lt;a href=&quot;/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/29/Schulman2013a.html&quot;&gt;Schulman et al.&lt;/a&gt; by not only warping the pose of the robot’s trajectory, but also the force that is applied to the objects. This makes for a more robust and generalizable scheme for manipulating deformable objects (such as rope).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;The algorithm has several steps:&lt;/p&gt;

&lt;h2 id=&quot;step-1&quot;&gt;Step 1&lt;/h2&gt;

&lt;p&gt;Perform scene registration, where the points from the test scene are mapped to the points from the demonstrated scene in order to come up with a function $f$ that transforms between the two.&lt;/p&gt;

&lt;h2 id=&quot;step-2&quot;&gt;Step 2&lt;/h2&gt;

&lt;p&gt;The trajectories from demonstrated scenes are mapped to the new scene using $f$: the poses are given by $\tilde{\mathbf{Q}}^d=f(\hat{\mathbf{Q}}^d)$ and the forces are given by $\tilde{\mathbf{U}}^d=\frac{df^d}{d\mathbf{p}}(\hat{\mathbf{Q}}^d)\hat{\mathbf{U}}^d$.&lt;/p&gt;

&lt;h2 id=&quot;step-3&quot;&gt;Step 3&lt;/h2&gt;

&lt;p&gt;The trajectories are time-aligned using “dynamic time warping” such that all demonstrated trajectories follow the same time course. This yields $\mathbf{Q}^d$ and $\mathbf{U}^d$.&lt;/p&gt;

&lt;h2 id=&quot;step-4&quot;&gt;Step 4&lt;/h2&gt;

&lt;p&gt;To extract the final trajectory based on the time-aligned demonstrated trajectories, Lee et al. use a Gaussian model of the force:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{u}^d_t\vert \mathbf{q}_t^d,\mathbf{\dot{q}}_t^d)=\mathcal{N}(\mathbf{K}_{pt}(\mathbf{\bar{q}}_t-\mathbf{q}_t^d)+\mathbf{K}_{vt}(\dot{\bar{\mathbf{q}}}_t-\dot{\mathbf{q}}_t^d)+\bar{\mathbf{u}}_t; \mathbf{C}_t)&lt;/script&gt;

&lt;p&gt;Here, $\bar{\mathbf{q}}_t$, $\dot{\bar{\mathbf{q}}}_t$, and $\bar{\mathbf{u}}_t$ are the positions, velocities, and forces of the desired trajectory and are computed by fitting a joint Gaussian distribution to $[\mathbf{q}_t^d,\dot{\mathbf{q}}_t^d,\mathbf{u}_t^d]$ at each time step and then taking the mean of that distribution. The variables $\mathbf{K}_{pt}$ and $\mathbf{K}_{vt}$ are the position and velocity gains and are computed from the covariances of the fitted joint Gaussians: $\mathbf{K}_{pt}=-\Sigma_{\mathbf{uq},t}\Sigma^{-1}_{\mathbf{qq},t}$ and $\mathbf{K}_{vt}=-\Sigma_{\mathbf{u\dot{q}},t}\Sigma^{-1}_{\mathbf{\dot{q}q},t}$ (see Section 8.1.3 of the Matrix Cookbook for a better intuition of where this comes from).&lt;/p&gt;

&lt;p&gt;Rather than directly fitting the covariances $\Sigma_t$, they use a inverse-Wishart prior on the covariance in order to encourage the gains to be non-negative. They additionally include samples from nearby time points which encourages the estimate to vary smoothly over time.&lt;/p&gt;

&lt;h2 id=&quot;step-5&quot;&gt;Step 5&lt;/h2&gt;

&lt;p&gt;Lee et al. now optimize the robot joint angles $\Theta=[\theta_1,\ldots{},\theta_T]^\top$ to match the desired trajectory (as computed in the previous step). Then, they convert the gains into joint-space gains $\mathbf{K}_{pt}^\theta$ and $\mathbf{K}_{vt}^\theta$ and compute the torque as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{f}_t=\mathbf{K}_{pt}^\theta(\theta_t-\theta_\mathrm{obs})+\mathbf{K}_{vt}^\theta(\dot{\theta}_t-\dot{\theta}_\mathrm{obs})+\mathbf{J}(\theta)^\top\bar{\mathbf{u}}_t&lt;/script&gt;

&lt;p&gt;where $\mathbf{J}(\theta)$ is the Jacobian and where $\theta_\mathrm{obs}$ and $\dot{\theta}_\mathrm{obs}$ are the observed joint angles and velocities.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This approach is a significant improvement over previous work, which was unable to generalize to situations in which the applied force was more important than the exact position (e.g. folding a larger towel than the one in the demonstrations requires applying the same force, but moving the arms out further in order to apply that force). The core idea is still the same, however: without having any task-specific or physical knowledge, map from demonstrated trajectories (including forces) to new situations.&lt;/p&gt;

&lt;p&gt;I think it’s really interesting that they are able to take force into account and trade off between force and position without actually having knowledge of the task; I’m surprised it works as well as it does! The intuition behind why it works is that when there is variance in the demonstrations, that implies that absolute position is less important, and that something else is more important—for example, force. Thus, where there is more variance in position, the force should be considered more strongly, and when there is low variance, position should be considered more strongly. I do wonder though whether this property always holds true. For example, if you were trying to unscrew a cap from a jar, and different demonstrations had caps that were stuck more or less tightly, I &lt;em&gt;think&lt;/em&gt; that the position of the gripper would end up still being about the same—it would be the force that differs. In that case, this wouldn’t work (I think) because it would move the gripper in the appropriate way, but not apply the right force; and in fact, having demonstrations of the force wouldn’t necessarily be sufficient either—to accomplish this task, you would have to have some notion of the position of the jar and the cap and how much the cap is moving, etc.&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Dec 2015 02:52:02 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Lee2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Lee2015.html</guid>
        
        
        <category>Physical reasoning without dynamics models</category>
        
      </item>
    
  </channel>
</rss>
