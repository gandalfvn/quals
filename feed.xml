<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 13 Jan 2016 16:50:57 -0800</pubDate>
    <lastBuildDate>Wed, 13 Jan 2016 16:50:57 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Stable fluids</title>
        <description>&lt;p&gt;&lt;span id=&quot;Stam1999&quot;&gt;Stam, J. (1999). Stable Fluids. &lt;i&gt;Proceedings Of the 26th Annual Conference on Computer Graphics and Interactive Techniques&lt;/i&gt;. doi:10.1145/311535.311548&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper introduces the first stable fluid simulation algorithm for computer graphics. This is a hard problem, because there is not analytic solution to Navier-Stokes. Previous methods had used explicit approximations such as Eulerian methods; however, these types of methods can become unstable and “blow up” for large timesteps. Using small enough timesteps is not a feasible option in computer graphics, in which the simulation needs to run in real time, thus prohibiting small timesteps.&lt;/p&gt;

&lt;p&gt;This algorithm is specific to contained fluids (i.e., gases) and approximates the following form of Navier-Stokes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \mathbf{u}}{\partial t}=\mathbf{P}\left(-(\mathbf{u}\cdot{}\nabla)\mathbf{u}+\nu\nabla^2\mathbf{u}+\mathbf{f}\right)&lt;/script&gt;

&lt;p&gt;where $\nabla\cdot{}\mathbf{u}=0$. Here, $\mathbf{u}$ is the velocity field, $\nabla$ is a vector of spatial partial derivatives, $\nu$ is the kinematic viscosity of the fluid, $\mathbf{f}$ is an external force, and $\mathbf{P}$ is a projection operator which projects any vector field onto its &lt;em&gt;divergence free&lt;/em&gt; part. Divergence free essentially means that at any point in the vector field, the flow towards that point is the same as the flow away from it.&lt;/p&gt;

&lt;p&gt;They compute the approximate solution to this equation in four steps. First, $\mathbf{w}_0$ is the solution from the previous time step:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}_0(\mathbf{x})=\mathbf{u}(\mathbf{x},t)&lt;/script&gt;

&lt;p&gt;Then, the first step is to &lt;em&gt;add force&lt;/em&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}_1(\mathbf{x})=\mathbf{w}_0+\Delta t\ \mathbf{f}(\mathbf{x},t)&lt;/script&gt;

&lt;p&gt;The second step is &lt;em&gt;advection&lt;/em&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}_2(\mathbf{x})=\mathbf{w}_1(\mathbf{p}(\mathbf{x},-\Delta t))&lt;/script&gt;

&lt;p&gt;where $\mathbf{p}(\mathbf{x},-\Delta t)$ says that the new velocity at $\mathbf{x}$ is the velocity at a location backwards in time by $-\Delta t$ (i.e., something like the velocity at the location given by $\dot{\mathbf{x}}\ \Delta t)$.&lt;/p&gt;

&lt;p&gt;The third step is &lt;em&gt;diffusion&lt;/em&gt;, in which the effects of viscosity are applied and solved as a linear system:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(\mathbf{I}-\nu\Delta t\nabla^2\right)\mathbf{w}_3(\mathbf{x})=\mathbf{w}_2(\mathbf{x})&lt;/script&gt;

&lt;p&gt;The fourth and final step is &lt;em&gt;projection&lt;/em&gt;, in which the system is projected back onto its divergence free part.&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 07:53:39 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/physically-based%20animation/2016/01/12/Stam1999.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/physically-based%20animation/2016/01/12/Stam1999.html</guid>
        
        
        <category>Physically-based animation</category>
        
      </item>
    
      <item>
        <title>Rational use of cognitive resources: levels of analysis between the computational and the algorithmic</title>
        <description>&lt;p&gt;&lt;span id=&quot;Griffiths2015&quot;&gt;Griffiths, T. L., Lieder, F., &amp;amp; Goodman, N. D. (2015). Rational Use of Cognitive Resources: Levels of Analysis Between the Computational and the Algorithmic. &lt;i&gt;Topics In Cognitive Science&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;(2), 217–229. doi:10.1111/tops.12142&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Griffiths et al. propose a focus on levels between the computational and algorithmic. They outline a method for approaching analysis at these intermediate levels called &lt;em&gt;resource-rational analysis&lt;/em&gt;. The key idea in resource-rational analysis is to begin with a computational-level analysis, assume an idealized &lt;em&gt;abstract computational architecture&lt;/em&gt; that solves the proplem posed at the computational level, and then examine how resources should be used optimally within that framework. It is specifically &lt;em&gt;not&lt;/em&gt; a proposal to modify the computational level:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Rather than blurring these lines and building constraints into computational-level theories, we suggest a different approach: define the computational-level theory without considering limitations on its execution, and then explore the consequences of those limitations as a further analysis that brings us closer to an algorithmic-level theory… Various proposals about limitations—or alternatively abstract computational architectures—provide us with levels of analysis between the computational and the algorithmic, and the principle of rationality provides us with a methodology for developing models at those intermediate levels. (pg. 220)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Griffiths et al. outline four steps in performing a resource-rational analysis:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;. Perform a computational-level analysis to determine the function of the system.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model of mental computation&lt;/strong&gt;. Pick a class of algorithms that approximate the optimal solution (e.g. particle filters), and define what the costs are in the model (e.g. number of samples).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Optimal resource allocation&lt;/strong&gt;. Determine which algorithm should be used in order to optimally trade off between accuracy and computational cost (i.e., be “resource rational”)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluate and refine&lt;/strong&gt;. Compare to human behavior, and revise assumptions in steps 1, 2, and 3 as necessary.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They posit a very specific definition of resource rationality, based on the notion of &lt;em&gt;value of computation&lt;/em&gt; (VOC), defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
c^*&amp;=\mathrm{arg}\max_{c\in C^n}\mathrm{VOC}(c)\\
\mathrm{VOC}(c)&amp;=\mathbb{E}_{P(B\vert c)}\left[\max_a \mathbb{E}_{P(Q,s\vert B)}[Q(s, a)]\right] - \mathrm{cost}(c)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $c$ is a computation, $s$ is the state, $a$ is the action, $Q(s,a)$ is the $Q$ function, and $B$ is the agent’s belief about $Q$ and $s$.&lt;/p&gt;

&lt;p&gt;Griffiths et al. note that in many cases, the initial resource-rational analysis may start by making certain assumptions of unbounded optimality (e.g., the ability to take perfect posterior samples). Further iterations of the analysis can identify these assumptions and apply a resource-rational analysis to them as well (e.g., correlated MCMC samples).&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I like this approach a lot, and I think that starting with approximation algorithms from computer science is likely to be incredibly fruitful. However, a lot of the algorithms from computer science have proven bounds for large $n$. I wonder to what extent there are other algorithms that computer scientists aren’t necessarily investigating that have provably better bounds (or other relevant properties) for small $n$? For example, are there inference algorithms that do poorly in terms of getting perfect posterior samples as $n$ goes to infinity, but which perhaps give somewhat better samples (e.g. less correlated) in the short term than more popular algorithms like MH?&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 07:53:39 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/rational%20process%20models/2016/01/12/Griffiths2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/rational%20process%20models/2016/01/12/Griffiths2015.html</guid>
        
        
        <category>Rational process models</category>
        
      </item>
    
      <item>
        <title>Selecting computations: theory and applications</title>
        <description>&lt;p&gt;&lt;span id=&quot;Hay2012&quot;&gt;Hay, N. J., Russell, S. J., Tolpin, D., &amp;amp; Shimony, S. E. (2012). Selecting Computations: Theory and Applications. &lt;i&gt;ArXiv Preprint ArXiv:1207.5878v1 [Cs.AI]&lt;/i&gt;. Retrieved from http://arxiv.org/abs/1207.5879&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper is quite related to &lt;a href=&quot;/quals/planning%20and%20decision%20making/2015/12/19/Guez2013.html&quot;&gt;Guez et al.&lt;/a&gt;, &lt;a href=&quot;/quals/planning%20and%20decision%20making/2015/12/16/Browne2012.html&quot;&gt;Browne et al.&lt;/a&gt;, and &lt;a href=&quot;/quals/planning%20under%20uncertain%20dynamics/2015/12/30/Bertuccelli2012.html&quot;&gt;Bertuccelli et al.&lt;/a&gt;. Here, Hay et al. suggest that to optimally make use of information earned when exploring a state/action space, a &lt;em&gt;metalevel&lt;/em&gt; decision problem needs to be solved:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Exploring unpromising or highly predictable paths to great depth is often wasteful; for a given amount of exploration, decision quality can be improved by directing exploration towards those actions sequences whose outcomes are helpful in selecting a good move. Thus, the &lt;em&gt;metalevel&lt;/em&gt; decision problem is to choose what future action sequences to explore (or, more generally, what deliberative computations to do), while the &lt;em&gt;object-level&lt;/em&gt; decision problem is to choose an action to execute in the real world. (pg. 1)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They show that the UCB1 or UCT bounds (used in bandit problems and MCTS) are suboptimal for the metalevel decision problem. The difference, they say, is that in bandit problems, the decision is relative to a utility you get in the real world, but in metalevel problems, the decision is relative to a cost of computation of &lt;em&gt;simulations&lt;/em&gt; rather than real-world actions.&lt;/p&gt;

&lt;p&gt;In some cases, the prior distribution of real-world outcomes is known and can be used in the simulations. However, in many other cases, the prior distribution on utilities is not available. Instead, the VOI (value of information) can be used as a way of determining what actions are good to take. These VOIs cannot be computed exactly, but under a few assumptions (myopic policy, samples are iid, expectation of a selection is equal to the sample mean, the distribution is bounded on both sides), they can be bounded from above. Hay et al. prove this bound, and show how it can be applied to MCTS: for root node sampling, rather than following the UCT policy, they use the VOI policy.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;A &lt;em&gt;metalevel decision process&lt;/em&gt; is denoted $M=(S,s_0,A_s,T,R)$, where $S$ are the states, $s_0$ is the intitial state, $A_s$ is the set of actions which includes simulated actions $E\in\mathcal{E}$ as well as the “stop” action $\perp$, $T$ is the transition function, and $R$ is the reward function, such that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
T(s,E,s^\prime)&amp;=p(E=e\ \vert\ E_1=e_1,\ldots{},E_n=e_n)\\
T(s,\perp,\perp)&amp;=1\\
R(s,E,s^\prime)&amp;=-c\\
R(s,\perp,\perp)&amp;=\max_i\mu_i(s)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mu_i(s)=\mathbb{E}[U_i\ \vert\ E_1=e_1,\ldots{},E_n=e_n]$. The value function of a policy $\pi$ for this MDP is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_M^\pi(s)=\mathbb{E}_M^\pi[-cN+\max_i\mu_i(S_N)\ \vert\ S_0=s]&lt;/script&gt;

&lt;p&gt;where $N$ is the total number of computations performed. The &lt;em&gt;expected&lt;/em&gt; number of computations is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E^{\pi^*}[N\ \vert\ S_0=s]\leq \frac{1}{c}\left(\mathbb{E}[\max_i U_i\ \vert\ S_0=s]-\max_i\mu_i(s)\right)&lt;/script&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;It would be interesting to see how this integrates with the Bayes-adaptive MCTS from &lt;a href=&quot;/quals/planning%20and%20decision%20making/2015/12/19/Guez2013.html&quot;&gt;Guez et al.&lt;/a&gt;. There, not only do they run simulations starting from the root node, they also resample the MDP from the prior distribution. So, I’m not sure whether the VOI approach here would still hold in that case. In general though, I do think it is important to take the metalevel decision making problem into account, so this is a really interesting direction to pursue particularly in the context of human reasoning (where our brains almost certainly need to make tradeoffs between doing more computation and acting).&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 05:15:18 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/rational%20process%20models/2016/01/12/Hay2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/rational%20process%20models/2016/01/12/Hay2012.html</guid>
        
        
        <category>Rational process models</category>
        
      </item>
    
      <item>
        <title>Bayesian fundamentalism or enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition</title>
        <description>&lt;p&gt;&lt;span id=&quot;Jones2011&quot;&gt;Jones, M., &amp;amp; Love, B. C. (2011). Bayesian fundamentalism or enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition. &lt;i&gt;The Behavioral And Brain Sciences&lt;/i&gt;, &lt;i&gt;34&lt;/i&gt;(4), 169–188. doi:10.1017/S0140525X10003134&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Jones &amp;amp; Love critique the use of Bayesian models of cognition, contrasting &lt;em&gt;Bayesian Fundamentalism&lt;/em&gt;, which:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… adheres to the tenet that human behavior can be explained through rational analysis — once the correct probabilistic interpretation of the task environment has been identified — without recourse to process, representation, resource limitations, or physiological or developmental data. (pg. 170)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;with &lt;em&gt;Bayesian Enlightenment&lt;/em&gt;, which:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… goes beyond the dogma of pure rational analysis and actively attemps to integrate with other avenues of inquiry in cognitive science… [it] thus treats Bayesian models as making both rational and mechanistic commitments, and it takes as a goal the joint evaluation of both. (pg. 170)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Jones &amp;amp; Love compare Bayesian Fundamentalism to Behaviorism and Evolutionary Psychology, arguing that Bayesian Fundamentalism doesn’t care about what is going on in the mind, only about the constraints of the environment and what the rational solution to the problem is given those constraints. They argue that despite this position, Bayesian models really &lt;em&gt;do&lt;/em&gt; make mechanistic assumptions about representations and processes. For example, in critiquing &lt;a href=&quot;/quals/theory%20learning/2016/01/09/Kemp2007.html&quot;&gt;Kemp et al.&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;However, it is not Bayes’ Rule or even the notion of overhypotheses that drives the prediction; rather it is the particular overhypotheses that were built into the model. In other words, the model was endowed with the capability to recognize a particular pattern (viz., regularity across words in which perceptual dimensions are relevant to meaning), so the fact that it indeed recognizes that pattern when presented with it is not surprising or theoretically informative. (pg. 178)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They also argue that despite emphasizing how important it is to specify the specific environmental constraints giving rise to the model, Bayesian Fundamentalists rarely do so. And, even if they did, it is nearly impossible to know what the right set of constraints are; in general it is possible to give a post-hoc argument for a particular set of constraints being “rational”:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The relevant environment for rational action could be the local environment present in the laboratory task, similar situations (however defined) that the person has experienced, all experiences over the person’s life, all experiences of our species, all experiences of all ancestral organisms traced back to single cell organisms, and so on. Furthermore, once the relevant environment is specified and characterized, the rational theorist has considerable flexibility in characterizing which relevant measures or statistics from the environment should enter into the optimality calculations. (pg. 181)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Jones &amp;amp; Love note furthermore that from an evolutionary perspective, it is not &lt;em&gt;behavior&lt;/em&gt; that is optimized but &lt;em&gt;mechanism&lt;/em&gt; that is optimized. Whatever the existing mechanism is, it is perhaps a slightly more optimized version of the mechanism that was there before, and so on. That means only that if the behavior is optimized at all, it is locally optimal, not globally optimal, because it depends on the process of optimization that has occurred. But even then, many things may simply be accidents or side effects and are not even optimized at all.&lt;/p&gt;

&lt;p&gt;Jones &amp;amp; Love argue that Bayesian Fundamentalism says nothing about development because Bayesian models are mechanism-free, and therefore it is not clear what develops:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;One key question for any developmental model is what develops. In rational models, the answer is that nothing develops. Rational models are mechanism-free, leaving only information sampled to change over time. Although some aspects of development are driven by acquisition of more observations, other aspects of development clearly reflect maturational changes in the mechanism. (pg. 182)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, the suggest that Bayesian Enlightenment holds promise by:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Treating Bayesian representations (e.g. the generative model or parameteters of conjugate priors) as actual hypotheses about psychological representations&lt;/li&gt;
  &lt;li&gt;Investigating Bayesian approximation algorithms (e.g. MCMC) as possible algorithmic-level processes&lt;/li&gt;
  &lt;li&gt;Performing rational analysis not within the context of the environment, but within the context of a mechanistic model&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Jones &amp;amp; Love actually make a lot of good points about how Bayesian models can be useful and fruitful. I disagree with their claims that they aren’t (or weren’t) already being used in that way. I also disagree with their characterization that Bayesian models aren’t informative if they recognize the patterns they were built to recognize (e.g. in the discussion of Kemp et al.). It is the connection with the data that makes them informative: if we have a hypothesis about how people reason in a particular domain, we can instantiate that explicit hypothesis in a Bayesian model and verify that the hypothesis does actually produce the same behavior as people. The model is just a tool for making the assumptions and implications of the hypothesis explicit.&lt;/p&gt;

&lt;p&gt;I am not sure why Jones &amp;amp; Love think that Bayesian models aren’t committed to hypotheses about psychological representation. The sense in which the computation of Bayesian models “doesn’t matter” is that Bayesians are not committed to the specific way by which the predictions of the model are computed; what they are committed to is the structure of the model. The structured representations offered by Bayesian models are their most powerful feature, in my opinion.&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 02:59:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/12/Jones2011.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/12/Jones2011.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Unfalsifiability and mutual translatability of major modeling schemes in choice reaction time</title>
        <description>&lt;p&gt;&lt;span id=&quot;Jones2014&quot;&gt;Jones, M., &amp;amp; Dzhafarov, E. N. (2014). Unfalsifiability and Mutual Translatability of Major Modeling Schemes for Choice Reaction Time. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;121&lt;/i&gt;(1), 1–32. doi:10.1037/a0034190&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Jones &amp;amp; Dzhafarov show that two popular response and response time (R&amp;amp;T) models—the &lt;em&gt;linear ballistic accumulator&lt;/em&gt; mode and &lt;em&gt;diffusion&lt;/em&gt; models—are unfalsifiable when distributional assumptions (which have previously been assumed to be an implementation detail) are removed. That is, they can be made to fit any R&amp;amp;T distribution, and thus do not provide any explanatory power regarding the mechanism behind responses and response times. Jones &amp;amp; Dzhafarov show how the LBM and diffusion models can be translated into another type of R&amp;amp;T model called the &lt;em&gt;Grice&lt;/em&gt; architecture, which is also unfalsifiable.&lt;/p&gt;

&lt;p&gt;They also discuss two assumptions regarding “selective influence”:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Stimulus information affects growth rates of the response processes, but not starting points or decision thresholds.&lt;/li&gt;
  &lt;li&gt;Speed-accuracy bias affects decision thresholds and possibly the starting point, but not the response processes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first assumption does not actually impose any constraints on the R&amp;amp;T models—they are still unfalsifiable. The second assumption does impose constraints on the R&amp;amp;T models, but Jones &amp;amp; Dzhafarov argue that this assumption is not well motivated. In particular, response processes can be recast in terms of decision thresholds and starting points, so it is not clear why the assumption needs to be made specifically about thresholds and starting points.&lt;/p&gt;

&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;$s$ — stimulus&lt;/li&gt;
  &lt;li&gt;$c$ — condition (or other factor)&lt;/li&gt;
  &lt;li&gt;$r$ — response&lt;/li&gt;
  &lt;li&gt;$t$ — response time&lt;/li&gt;
  &lt;li&gt;$G^{s,c}(r,t)$ — joint distribution over responses and upper bound on response time&lt;/li&gt;
  &lt;li&gt;$h^{s,c}(r,t)$ — joint hazard function specifying the probability density of giving response $r$ at time $t$, given that no response has occurred before $t$&lt;/li&gt;
  &lt;li&gt;$R_r^{s,c}(t)$ — response process (generally random)&lt;/li&gt;
  &lt;li&gt;$\theta_r^{s,c}$ — threshold&lt;/li&gt;
  &lt;li&gt;$T_r^{s,c}$ — first passage time (the time it would take $R_r^{s,c}$ to cross $\theta_r^{s,c}$ for the first time, independent of all other processes)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;grice-architecture&quot;&gt;Grice architecture&lt;/h2&gt;

&lt;p&gt;In the Grice model, the distribution of thresholds is independent of stimulus and condition. For each stimulus, there are $n$ deterministic response processes, and the thresholds $\theta_1,\ldots{},\theta_n$ are sampled from this fixed joint distribution before each trial. The processes then “race” to their respective thresholds, and whichever arrives first determines the response and response time.&lt;/p&gt;

&lt;h2 id=&quot;linear-ballistic-accumulation&quot;&gt;Linear ballistic accumulation&lt;/h2&gt;

&lt;p&gt;The LBA is also a race model. Each response process is a deterministic linear function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_r^{s,c}(t)=z_r^c+k_r^st&lt;/script&gt;

&lt;p&gt;where $z_r$ is the starting point of the process and $k_r^s$ is the process growth rate, which are sampled independently for each process on each trial. Thresholds $\theta_r^{s,c}=b^c$ are deterministic and shared across processes.&lt;/p&gt;

&lt;p&gt;Normally, it is assumed that starting points are sampled from a uniform distribution, and growth rates are sampled from a normal distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z_r^c&amp;\sim U(0, A^c)\\
k_r^s&amp;\sim \mathcal{N}(v_r^s, \eta^2)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;There is also a constant &lt;em&gt;nondecision time&lt;/em&gt; $t_0$ added to the first-passage time of the winning process.&lt;/p&gt;

&lt;p&gt;To summarize, the parameters are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$b^c$ — threshold for all response processes&lt;/li&gt;
  &lt;li&gt;$A^c$ — upper bound on starting points&lt;/li&gt;
  &lt;li&gt;$v_r^s$ — mean growth rate of response process $r$&lt;/li&gt;
  &lt;li&gt;$\eta$ — standard deviation of growth rate for all response proceses&lt;/li&gt;
  &lt;li&gt;$t_0$ — nondecision time&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the distributional assumptions on $z$ and $k$ are lifted, then the model is unfalsifiable.&lt;/p&gt;

&lt;h2 id=&quot;diffusion&quot;&gt;Diffusion&lt;/h2&gt;

&lt;p&gt;The response process is a stochastic process with a random starting point $z^c$, a random growth/drift rate $k^s$, a (usually fixed) diffusion rate $\sigma^2$, and a decay term with parameter $\beta$ (which is 0 in the Wiener model):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{d}R^{s,c}(t)=(k^s-\beta(R^{s,c}(t)-z^c))\mathrm{d}t + \sigma\mathrm{d}B(t)&lt;/script&gt;

&lt;p&gt;where $B(t)$ is Brownian motion and $R^{s,c}(0)=z^c$. The process terminates when it reaches 0 or a threshold $a^c$. The starting point is sampled from a uniform distribution, the growth rate from a normal distribution, and the nondecision time from a uniform distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z^c&amp;\sim \mathrm{Uniform}(\bar{z}^c-\frac{\delta_z}{2},\bar{z}^c+\frac{\delta_z}{2})\\
k^s&amp;\sim \mathcal{N}(v^s, \eta^2)\\
t_0&amp;\sim \mathrm{Uniform}(T_\mathrm{er}-\frac{\delta_t}{2},T_\mathrm{er}+\frac{\delta_t}{2})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\delta_z$ and is set such that $0\leq z^c\leq a^c$ and $\delta_t\leq 2T_\mathrm{er}$.&lt;/p&gt;

&lt;p&gt;To summarize, the parameters are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\beta$ — decay rate (set to 0 for the Wiener model)&lt;/li&gt;
  &lt;li&gt;$a^c$ — threshold separation&lt;/li&gt;
  &lt;li&gt;$\sigma$ — diffusion rate&lt;/li&gt;
  &lt;li&gt;$v^s$ — mean growth rate&lt;/li&gt;
  &lt;li&gt;$\eta$ — standard deviation of the growth-rate distribution&lt;/li&gt;
  &lt;li&gt;$\bar{z}^c$ — mean starting point&lt;/li&gt;
  &lt;li&gt;$\delta_z$ — range of the start-point distribution&lt;/li&gt;
  &lt;li&gt;$T_\mathrm{er}$ — mean nondecision time&lt;/li&gt;
  &lt;li&gt;$\delta_t$ — range of the nondecision distribution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The diffusion model without distributional assumptions on $z$, $k$, and $t_0$ is shown to be unfalsifiable, even when $\sigma=\delta_z=\delta_t=0$ and $\bar{z}^c=a^c/2$ for any $a^c=a$. Allowing these parameters to vary would in theory just make the model more flexible, but it’s already maximally flexible so in reality they don’t have much effect.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This article isn’t so much a challenge for probabilistic/Bayesian models of cognition, but it is important in the context of my own work in which I would like to be able to use reaction times as a proxy for things like simulation, and number of simulations. As a process model, though, I find the Grice framework a bit dissatisfying because its stochasticity doesn’t seem realistic. The important thing (to me) in these types of models would be the idea that the mind is getting some form of evidence over time—by discrete samples, or a continuous stream of perceptual samples, etc.—and thus the variability shouldn’t be in setting a threshold but in terms of the evidence. The DDM thus feels more interpretable to me, despite the fact that it can be translated into equivalent versions of the other models.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 10:42:58 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Jones2014.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Jones2014.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Optimal predictions in everyday cognition: the wisdom of individuals or crowds?</title>
        <description>&lt;p&gt;&lt;span id=&quot;Mozer2008&quot;&gt;Mozer, M. C., Pashler, H., &amp;amp; Homaei, H. (2008). Optimal predictions in everyday cognition: the wisdom of individuals or crowds? &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;32&lt;/i&gt;(7), 1133–1147. Retrieved from http://onlinelibrary.wiley.com/doi/10.1080/03640210802353016/abstract&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Mozer et al. question whether the conclusion of &lt;a href=&quot;/quals/probabilistic%20models%20of%20cognition/2015/11/08/Griffiths2006.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt;—that people have accurate knowledge of full prior distributions—is justified. In particular, they argue that the same results can be obtained with a model that only uses a few ($k$) samples from the prior. They show that a simple heuristic model (termed Min$k$) actually outperforms the full Bayesian model, and also matches the inter-participant variance better than the Bayesian model.&lt;/p&gt;

&lt;p&gt;Mozer et al. suggest that their Min$k$ model is perhaps more like an algorithmic-level theory, while the fully Bayesian model is a computational-level theory. However, they caution that by focusing purely on the computational level, important factors such as variability and processing time might be missed.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Mozer et al. suggest a few different models. First, the model from Griffiths &amp;amp; Tenenbaum (the &lt;em&gt;G&amp;amp;T-Bayesian&lt;/em&gt; model) predicts the median of the posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(t_{\mathrm{total}}\vert t_\mathrm{cur})\propto p(t_\mathrm{cur}\vert t_\mathrm{total})p(t_\mathrm{total})&lt;/script&gt;

&lt;p&gt;The &lt;em&gt;Min$k$&lt;/em&gt; model retrieves $k$ samples from memory and predicts the minimum sample that is greater than $t_\mathrm{cur}$. If no samples are greater than $t_\mathrm{cur}$, then the model predicts $(1+g)t_\mathrm{cur}$, where $g$ is guess proportion of the query.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;GT$k$Guess&lt;/em&gt; model is similar to Min$k$, except that instead of returning the minimum, it returns the median of the posterior computed with an empirical prior estimated via $k$ samples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(t_\mathrm{total})=\frac{1}{k}\sum_{i=1}^k \delta_{s_i,t_\mathrm{total}}&lt;/script&gt;

&lt;p&gt;If the prior estimate is less than $t_\mathrm{total}$, then the GT$k$Guess model performs the same guess with $g$ as the Min$k$ model.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;GT$k$Smooth&lt;/em&gt; model doesn’t guess, but instead uses something like a kernel density estimate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(t_\mathrm{total})\propto \sum_{i=1}^k \exp\left(\frac{(t_\mathrm{total}-s_i)^2}{2\sigma^2}\right)&lt;/script&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think this is actually a really important point—average participant judgments can’t necessarily be used to say something about the knowledge of individuals. I do think, though, that starting at the computational level and working down can still be revealing. In this case, the computational level analysis reveals that at least in &lt;em&gt;aggregate&lt;/em&gt; people’s judgments reflect true statistical regularities in the world. It could have been the other way around, and a computational level analysis allows us to first look at the high-level picture. Given that, we can then ask questions like the one that Mozer et al. ask here: what is actually the reason why people’s judgments reflect regularities? Is it that each participant has accurate knowledge of the prior, or that knowledge across people is distributed according to the prior? Mozer et al. show that it is the latter.&lt;/p&gt;

&lt;p&gt;I would be interested to see how well an analysis based on mechanisms for sampling would do. For example, would a model that uses rejection sampling also give a consistent algorithmic-level account? I would hypothesize that it would give the same results. You could also measure response time to see if people take longer to respond in cases where they are unlikely to get samples from the prior that are greater than $t_\mathrm{total}$—i.e., they have to make more rejections, and so it takes them longer to respond.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 08:27:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Mozer2008.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Mozer2008.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Judgment under uncertainty: heuristics and biases</title>
        <description>&lt;p&gt;&lt;span id=&quot;Tversky1974&quot;&gt;Tversky, A., &amp;amp; Kahneman, D. (1974). Judgment under Uncertainty: Heuristics and Biases. &lt;i&gt;Science&lt;/i&gt;, &lt;i&gt;185&lt;/i&gt;(4157), 1124–1131. doi:10.1126/science.185.4157.1124&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Tversky &amp;amp; Kahneman give an overview of three commonly used heuristics that lead to systematic biases: representativeness, availability, and anchoring.&lt;/p&gt;

&lt;h2 id=&quot;representativeness&quot;&gt;Representativeness&lt;/h2&gt;

&lt;p&gt;The representativeness heuristic was discussed in &lt;a href=&quot;/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html&quot;&gt;Kahneman &amp;amp; Tversky&lt;/a&gt;, so I won’t go into too much detail about it here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insensitivity to prior probabilities&lt;/li&gt;
  &lt;li&gt;Insensitivity to sample size&lt;/li&gt;
  &lt;li&gt;Misconceptions of chance — e.g. HTHTTH is more representative than HHHTTT&lt;/li&gt;
  &lt;li&gt;Insensitivity to predictability&lt;/li&gt;
  &lt;li&gt;The illusion of validity — confidence is a function based more on degree of representativeness rather than accuracy&lt;/li&gt;
  &lt;li&gt;Misconceptions of regression&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;availability&quot;&gt;Availability&lt;/h2&gt;

&lt;p&gt;The availability heuristic states that people will estimate things like probabilities by seeing how many items from the distribution come to mind. Thus, things that come to mind more easily will be estimated to have higher probability, even if they don’t actually have higher probability.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Biases due to the retrievability of instances — e.g. things like fame or salience will make things more available, and thus be considered more numerous&lt;/li&gt;
  &lt;li&gt;Biases due to the effectiveness of a search set — e.g. judging whether more words start with ‘r’ or have ‘r’ as the third letter&lt;/li&gt;
  &lt;li&gt;Biases of imaginability — e.g. intuitively computing “10 choose $k$” for $2\leq k\leq 8$. People are better able to imagine disjoint sets for $k=2$ than $k=8$, so they say that “10 choose 2” is higher than “10 choose 8”, even though they are the same&lt;/li&gt;
  &lt;li&gt;Illusory correlations — overestimating the frequency of co-occurring events (e.g. believing that patients who exhibit suspiciousness draw characters with shifty eyes, because “suspiciousness” and “eyes” have a strong association, even if they might not predict anything true about the diagnosis)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;anchoring-and-adjustment&quot;&gt;Anchoring and Adjustment&lt;/h2&gt;

&lt;p&gt;The anchoring heuristic states that people estimate values by starting at one value (the &lt;em&gt;anchor&lt;/em&gt;) and adjusting either up or down. Usually, people fail to fully adjust, and thus their estimates are biased towards the anchor.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insufficient adjustments — e.g. in multiplying $8\times 7\times 6\times 5\times 4\times 3\times 2\times 1$ vs. multiplying $1\times 2\times 3\times 4\times 5\times 6\times 7\times 8$, people will report higher estimates for the first product and lower estimates for the second product&lt;/li&gt;
  &lt;li&gt;Biases in the evaluation of conjunctive and disjunctive events — e.g. overestimation that things with high probability will co-occur, underestimation that at least one thing with low probability will occur. The anchors in this case are the probabilities which are then insufficiently adjusted, leading to overestimates of high probability things and underestimates of low probability things.&lt;/li&gt;
  &lt;li&gt;Anchoring in the assessment of subjective probability distributions — e.g. starting at current (mean) estimate and adjusting upward to get to the 90th percentile&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Each of these heuristics is quite telling about the way that people think. Although they’ve been taken in the past that people are just “irrational”, I think they’re more interesting when thought about in the context of how people are actually processing information. The case is clearer in cases like the availability heuristic, for example: it seems plausible that whatever algorithm is used for retrieving things from memory will bias us when we try to objectively estimate the probabilities of things. Generally, the point of memory seems to be to provide us with information that is going to be &lt;em&gt;relevant&lt;/em&gt;, which means there will be an effect of both frequency and utility—the combination of which would manifest itself as “availability”.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 07:27:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Tversky1974.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Tversky1974.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>On the psychology of prediction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kahneman1973&quot;&gt;Kahneman, D., &amp;amp; Tversky, A. (1973). On the psychology of prediction. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;80&lt;/i&gt;(4), 237–251. doi:10.1037/h0034747&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kahneman &amp;amp; Tversky discuss the &lt;em&gt;representativeness&lt;/em&gt; heuristic, in which people seem to evaluate evidence based on how representative it is of an outcome rather than based on the posterior probability of the outcome given the evidence. The present results from several experiments illustrating this effect, and demonstrating its robustness to changes in wording, reliability of evidence, and prior probability.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In the first experiment, participants were given vignettes such as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tom W. is of high intelligence, although lacking in true creativity. He has a need for order and clarity, and for neat and tidy systems in which every detail finds its appropriate place. His writing is rather dull and mechanical, occasionally enlivened by somewhat corny puns and by flashes of imagination of the sci-fi type. He has a strong drive for competence. He seems to have little feel and little sympathy for other people and does not enjoy interacting with others. Self-centered, he nonetheless has a deep moral sense.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Participants in one group rated the base rates of graduate student disciplines (and did not see the vignette). In another group, participants rated how similar the vignette description was to the average graduate student in each discipline. In the third group, participants rated the likelihood that Tom W. was a graduate student in each discipline. Overwhelmingly, participants in the third group ignored the base rate evidence and gave responses similar to the second group. In a follow-up experiment, they had two groups which were told the reliability of predictions of “students like themselves”. This reliability did have an effect on their likelihood ratings, though it did not bring the ratings any close to the base rate. In contrast, if particpants were given no information at all, they reverted to using the base rate.&lt;/p&gt;

&lt;p&gt;In another experiment, they focused on manipulating the base rate: for example, giving descriptions and asking how likely the description was of an engineer or a lawyer, where the number of engineers and lawyers was manipulated. However, changing this proportion did not seem to have an effect: people still seemed to report what was essentially the likelihood, rather than the posterior. Interestingly, when given &lt;em&gt;worthless&lt;/em&gt; information (as opposed to &lt;em&gt;no&lt;/em&gt; information), people judged the probability at 50%, rather than reverting the the prior. (Side note: if you gave people worthless information and they had three categories to choose from, would they give the probability as 50% still or 33%?)&lt;/p&gt;

&lt;p&gt;Kahneman &amp;amp; Tversky also looked at “prediction of outcome” (e.g. predicting GPA from a description) versus “evaluation of inputs” (e.g. rating impressions from a description) versus “translation” (e.g. translating an input variable on one scale to another scale). According to the representativeness heuristic, predictions and evaluations should be essentially the same; according to statistics, however, the predictions are more unreliable and thus should exhibit regression to the mean. Similarly, the representativeness heuristic predicts that predictions are no more regressive than translations, even though statistics predicts otherwise.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The results presented by Kahneman &amp;amp; Tversky are incredibly compelling: despite knowing what the correct calculation to do is, it &lt;em&gt;still&lt;/em&gt; feels more “natural” to respond in the manner that they equate with representativeness. &lt;a href=&quot;http://web.mit.edu/cocosci/Papers/cogsci01_final.pdf&quot;&gt;Tenenbaum &amp;amp; Griffiths&lt;/a&gt; (2001) cast representativeness in a different light from the notion of “similarity” discussed by Kahneman &amp;amp; Tversky, defining representativeness as being a measure of how good of an example something is. Formally, they define it as the log ratio of the likelihood of the current hypothesis to all other hypotheses:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(d, h_i)=\log\frac{P(d\vert h_i)}{\sum_{h_j\neq h_i} P(d\vert h_j)P(h_j\vert \bar{h}_i)}&lt;/script&gt;

&lt;p&gt;where $\mathcal{H}$ is the set of all hypotheses under consideration and $P(h_j\vert \bar{h}_i)$ is the prior probability of $h_j$ given that $h_i$ is not the true explanation of $d$ (i.e. $P(h_j)/(1-P(h_i))$). I don’t know if this particular model has been applied specifically to the Kahneman &amp;amp; Tversky experiments here, though. I think you probably couldn’t apply them directly because the report similarity ranks rather than real likelihood scores.&lt;/p&gt;

&lt;p&gt;I do find it interesting that people are interpreting the phrasing of the problem to be asking for something along the lines of “representativeness” rather than “posterior probability”. Even if you can quantify what “representativeness” means, it doesn’t explain &lt;em&gt;why&lt;/em&gt; people interpret the question that way (and not the way it is meant to be interpreted).&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 05:39:59 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Theory learning as a stochastic search in a language of thought</title>
        <description>&lt;p&gt;&lt;span id=&quot;Ullman2012&quot;&gt;Ullman, T. D., Goodman, N. D., &amp;amp; Tenenbaum, J. B. (2012). Theory learning as stochastic search in the language of thought. &lt;i&gt;Cognitive Development&lt;/i&gt;, &lt;i&gt;27&lt;/i&gt;(4). doi:10.1016/j.cogdev.2012.07.005&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Ullman et al. suggest an algorithm-level approach for theory learning. In contrast to &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Griffiths2009.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt; and &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Kemp2010.html&quot;&gt;Kemp et al.&lt;/a&gt;, which proposed computational-level models for theory learning, Ullman et al. build on previous work (i.e., they still use a hierarchical Bayesian model) and demonstrate that MCMC-like search procedure can account for similar learning dynamics as are found in children.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Ullman et al. express theories using Horn clauses, which are logical expressions of the form $r\leftarrow (f\wedge g\wedge\ldots{}\wedge s\wedge t)$ where each term is a predicate such as $f(X)$ or $s(X, Y)$, where $X$ and $Y$ are entities in the domain.&lt;/p&gt;

&lt;p&gt;A theory $T$ is produced from a &lt;em&gt;univeresal theory grammar&lt;/em&gt; $U$, which is a probabilistic context-free Horn clause grammar (PHCG). This grammar includes what they call &lt;em&gt;law templates&lt;/em&gt; which encode assumptions for what types of laws are good. For example, one template is $F(X,Y)\leftarrow F(X,Z)\wedge F(Z,Y)$ which captures transitivity. The $F$ could then be replaced with something like $\mathrm{interacts}(\cdot{})$.&lt;/p&gt;

&lt;p&gt;Together, the grammar and the templates specify the prior $p(T\vert U)$ over theories. A particular theory—for example, in the magnetism domain—might have core predicates $f(X)$ and $g(X)$, surface predicates of $interacts(X, Y)$, and laws such as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathrm{interacts}(X, Y)&amp;\leftarrow f(X)\wedge f(Y)\\
\mathrm{interacts}(X, Y)&amp;\leftarrow f(X)\wedge g(Y)\\
\mathrm{interacts}(X, Y)&amp;\leftarrow \mathrm{interacts}(Y, X)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Given a theory, a model $M$ can be produced which fits the data. For example, in the magnetism domain, the model might assign magnets to $f(X)$ and magnetic objects to $g(X)$ (and everything else unassigned). The prior on models $p(M\vert T)$ is a beta prior (per binary fact).&lt;/p&gt;

&lt;p&gt;Finally, the data $D$ are actual observations, for example, “this object interacts with this other object”. The likelihood $p(D\vert M, T)$ is Bernoulli.&lt;/p&gt;

&lt;p&gt;Thus the full model is something like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
T\vert U &amp;\sim \mathrm{PHCG}(U)\\
M^i &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
D^{ij} &amp;\sim \mathrm{Bernoulli}(M^i)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the posterior is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(T\vert D, U)\propto P(T\vert U)\sum_M p(D\vert M, T)p(M\vert T)&lt;/script&gt;

&lt;p&gt;To approximate the sum, they use Gibbs sampling. To search in the space of theories, they use a modified version of MCMC where the acceptance ratio is annealed, so that changes are less likely to be made as time goes on. They propose new theories at each step of the chain with the following possible modifications:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add or delete a predicate from a law&lt;/li&gt;
  &lt;li&gt;Change one predicate to an alternative of the same or different form within a law&lt;/li&gt;
  &lt;li&gt;Add or delete a whole law&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This approach of performing a stochastic search in theory space seems generally plausible as a mechanism for how children learn theories. However, I think there are a few things here that aren’t fully satisfing:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The MCMC chain takes many iterations to run: for example, 1600 in the magnetism case. I realize you can’t exactly map inference (“thinking”) time directly to MCMC steps, but it seems like a lot. If this is a mechanistic account, then it’s essentially saying that children propose 1600 different theories before they reach the final one.&lt;/li&gt;
  &lt;li&gt;Children presumably don’t do inference over the entire dataset at once; I would expect there to be some sort of online learning going on. Ullman et al. do investigate the effect of varying amounts of data on the learning process, but they use separate datasets rather than looking at an online method.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Related to the last point, I suppose there are two options for what people do: either they use some memoryless inference procedure, or they do inference over a set of data, but I would expect that to not include &lt;em&gt;everything&lt;/em&gt; they’ve ever encountered. There will be some effect of memory (perhaps remembering data points that were surprising, and more recent data points).&lt;/p&gt;

&lt;p&gt;I wonder if, rather than using a random MCMC algorithm, an active search or Bayesian optimization type of algorithm might work better here in terms of sampling efficiency.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 03:10:05 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/11/Ullman2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/11/Ullman2012.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>A probabilistic model of theory formation</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kemp2010&quot;&gt;Kemp, C., Tenenbaum, J. B., Niyogi, S., &amp;amp; Griffiths, T. L. (2010). A probabilistic model of theory formation. &lt;i&gt;Cognition&lt;/i&gt;, &lt;i&gt;114&lt;/i&gt;(2), 165–196. doi:10.1016/j.cognition.2009.09.003&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kemp et al. describe a method for clustering entities based on  relations, rather than features. They argue that this clustering algorithm can be thought of as a model of theory formation in people, reflecting how people learn and organize concepts. In particular, they focus on the idea of &lt;em&gt;framework theories&lt;/em&gt;, which “specify the fundamental concepts that exist in a domain and the possible relationships between these concepts” (pg. 166). They ask three fundamental questions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;What are theories?&lt;/strong&gt; Framework theories are “represented as a probabilistic model which includes a set of categories and a matrix of parameters specifying relationships between those categories” (pg. 166)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How are theories used to make inductive inferences?&lt;/strong&gt; “Each of our theories specifies the relationships between categories that are possible or likely, and predictions about unobserved relationships between entities are guided by inductive inferences about their category assignments” (pg. 166)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How are theories acquired?&lt;/strong&gt; Kemp et al. consider the case of acquiring both the concepts and causal laws of a theory simultaneously. “Given a formal characterization of a theory, we can set up a space of possible theories and define a prior distribution over this space” (pg. 167)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They apply their model (described below) to several different real-world datasets: clustering animals and features, clustering medical terms and predicates, and recovering kinship categories. They also run two experiments in which they show how their model matches the way that people learn causal theories of physical relationships.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, Kemp et al. had people play with a set of blocks. Each block either was a member of category $A$ or category $B$, and depending on the condition, the relationship between $A$ and $B$ was either: that $A$ blocks caused $B$ blocks to light up; that $A$ blocks caused $B$ blocks to light up and vice versa; that $A$ blocks caused $B$ blocks to light up with $p=0.5$; and that $A$ blocks caused $B$ blocks to light up with $p=0.5$ and vice versa. Note that &lt;em&gt;specific&lt;/em&gt; blocks were deterministic, $p$ was used to determine which pairs of blocks would affect each other. Participants got experience playing with the blocks and were periodically asked to predict what would happen if a new block was touched to an old block. They then saw the new block touch a different old block, and were asked the same question.&lt;/p&gt;

&lt;p&gt;Experiment 2 was similar to Experiment 1, except that only the relationship where $A$ blocks caused $B$ blocks to light up and vice versa was used. After getting experience with the blocks, participants were shown three new blocks, $x$, $y$, and $z$, where either $x$ and $z$ were in the same category and $y$ was in a different category, or where $y$ and $z$ were in the same category and $x$ was in a different category. By seeing that $x$ interacts with $y$ and that $z$ interacts with $y$, participants should (and do) infer that $x$ and $z$ are in the same category—but they shouldn’t know which one. After seeing $z$ interact (or not interact) with an old block, participants should (and do) infer which categories $x$ and $y$ are in.&lt;/p&gt;

&lt;p&gt;The model matches people’s behavior in both of these experiments. In contrast, particularly in Experiment 2, a purely feature-based categorization model (which doesn’t take into account relations) does not appropriately generalize.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Let the observed data include $m$ relations ($R$) over $n$ types. Then, a relational system is characterized by a pair $(z, \eta)$, where $z$ is a partition of entities into categories and $\eta$ is a matrix of parameters specifying how the categories interact with each other for a relation $R$ (a.k.a. a &lt;em&gt;category graph&lt;/em&gt; where the edge from $A$ to $B$ has weight $\eta(A,B)$). Then, the theory that best characterizes the data is given by the MAP estimate of:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z,\eta\vert R)\propto p(R\vert \eta,z)p(\eta\vert z)p(z)&lt;/script&gt;

&lt;p&gt;where $p(R\vert \eta,z)$ is the probability of observing the relations given categories and parameters, $p(\eta\vert z)$ is the probability of the parameters given the partition, and $p(z)$ is the prior over category assignments. More formally:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z\vert\gamma &amp;\sim \mathrm{CRP}(\gamma)\\
\eta(A, B)\vert\alpha,\beta &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
R(i, j)\vert z, \eta &amp;\sim \mathrm{Bernoulli}(\eta(z_i, z_j))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In this case, there is one relation and one type (though multiple entities within that type); however, in the general case, there are multiple relations with $m$ dimensions and $n$ types. If $T^i$ is the $i$th type, then let $z^i$ be the partition into category assignments for $T^i$. If $R^j$ is the $j$th relation, then let $\eta^j$ be a separate parameter matrix for each $R^j:T^{d_1}\times T^{d_2}\times \ldots{}\times T^{d_m}$, where $d_k$ is the type corresponding to the $k$th dimension. The full general specification is then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z^i\vert\gamma &amp;\sim \mathrm{CRP}(\gamma)\\
\eta^j(z_{x_1}^{d_1},z_{x_2}^{d_2}\ldots{},z_{x_m}^{d_m})\vert\alpha,\beta &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
R^j(x_1,x_2,\ldots{},x_m)\vert z^1,\ldots{},z^n, \eta^j &amp;\sim \mathrm{Bernoulli}(\eta^j(z_{x_1}^{d_1},z_{x_2}^{d_2}\ldots{},z_{x_m}^{d_m}))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;To fit the model, they first integrate out $\eta$ and find the best category assignments $z$ using a hill-climbing procedure or MCMC. Given $z$, the MAP parameter assignments $\eta$ can be computed analytically. Similarly, unobserved relations between specific entities can be predicted given $z$.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This framework seems to be a generalization of the one described by &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Griffiths2009.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt; — for example, Kemp et al.’s experiments here are very similar to the “blicket detector” paradigm. However, Griffiths &amp;amp; Tenenbaum only consider there to be blickets and non-blickets. In this case, the number of types is left unspecified. Actually, it seems not so much that this is a generalization of Griffiths &amp;amp; Tenenbaum, but that it’s a more abstract sort of learning. This clusters entities and relations into an ontology that could be used by the framework described by Griffiths &amp;amp; Tenenbaum, but it’s not necessarily a &lt;em&gt;causal&lt;/em&gt; ontology.&lt;/p&gt;

&lt;p&gt;I am less clear on exactly how this type of framework for theories relates to that discussed in the other paper by &lt;a href=&quot;/quals/theory%20learning/2016/01/09/Kemp2007.html&quot;&gt;Kemp et al.&lt;/a&gt;. In that paper, they discuss how to parse objects into different ontological kinds, which is essentially a type of feature-based clustering. I suppose that model is something that is meant to be applied a more domain-specific level—e.g. perhaps within a single category of the kind that is discovered by the present paper. It would be nice to see a tighter coupling between these different types of ontologies and ways of learning theories.&lt;/p&gt;

</description>
        <pubDate>Sun, 10 Jan 2016 11:32:56 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Kemp2010.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Kemp2010.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
  </channel>
</rss>
