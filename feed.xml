<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 06 Jan 2016 14:32:31 -0800</pubDate>
    <lastBuildDate>Wed, 06 Jan 2016 14:32:31 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>The experience of force: the role of haptic experience of forces in visual perception of object motion and interactions, mental simulation, and motion-related judgments</title>
        <description>&lt;p&gt;&lt;span id=&quot;White2012a&quot;&gt;White, P. A. (2012). The experience of force: The role of haptic experience of forces in visual perception of object motion and interactions, mental simulation, and motion-related judgments. &lt;i&gt;Psychological Bulletin&lt;/i&gt;, &lt;i&gt;138&lt;/i&gt;(4), 589–615. doi:10.1037/a0025587&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, White proposes a theory of action and perception that is based on the notion of force. Specifically, he argues that during our interactions with the world, we perceive force from our haptic system (along with other sensory modalities), and these perceptions get stored in memory along with the relevant actions associated with them. Then, when we perceive new situations, we activate these stored representations which allows us to make predictions and judgments about motion and other factors.&lt;/p&gt;

&lt;p&gt;First, White discusses evidence for forward models of action in the motor system, as well as evidence for the role of mechanoreceptor feedback. What is sounds like he proposes is a sort of forward model like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[\mathbf{x}_{t+1}, \mathbf{s}_{t+1}] = f(\mathbf{x}_t,\mathbf{s}_t,\mathbf{u}_t)&lt;/script&gt;

&lt;p&gt;where $\mathbf{x}$ is the state of the system, $\mathbf{s}$ is the sensory information (e.g. from the haptic system), and $\mathbf{u}$ are the controls (forces) of the system. A prediction error for the sensory information (e.g. mechanoreceptor feedback) is also computed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{\delta}_t=\mathbf{s}_t - \hat{\mathbf{s}}_t&lt;/script&gt;

&lt;p&gt;where $\mathbf{s}_t$ is the predicted sensory information and $\hat{\mathbf{s}}_t$ is the true sensory information. The feedback $\mathbf{\delta}_t$ is thus the error signal, which is going to be zero when our predictions of force are accurate. White also argues that perception of additional object properties (texture, rigidity, mass, etc.) are computed based on sensory information from mechanoreceptors. I’ll denote these properties as $\mathbf{\pi}$.&lt;/p&gt;

&lt;p&gt;All of these different sources of information are stored in long-term memory, roughly (it seems) in the form of tuples such as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{m}_t=[\mathbf{x}_t,\mathbf{\delta}_t,\mathbf{\pi}]&lt;/script&gt;

&lt;p&gt;White describes these as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A stored representation of an action on an object is a multimodal episodic trace combining haptic information such as the disposition and movement of the limbs during execution of the action, visual information about body movement and the associated motion of the object acted on, auditory information such as sounds elicited by contact between extremity and object, and in principle, information in any sensory modality. Internally available information such as the content of the forward model also forms part of the representation. (pg. 607)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Importantly, we store our &lt;em&gt;prediction error&lt;/em&gt; of sensory information, rather than the absolute sensory information itself.
These stored representations are activated by matching to similar perceptual stimuli (e.g. visual stimuli).&lt;/p&gt;

&lt;p&gt;White uses this formulation of stored representations to offer a unifying account for several lines of research:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The storing of sensory feedback, rather than direct sensory information, predicts that when making judgments about force in terms of one moving object acting on a stationary, we assign notions of force &lt;em&gt;from&lt;/em&gt; the moving object (because we do store $\mathbf{u}_t$) but not &lt;em&gt;to&lt;/em&gt; the moving object (because, for static objects, the sensory prediction error should be zero). If both objects are moving, however, we should assign a notion of force that the second object is applying to the first object because the sensory prediction error is nonzero. This explains, for example, Michottean launching effects.&lt;/li&gt;
  &lt;li&gt;To the extent that visual perception of motion matches stored representations corresponding to actions, we should perceive that motion as being internally caused. This extends to biological plausibility as well. Importantly, biologically generated motion has different velocity profiles than, for example, two nonbiological objects colliding—thus visual motion that matches the biological motion velocity profile should be interpreted as more biological.&lt;/li&gt;
  &lt;li&gt;Representational momentum&lt;/li&gt;
  &lt;li&gt;Perception of inanimate entities as intentional&lt;/li&gt;
  &lt;li&gt;Mental simulation&lt;/li&gt;
  &lt;li&gt;Perception of mass&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a surprisingly consistent and satisfying account of how perception arises from the combination of visual and haptic feedback. Assuming people do store information as something like $\mathbf{m}_t$ defined above, and they have access to forward an inverse models, it should be possible to reconstruct $\mathbf{u}_t$ (from both $\mathbf{x}_t$ and $\mathbf{x}_{t+1}$), which is consistent with White’s assertions. I am skeptical, though, that all we are doing is “storing” and “matching” representations. It is not at all clear to me how it would work to match the motion of a 2D ball (e.g. in the Michotte experiments) to the stored motion of ourselves. Additionally, it sounds like White is advocating for something like an exemplar model, but I find it much more likely that we use our experiences to build structured forward or inverse models. There may be multiple forward models (as suggested by Kawato) that are perhaps combined in certain ways, but give that there is evidence for some generalization (also described by Kawato), I would be very surprised if all that was going on was just storing and matching exemplars.&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 04:47:29 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/06/White2012a.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/06/White2012a.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Prediction precedes control in motor learning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Flanagan2003&quot;&gt;Flanagan, R. R., Vetter, P., Johansson, R. S., &amp;amp; Wolpert, D. M. (2003). Prediction precedes control in motor learning. &lt;i&gt;Current Biology&lt;/i&gt;, &lt;i&gt;13&lt;/i&gt;(2), 146–150. doi:10.1016/S0960-9822(03)00007-1&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Flanagan et al. describe an experiment in which participants must grip an object with their index finger and thumb, and move it in a straight line to a target. The dynamics of the object were modified so that when they moved it in the horizontal plane, a proportional vertical force was applied to the object. Thus, to learn to move it in a straight line, participants had to adapt to the vertical force.&lt;/p&gt;

&lt;p&gt;Flanagan et al. found that participants took a long time (about 70 trials) before they were able to fully adjust their trajectories to be straight. However, they took much less time (about 10 trials) to adjust the force with which they gripped the object to match that of the corresponding load force. These results suggest that there are two internal models (one for the grip force, and one for the arm trajectory) that are being learned at separate rates. Specifically, Flanagan et al. suggest that in the first case, it is a forward kinematic model that is being learned, while in the second case, it is a inverse dynamics model that is being learned. This is consistent with the demands of the task: the novel dynamics of the object require learning a new mapping from desired trajectory to motor commands (the inverse model), but they do not require learning a new mapping for controlling the load force. Rather, the motor system needs only to predict the load force so that it can appropriately adjust for it.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This paper basically answers the question I ended with in &lt;a href=&quot;/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html&quot;&gt;Kawato’s review&lt;/a&gt;: learning operates independently in the forward and inverse models. Flanagan et al. suggest that, computationally, this may be able to be explained by something like &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/20/Nguyen-Tuong2011.html&quot;&gt;distal teacher learning&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 13:30:48 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Flanagan2003.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Flanagan2003.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Internal models for motor control and trajectory planning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kawato1999&quot;&gt;Kawato, M. (1999). Internal models for motor control and trajectory planning. &lt;i&gt;Current Opinions In Neurobiology&lt;/i&gt;, &lt;i&gt;9&lt;/i&gt;(6), 718–727. doi:10.1016/S0959-4388(99)00028-8&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this review article, Kawato discusses the role of internal models in motor control. He argues that both forward and inverse internal models are used in motor control. In particular:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Fast and coordinated arm movements cannot be executed solely under feedback control, since biological feedback loops are slow and have small gains. Thus, the internal model hypothesis proposes that the brain needs to acquire an inverse dynamics model of the object to be controlled through motor learning, after which motor control can be executed in a pure feedforward manner.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, Kawato gives an overview of the existence of internal models. One piece of evidence comes from perturbing the dynamics of participants’ motions. Initially, people make the wrong movements under these novel dynamics. However, they eventually adapt and can make the correct motion. If the new dynamics are removed, then they again make errors because they are now using an incorrect model of the inverse dynamics. Another piece of evidence comes from &lt;em&gt;grip-force—load-force coupling&lt;/em&gt;, which is the coupling of a grip force (e.g. thumb and index finger) and load force (e.g. weight of the object that is being held). When gripping an object like this, and moving one’s arm to a new location, the motor system must both determing the appropriate trajectory for the arm as well as the grip force needed to hold the object. The way this works is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The inverse model of the combined dynamics of the arm, hand, and object calculates the necessary motor commands from the desired trajectory of the arm. These commands are sent to the arm muscles as well as to the forward dynamics model as the efference copy. Then, the forward model can predict an arm trajectory that is slightly in the future. Given the predicted arm trajectory, the load force is calculated; then, by multiplying a friction coefficient and a safety factor, the necessary minimum level of grip force can be calculated.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next, Kawato discusses neurological evidence for internal models in the cerebellum. I’m not going to go into detail on this.&lt;/p&gt;

&lt;p&gt;Next, Kawato discusses the structure of internal models. Specifically, is it that internal models are just implemented as mappings between states and actions, or are they implemented using some sort of generalizable parameterization? To test this, the “generalization” paradigm is used, in which participants are trained to do a specific task under novel dynamics. They are then instructed to do another task, still under the novel dynamics, to see whether the dynamics have been fully generalized to new situations or not. The results are somewhere in the middle: some generalization occurs, but not perfect generalization.&lt;/p&gt;

&lt;p&gt;Finally, Kawato discusses how the motor system computes trajectories. There are apparently two competing models: kinematic models (e.g. the minimum jerk model) and dynamics models (e.g. the minimum torque-change mdoel). Kawato proposes the &lt;em&gt;minimum variance&lt;/em&gt; model as middle ground between these two models. In the minimum variance model, the objective function minimizes a kinematic value (the variance of the end pose); however, the variance is determined by motor commands, which are part of the dynamics. If this model is correct, then it gives another motivation for there being both be a kinematic internal model (i.e. a forward model) as well as a dynamics internal model (i.e. a inverse dynamics model)&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Based on this article, there is very strong evidence for internal models in the motor system. Moreover, there is evidence for &lt;em&gt;both&lt;/em&gt; forward kinematic models and inverse dynamics models which seem to work in tandem.&lt;/p&gt;

&lt;p&gt;I wonder how these models are learned. In particular, when people’s motions are perturbed in some of the studies that Kawato describes, is it that people are updating their forward models, or their dynamics models, or both? Does learning in one affect the other, or are they independent?&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 11:48:31 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Kawato1999.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Temporal and kinematic properties of motor behavior reflected in mentally simulated action</title>
        <description>&lt;p&gt;&lt;span id=&quot;Parsons1994&quot;&gt;Parsons, L. M. (1994). Temporal and kinematic properties of motor behavior reflected in mentally simulated action. &lt;i&gt;Journal Of Experimental Psychology: Human Perception and Performance&lt;/i&gt;, &lt;i&gt;20&lt;/i&gt;(4), 709–730. doi:10.1037/0096-1523.20.4.709&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Parsons asks the question: how similar are mental simulations of motor actions to actual motor actions themselves? In a series of five experiments, he demonstrates that they correspond quite closely—though they differ in a few revealing ways—supporting the hyothesis that mental simulations do operate on a detailed model of the body and utilize the same type of trajectory optimization/motion planning that the motor system does.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In all experiments, Parsons showed participants images of left and right hands from six cardinal perspectives and 12 orientations ($30^\circ$, $60^\circ$, $90^\circ$, $120^\circ$, and $150^\circ$; clockwise and counterclockwise).&lt;/p&gt;

&lt;p&gt;In Experiment 1, participants first performed a task in which they had to move their hand into the specified target position. They then performed another task in which they had to determine whether the specified target position depicted a right hand or a left hand (which was hypothesized to elicit mental simulation). Results showed that response times for the movement task and the left-right judgments were nearly identical, and that they increased as the distance between the original hand position and the target posture increased (in trajectory space). The cases where the RTs were less similar between the two tasks was for awkward and uncommon hand positions, in which case the response times for the left-right judgments tended to be longer (perhaps because people have a less good kinematic model of their hands in those positions).&lt;/p&gt;

&lt;p&gt;Experiment 2 was the same as Experiment 1, except that instead of making left-right judgments, participants were told to imagine moving their hand to the target position, thus explicitly engaging them in mental simulation. The results were pretty much the same as in Experiment 1, suggesting that participants in the left-right judgments were indeed using mental simulation.&lt;/p&gt;

&lt;p&gt;Experiment 3 was a control condition in which Parsons controlled for the time to perceive the target. Under this control, participants had RTs that were about 300ms faster on average, and which were especially faster for positions which were more awkward, suggesting that the awkward positions also take more time to perceive.&lt;/p&gt;

&lt;p&gt;Experiments 4 and 5 had participants make left-right judgments (Experiment 4) or move to the specified target position (Experiment 5) but additionally had participants begin with their hands in different initial positions. The idea was to test whether mental simulation began from a canonical body pose, or from the current body pose. Parsons found that the original position did influence RTs in a way that was similar both for the left-right judgments and for movements.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is such a cool set of experiments. What’s really remarkable about this is that participants can look at a schematized drawing of a hand, map it to their own hand (without looking at their own hand), and make the desired movement. Given the results that people take the same amount of time to make the right-left judgments, or just to explicitly perform the mental simulation, this is fairly strong evidence that people have a detailed mental model of their own body that they can use for motion planning and trajectory optimization.&lt;/p&gt;

&lt;p&gt;One might say that it is not necessarily the case that this mental model need be &lt;em&gt;kinematic&lt;/em&gt;—it could just be that people are determining the pose that they need to reach in configuration space, that they know their current pose in configuration space, and that then they compute some trajectory optimization and execute the resulting plan. But, it wouldn’t really make sense why people would need to do this for left-right judgments: if all it were was that people were trying to reach some point in configuration space, then they would presumably have the information already as to which hand it was. I suppose, though, that even if they have a kinematic model they wouldn’t necessarily &lt;em&gt;need&lt;/em&gt; to do a mental simulation in the left-right task if they’ve already mapped the stimulus onto their body. Or, perhaps, mapping the stimulus onto the body &lt;em&gt;is&lt;/em&gt; the mental simulation? It might be revealing to, instead of controlling for the perception time (as in Experiment 3), have people look at the stimulus and then indicate when they are ready to make the motion. If the perception itself is what is making use of the mental simulation, then that RT should be the same as the RT in the left-right judgments. But that would still just make things more puzzling: how is it that the mental simulation seems to match the actual movement if the point of the mental simulation is to determine how to make the movement?&lt;/p&gt;

&lt;p&gt;To be more precise, in robotics, it seems that the process is something like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Determine initial and goal states in configuration space.&lt;/li&gt;
  &lt;li&gt;Perform trajectory optimization to find a path from the start to the goal.&lt;/li&gt;
  &lt;li&gt;Execute the found trajectory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, one could say that a method of solving the right-left judgment would just be the first step of the above process: if you know the goal state, there is no need to actually compute and execute a trajectory. Perhaps the reason why we see something that looks like computation and execution of the trajectory is that we don’t deterministically compute the goal state. Instead, perhaps we form hypotheses about what the goal state is, and then attempt to compute multiple trajectories under these different hypotheses until we find one. If computing the trajectory is linear in the distance of the trajectory, then this should reflect the observed behavior.&lt;/p&gt;

&lt;p&gt;A further question I thought of relates our mental simulation of action to the mental simulation of the motion of objects. Trajectory optimization in robotics tends to usually be in terms of control (i.e., torques); if the same is true in humans, and our mental simulations reflect this, then does that extend to our mental simulation of other objects? That is, do we mentally simulate the &lt;em&gt;control&lt;/em&gt; of those objects (e.g. in mental simulation do we imagine that they are being rotated &lt;em&gt;by&lt;/em&gt; some entity) or just the kinematic motion of those objects? Presumably, it is at least sometimes the control (e.g. if you imagine grasping a coffee cup and taking a sip of coffee), but presumably at other times it is not (e.g. if you imagine a leaf falling from a tree). In the mental rotation case, my intuition is that it really could be either one. Understanding this distinction is going to be an important question, I think.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 08:32:04 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Parsons1994.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/motor%20control%20and%20action/2016/01/05/Parsons1994.html</guid>
        
        
        <category>Motor control and action</category>
        
      </item>
    
      <item>
        <title>Kinematic mental simulations in abduction and deduction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Khemlani2013&quot;&gt;Khemlani, S. S., Mackiewicz, R., Bucciarelli, M., &amp;amp; Johnson-Laird, P. N. (2013). Kinematic mental simulations in abduction and deduction. &lt;i&gt;Proceedings Of the National Academy of Sciences of the United States of America&lt;/i&gt;, &lt;i&gt;110&lt;/i&gt;(42), 16766–71. doi:10.1073/pnas.1316275110&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Khemlani et al. conduct a series of experiments in which they have people solve programming-like problems, come up with algorithmic solutions to those problems, and execute existing algorithms. Their experiments operate in a domain that involves train tracks with a set of cars where the problems are to rearrange the cars by sliding them to different sections of the tracks. They also propose a model of how people solve these types of tasks, based on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Johnson-Laird2012.html&quot;&gt;Johnson-Laird’s model theory&lt;/a&gt; but with the extension to &lt;em&gt;kinematic&lt;/em&gt; mental models. Importantly, there are three assumptions that they make about the way in which people use mental models:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mental models are iconic (i.e., they have the same form as the thing they represent)&lt;/li&gt;
  &lt;li&gt;Kinematic mental models are based in time (i.e., the sequence of operations that they simulate are thus ordered in time)&lt;/li&gt;
  &lt;li&gt;Mental models can be schematic (i.e., not necessarily a visual mental image)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The kinematic mental models theory makes several predictions about people’s performance on programming-like tasks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Solutions that involve more steps, and steps that operate on more objects (operands), will take longer and be more prone to error. Thus a solution that takes 5 steps each operating on a single car should be faster than one that takes 7, but also faster than one that takes 5 steps each operating on two cars.&lt;/li&gt;
  &lt;li&gt;People should find it easier to generate algorithms that use while loops than algorithms that use for loops, because “naive individuals use simulations to abduce algorithms”. (It’s not really clear to me how this follows?)&lt;/li&gt;
  &lt;li&gt;People should find it more difficult to abduce algorithms that have higher Kolmogorov complexity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Experiment 1 (“problem solving”) had participants solve rearrangement problems. They were able to actually move the cars on the track using a computer interface. Khemlani et al. found that participants made more moves when their model made more moves, and that participants also made more moves as the number of operands increased. They found similar results with response times.&lt;/p&gt;

&lt;p&gt;Experiment 2 (“abduction”) had participants come up with an algorithm to solve each problem (i.e. a description of how to solve the problem, in English). They for each type of problem, they had to solve it for 8 cars, and then for an unspecified number of cars. Participants were close to ceiling in coming up with algorithms for the 8 car problems, but varied on the problems that had an unspecified number of cars (where, as predicted, solutions with a higher Kolgomorov complexity had a lower success rate). Participants also used while loops more frequently.&lt;/p&gt;

&lt;p&gt;Experiment 3 (“deduction”) gave participants a description of the procedure, the intial state of the trains, and asked them to determine what the end result would be after executing the procedure. They attempted to control for amount of information in the descriptions, by making sure that each description was the same length (number of words). They again found that people’s success was correlated with the algorithm’s Kolmogorov complexity.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Creation of an algorithm (abduction) involves three steps: first computing solutions to two specific problems, then recovering the loop that must be performed, and then converting the structure of the solution into a verbal description. The specific solutions are solved using a “partial means-ends analysis”, in which the problem is broken down into subgoals, where the first goal is to get the rightmost car on the right track, and so on. The way their model recovers the loop is to first find repeated sequences of at least two moves in both of the solutions to the specific problems, and then either:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the case of a for loop, solve a system of linear equations based on the solutions to the two specific problems.&lt;/li&gt;
  &lt;li&gt;In the case of a while loop, determine the condition that needs to be satisified for the while loop to halt.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To compute Kolmogorov complexity of an algorithm, they simply take the number of characters in the LISP program and multiplied by the number of bits for each character.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a really cool exploration of how people reason about more structured plans such as algorithms. It would be really interesting to see if you coded people’s algorithms in Experiment 2 into actual code, how well they corresponded to the algorithms produced by mAbducer, rather than just comparing the success rate. That is, are people actually coming up with solutions like those predicted by the model theory?&lt;/p&gt;

&lt;p&gt;I also wonder how well an approach based on some type of grammar would work (e.g. something like Kevin Ellis’ paper from NIPS 2015, “Unsupervised Learning by Program Synthesis”). I also wonder how their mAbducer program compares to typical approaches to program induction in general. Are there key differences that the model theory predicts that general program induction algorithms would not predict?&lt;/p&gt;

&lt;p&gt;Khemlani et al. use a very specific notion of “simulation” in this paper, which is essentially the simulation of a computer program: the sequential application of known rules beginning with a given initial state. There are other types of computations that they assume, too (e.g. that are used to solve the problems in the first place), but once specific instances of the programs have been solved, simulations of those programs are used to abduce a general solution to the problem. Simulations of the general solution are then used to perform deduction on new problems.&lt;/p&gt;

&lt;p&gt;One potential issue with the third experiment is that participants didn’t actually come up with the solutions themselves. I wonder if there is something important about the way that people form their mental models: it might be that you can’t just give them a description of the program, but that they actually need to abduce it themselves. If this were the case, I wonder if the accuracies at solving the deduction problems would be higher.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2016 06:06:28 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/05/Khemlani2013.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/05/Khemlani2013.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Inference with mental models</title>
        <description>&lt;p&gt;&lt;span id=&quot;Johnson-Laird2012&quot;&gt;Johnson-Laird, P. N. (2012). Inference with Mental Models. In &lt;i&gt;The Oxford Handbook of Thinking and Reasoning&lt;/i&gt; (pp. 134–145). doi:10.1093/oxfordhb/9780199734689.001.0001&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this chapter, Johnson-Laird describes the theory of mental models:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Perception yields models of the world that lies outside us. An understanding of discourse yields models of the world that the speaker describes for us. And thinking, which enables us to anticipate the world and to choose a course of action, relies on internal manipulations of these mental models. The present chapter is about this theory, which it refers to as the &lt;em&gt;model&lt;/em&gt; theory, and about its experimental corroborations. (pg. 134)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Johnson-Laird first defines what a mental model is. In particular, it follows three principles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mental models are iconic; that is, they have the same structure of the thing the model represents.&lt;/li&gt;
  &lt;li&gt;Each mental model is the simplest way of describing a possibility (and thus implicitly incorporates other irrelevant possibilities).&lt;/li&gt;
  &lt;li&gt;Mental models only represent what is possible, and not what is impossible.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;deduction&quot;&gt;Deduction&lt;/h2&gt;

&lt;p&gt;Next, Johnson-Laird describes how model theory applies to deductive reasoning, and gives empirical evidence for several predictions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fewer and simpler models require less time to process.&lt;/li&gt;
  &lt;li&gt;People will overlook certain models of premises (e.g. because mental models encode implicit information) and thus produce certain errors.&lt;/li&gt;
  &lt;li&gt;People can detect invalid inferences by coming up with counterexamples. Johnson-Laird emphasizes that there are two types of invalid inferences: first, that the conclusion is inconsistent with the premises (people seem to just be able to detect this inconsistency), and second, the conclusion is consistent with the premises but does not follow from them (people come up with a counterexample).&lt;/li&gt;
  &lt;li&gt;People confidently make “illusory” inferences which are invalid. In particular, “when they think about the truth of one assertion, they fail to think about the consequences of the falsity of other assertions”.&lt;/li&gt;
  &lt;li&gt;People who have never performed these sorts of deductions will develop their own strategies for doing so automatically.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;probabilistic-reasoning&quot;&gt;Probabilistic reasoning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;intentional reasoning&lt;/strong&gt;: people use heuristics to infer the probability of an event from evidence&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;extensional reasoning&lt;/strong&gt;: people infer the probability of an event based on different ways the event might occur&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;principle of equiprobability&lt;/strong&gt;: mental models are uniformly equally probable (unless there is reason to believe otherwise)&lt;/p&gt;

&lt;h2 id=&quot;induction&quot;&gt;Induction&lt;/h2&gt;

&lt;p&gt;Johnson-Laird argues that there are two systems of inductive reasoning: one which is &lt;em&gt;intuitive&lt;/em&gt; (system 1) and which has no access to working memory (and thus can only make fast, but potentially erroneous inferences), and one which is &lt;em&gt;explicit&lt;/em&gt; (system 2) and which is “slow, voluntary, and conscious”. For intuitive inductions, they can be affected by &lt;em&gt;modulation&lt;/em&gt;, which is the idea that “the meanings of clauses, coreferential links between them, general knowledge, and knowledge of context, can modulate the core meanings of sentential connectives” (pg. 146). This can affect things like property relations (e.g. “if the dish is kidney beans, then its basis is beans” vs. “if the dish is made of meat, then it can be Portugese stew”, in which case not-A and B is ok in the first case but not the second) and also temporal relations (e.g. “if she put the book on the shelf, then it fell off”).&lt;/p&gt;

&lt;h2 id=&quot;abduction&quot;&gt;Abduction&lt;/h2&gt;

&lt;p&gt;Abduction is the idea of going beyond just inference (i.e. concluding something beyond the information given) to actually inferring a theory or explanation for the conclusions. For example, when asked “If the trigger is pulled, then the pistol will fire. The trigger is pulled, but the pistol does not fire. Why not?”, people seem to augment their mental models in order to find a set of premises that does make the conclusions valid, such as that there were no bullets in the pistol.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I spent a long time thinking about the illusory inferences, and I’m dissatisfied with the explanation. Here is an example of one, where only one of the propositions is true:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If there is a king then there is an ace or else if there isn’t a king then there is an ace.
There is a king.
What follows?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;People invariably say “there is an ace”, but this is technically incorrect. The reason is that the way this is supposed to be parsed is something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;P1: king --&amp;gt; ace
P2: not king --&amp;gt; ace
P1 xor P2
king
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Because only one of P1 or P2 can be true, that means if P1 were false then &lt;code class=&quot;highlighter-rouge&quot;&gt;king --&amp;gt; not ace&lt;/code&gt; and therefore “there is an ace” is not a valid conclusion. Johnson-Laird claims this is based on how people construct their mental models, but I think it has more to do with the language of the scenario. When I write it out as I did just above, the answer is much more obvious—it took me a long time to realize why it was obvious because I was actually parsing the entire situation incorrectly. Perhaps it is just my programming background, but I intuitively parsed this statement into a program like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;king = True
if king:
    ace = True
elif not king:
    ace = True
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Clearly, at the end of execution, &lt;code class=&quot;highlighter-rouge&quot;&gt;ace&lt;/code&gt; will be true. I think the effect would be much lessened if instead the problem were given like:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There are two rules, only one of which is true: (1) a king implies an ace, or (2) lack of a king implies an ace.
There is a king.
What follows?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An informal test of $n=1$ suggests this is a better phrasing, as they correctly determined that there is nothing you can deduce here. Looking it up, I see that Johnson-Laird did test for exactly this in &lt;a href=&quot;http://mentalmodels.princeton.edu/papers/1999illusory.pdf&quot;&gt;Experiment 2 of this paper&lt;/a&gt;. They did find that more people got the correct answer when they changed the language (25% vs 0%), though the majority still made the error.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 13:28:36 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Johnson-Laird2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Johnson-Laird2012.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Qualitative modeling</title>
        <description>&lt;p&gt;&lt;span id=&quot;Forbus2011&quot;&gt;Forbus, K. D. (2011). Qualitative modeling. &lt;i&gt;Wiley Interdisciplinary Reviews: Cognitive Science&lt;/i&gt;, &lt;i&gt;2&lt;/i&gt;(4), 374–391. doi:10.1002/wcs.115&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this article, Forbus summarizes the state of the field of qualitative reasoning, explaining how qualitative reasoning works and what it can be applied to. He begins by outlining three principles that are core to qualitative modeling:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Discretization&lt;/em&gt; – qualitative representations are almost always discrete.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Relevance&lt;/em&gt; – the manner in which the discretization is chosen (an in general how the model is set up) depends on the relevance of different aspects of the situation that is being modeled.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ambiguity&lt;/em&gt; – predictions made by qualitative models are ambiguous and there may be multiple possible predictions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Next, Forbus describes what a qualitative representation is. I won’t go into detail on this, since it’s mostly covered by my notes on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Kuipers1986.html&quot;&gt;Kuipers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Given the definitions for qualitative representations, Forbus describes the qualitative mathematical operations that can be applied to them. One of &lt;em&gt;qualitative proportionality&lt;/em&gt;, which is that “all else being equal, if $B$ increases, then $A$ will increase, and if $B$ decreases, then $A$ will decrease” (pg. 4). To get around the fact that not all functions are monotonic, Forbus describes to options. First, functions can be broken up into &lt;em&gt;model fragments&lt;/em&gt; where monotonicity holds within a given fragment. Second, a &lt;em&gt;compositional modeling language&lt;/em&gt; can be used instead. The notation used in the explanation of the compositional modeling language isn’t explained, though, so I’m not entirely clear on how it works. Same thing goes for &lt;em&gt;confluences&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Forbus moves on to describe the pieces of a qualitative model, which include &lt;em&gt;processes&lt;/em&gt; (e.g., heat flow), &lt;em&gt;components&lt;/em&gt; (individual discrete parts that can be combined), and &lt;em&gt;fields&lt;/em&gt; (a division of space in qualitatively distinct regions where some qualitative parameter is constant). These pieces, represented as model fragments, are assembed in the &lt;em&gt;compositional modeling&lt;/em&gt; methodology to form a &lt;em&gt;domain theory&lt;/em&gt;. There are also &lt;em&gt;modeling assumptions&lt;/em&gt; which represent choices that need to be made depending on their relevance to the situation (for example, whether a thermal object should be considered a regular thermal object or just a temperature source).&lt;/p&gt;

&lt;p&gt;The qualitative model can then be used in a qualitative simulation. I also won’t go into details here as I already wrote about it in my notes on &lt;a href=&quot;/quals/mental%20models/2016/01/04/Kuipers1986.html&quot;&gt;Kuipers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Forbus discusses how qualitative modeling can be used to model causality and spatial reasoning. For spatial reasoning, there are a few considerations: topology (which can be represented using a region connection calculus), direction, position, and shape. Space itself also needs to be decomposed into discrete regions and edges; this can be done with something called &lt;em&gt;place vocabularies&lt;/em&gt; (e.g. this is what was used by the FROB system, see &lt;a href=&quot;/quals/mental%20models/2015/12/19/Gentner1983.html&quot;&gt;Chapter 4 of Gentner&lt;/a&gt;).&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The term “simulation” when used in “qualitative simulation” does seem to mean something along the lines of simulation of a physical process—though because it is discrete and approximate, it is perhaps closer to something like the simulations used in Monte-Carlo Tree Search. It would be interesting to see how qualitative simulation would do in certain reinforcement learning settings when combined with something like MCTS. For example, in playing Atari games, maybe something like qualitative simulation would be useful for efficiently determining high-level actions a player could take (rather than the low level actions of left/right/jump/etc).&lt;/p&gt;

&lt;p&gt;One thing that is not clear to me, though, is how qualitative modeling fits into scenarios where there is uncertainty about the environment. In one sense, it’s great at doing that, because you don’t necessarily need to numerically specify every parameter. In another sense, though, it doesn’t seem like qualitative modeling would work well in an uncertain case where, for example, it might be &lt;em&gt;possible&lt;/em&gt; for a situation to unfold in a certain way (depending on the initial conditions), but highly unlikely. It doesn’t seem like qualitative simulation is set up for being able to express the magnitude of uncertainty or quality.&lt;/p&gt;

&lt;p&gt;Also, despite their suggestion to the contrary, qualitative models suffer from many of the same issues that are discussed by &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/28/Davis.html&quot;&gt;Davis &amp;amp; Marcus&lt;/a&gt; as physical simulation does. Importantly, qualitative modeling doesn’t free you from having to make hard choices about &lt;em&gt;how&lt;/em&gt; to set the simulation up, something that Forbus explicitly notes in this paper. You still have to make the relevant decisions about which properties are important, how to do the discretization, etc.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 08:58:54 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Forbus2011.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Forbus2011.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Qualitative simulation</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kuipers1986&quot;&gt;Kuipers, B. (1986). Qualitative Simulation. &lt;i&gt;Artificial Intelligence&lt;/i&gt;, &lt;i&gt;29&lt;/i&gt;(3), 289–338. doi:10.1016/0004-3702(86)90073-1&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Kuipers gives an overview of what qualitative simulation is, how it can be used, and what some its limitations are. He focuses specifically on the QSIM system, which is similar to other qualitative reasoning systems (e.g. from de Kleer, Forbus) but which also has a few differences. Regardless of the specific system, the point of qualitative reasoning is to break down continuous sytems into discrete analogues. Given a set of constraints that the continuous system must follow, along with an initial qualitative state description, qualitative reasoning determines how the system must evolve without necessarily specifying any precise values. For example, qualitative reasoning can be used to infer that if a ball is thrown upwards, it must eventually come down again (but the specific position and velocity of the ball need not be specified).&lt;/p&gt;

&lt;p&gt;Kuipers shows that the QSIM method will always produce the true behavior of the system that it describes, but it may additionally produce false behaviors if the proper constraints aren’t given (e.g. conservation of energy).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;qualitative-behavior&quot;&gt;Qualitative behavior&lt;/h2&gt;

&lt;p&gt;The qualitative description of a continuous and differentiable function $f:[a,b]\rightarrow\mathbb{R}^*$ relies on a set of &lt;em&gt;landmark values&lt;/em&gt;, which include 0, $f(a)$, $f(b)$, and $f(t)$ where $t$ is a critical point of the function. A &lt;em&gt;distinguished time point&lt;/em&gt; is the corresponding $t$ for a landmark value.&lt;/p&gt;

&lt;p&gt;Then, the &lt;em&gt;qualitative state&lt;/em&gt; of the function at a particular point in time is $\mathrm{QS}(f,t)=[\mathrm{qval},\mathrm{qdir}]$, where $\mathrm{qval}$ is either a landmark value or the interval between landmark values, and where $\mathrm{qdir}$ is $\mathrm{sgn}(f^\prime(t))$. The &lt;em&gt;qualitative behavior&lt;/em&gt; of a function is the sequence $[\mathrm{QS}(f,t_0),\mathrm{QS}(f,t_0,t_1),\mathrm{QS}(f,t_1),\ldots{},\mathrm{QS}(f,t_{n-1},t_n),\mathrm{QS}(f,t_n)]$, where each $t_i$ is a distinguished time point.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;qualitative state transition&lt;/em&gt; is either a &lt;em&gt;P-transition&lt;/em&gt;, which is the transition from a distinguished time point to an interval between distinguished time points: $\mathrm{QS}(f,t_i)\Rightarrow \mathrm{QS}(f, t_i, t_{i+1})$. Similarly, a &lt;em&gt;I-transition&lt;/em&gt; is the other way around: $\mathrm{QS}(f,t_{i-1},t_i)\Rightarrow\mathrm{QS}(f,t_i)$.&lt;/p&gt;

&lt;h2 id=&quot;qualitative-structure&quot;&gt;Qualitative structure&lt;/h2&gt;

&lt;p&gt;Constraints can be imposed on either a single function, or on a pair of functions, for example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ADD}(f, g, h)$ is defined as $f(t)+g(t)=h(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{MULT}(f, g, h)$ is defined as $f(t)\cdot{}g(t)=h(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{MINUS}(f, g)$ is defined as $f(t)=-g(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{DERIV}(f, g)$ is defined as $f^\prime(t)=g(t)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{M}^+(f, g)$ is defined as $f(t)=H(g(t))$ where $H^\prime(x)&amp;gt;0$. This rougly says that $f$ and $g$ are directly proportional&lt;/li&gt;
  &lt;li&gt;$\mathrm{M}^-(f, g)$ is defined as $f(t)=H(g(t))$ where $H^\prime(x)&amp;lt;0$. This roughly says that $f$ and $g$ are inversely proportional.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that these constraints only need to hold for $t$ within the domain of $f$ and $g$ (i.e., so you could have $x=\cos\theta$ on $\theta\in(0,\pi)$ and then $\mathrm{M}^-(\theta, x)$ will be true for that specific range of $\theta$).&lt;/p&gt;

&lt;h2 id=&quot;qualitative-simulation&quot;&gt;Qualitative simulation&lt;/h2&gt;

&lt;p&gt;The qualitative simulation takes as input the functions, constraints, landmark values, upper and lower range limits, an initial time point $t_0$, and initial qualitative values for the functions.&lt;/p&gt;

&lt;p&gt;The qualitative simulation returns as output the qualitative behavior descriptions for the given functions, which include: the distinguished time points, landmark values (which may include additional ones not given as input), and qualitative state descriptions for each distinguished time point and interal between time points.&lt;/p&gt;

&lt;p&gt;The way that the simulation works for a single state is, roughly:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;For each function, determine the P-transitions and I-transitions for the current qualitative state&lt;/li&gt;
  &lt;li&gt;For each constraint, form combinations of “transition tuples” for each of the functions involved in the constraint, and filter out those which violate the constraint&lt;/li&gt;
  &lt;li&gt;For each constraint, filter our transition tuples for which there are no transition tuples for adjacent constraints that have the same transition for the shared parameter (e.g. $(I4,I9)$ for $\mathrm{DERIV}(Y,V)$ would be filtered out if there is no corresponding tuple beginning with $I9$ for $\mathrm{DERIV}(V, A)$).&lt;/li&gt;
  &lt;li&gt;Generate possible global interpretations based on the remaining tuples&lt;/li&gt;
  &lt;li&gt;Check which interpretations constitute either a state that has been visited before (cylce) or if the function diverges (goes to infinity). These are leaves of the seach tree; others are next possible states.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This was a really nice, detailed explanation of how (one particular version of) qualitative simulation and reasoning works. I think it’s cool that you can show that this type of qualitative reasoning will produce the right answer (even if it might also produce other answers). It seems like the big challenge is—as Kuipers points out in the discussion—is determining what the constraints on the system are in the first place. This is somewhat related to the question of how you set up a physical simulation in the first place (i.e. object positions, velocities), though in this case the choices that need to be made are the functional constraints, rather than specifics about the object. Certaintly in qualitative simulation it seems that it is easier to specify information about the state of the object: the hard part is in determining what information to specify about the function.&lt;/p&gt;

&lt;p&gt;Also, this seems to apply specifically to continuous systems than can be described by a differential equation, though, so I’m interested in understanding better how this approach works with things that are not continuous (for example, colliding objects) or situations where there are additional types of constraints, such as inequality or equality constraints. I expect some of the next readings will give me a better sense of this.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Jan 2016 04:25:40 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20models/2016/01/04/Kuipers1986.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20models/2016/01/04/Kuipers1986.html</guid>
        
        
        <category>Mental models</category>
        
      </item>
    
      <item>
        <title>Embodied language: a review of the role of the motor system in language comprehension</title>
        <description>&lt;p&gt;&lt;span id=&quot;Fischer2008&quot;&gt;Fischer, M. H., &amp;amp; Zwaan, R. A. (2008). Embodied language: a review of the role of the motor system in language comprehension. &lt;i&gt;Quarterly Journal Of Experimental Psychology&lt;/i&gt;, &lt;i&gt;61&lt;/i&gt;(6), 825–850. doi:10.1080/17470210701623605&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This is a review article by Fischer &amp;amp; Zwaan about whether and how the motor system is engaged in language comprehension. They are primarily interested in understanding how internal “simulations” by the motor system of an action affects, or is affected by, language describing the same action—a phenomena they term &lt;em&gt;motor resonance&lt;/em&gt;. They don’t give a precise definition of it, beyond that it is the “possibility that language comprehension may incorporate, and possibly even require as an essential component, some activity of the motor system that could be characterized as ‘motor resonance’” (pg. 826).&lt;/p&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan first review a set of theories regarding the relationship between perception and action. These include the &lt;em&gt;two-visual-pathways theory&lt;/em&gt; (which holds that there are two separate pathways in the brain, one for perception and one for action), the &lt;em&gt;theory of event coding&lt;/em&gt; or TEC (which holds that action and perception use the same underlying representations and thus compete for cognitive resources of those representations), &lt;em&gt;mirror neurons&lt;/em&gt; (which are neurons that are active when we engage in an action, and when we see others engaging in the same action), and &lt;em&gt;motor cognition&lt;/em&gt; (which is based on the idea that “we simulate our own as well as other people’s behavior as part of understanding it”, pg. 831).&lt;/p&gt;

&lt;p&gt;Next, Fischer &amp;amp; Zwaan discuss what mechanisms are involved in action simulation. The first mechanism they discuss is computation of affordances, i.e., that “the motor system spontaneously uses object information to compute possible actions in the light of one’s current posture and to select favourable responses” (pg. 831). Similarly, they discuss evidence that planning an action to a particular location can facilitate perception in that area. The second mechanism they discuss is motor resonance during action observation, which seems to be the idea that people use their motor system to make predictions about the actions of others during observation of those actions. The third mechanism is the time course of motor simulation, which they seem to suggest is involved specifically with forward prediction (and therefore facilitates judgements that involve a prediction, as opposed to a reverse inference or something unrelated). However, Fischer &amp;amp; Zwaan also state that “viewing the result of an action activates the processes that would bring about that result”, which is really an inference, not a forward simulation.&lt;/p&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan now turn to discussing how the motor system is involved in actual language comprehension. They make the distinction between &lt;em&gt;communicative motor resonance&lt;/em&gt; (i.e., motor simulation of speech production) versus &lt;em&gt;referential motor resonance&lt;/em&gt; (i.e., motor simulation of the action described by the language). They discuss evidence for communicative motor resonance in lower level phonological processing, and evidence for referential motor resonance in lexical access (i.e. the semantic meaning of individual words) as well as full sentence comprehension (in which information from multiple words must be integrated) and also discourse comprehension (which is, as they note, more ecologically valid as it is how we interpret language in real-world contexts).&lt;/p&gt;

&lt;p&gt;They close with three questions. First, the &lt;em&gt;association question&lt;/em&gt; is whether activation of the motor system co-occurs when performing a cognitive task (they state that they think the evidence reviewed in this article supports an answer of “yes” to this question). Second, the &lt;em&gt;necessity question&lt;/em&gt; is whether the activation of the motor system is required for language comprehension. Third, the &lt;em&gt;sufficiency question&lt;/em&gt; is whether activation of the motor system is sufficient for language comprehension.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan seem to use the term “simulation” here to mean neural activation (in the mirror neuron sense) of the same motor regions that are used for actually executing actions. This view, to me, isn’t that useful, though—talking about simulation as &lt;em&gt;neural&lt;/em&gt; simulation is either tautological or unrealistic, depending on how you view it. On the one hand, if we recognize actions in other people as the same types of actions we ourselves make, it would be incredibly surprising if there were no shared neural activity. On the other hand, if it is more than just a shared representation and is actually involved in &lt;em&gt;producing&lt;/em&gt; the action, then it doesn’t make much sense why motor simulation wouldn’t also produce the action.&lt;/p&gt;

&lt;p&gt;I don’t doubt that some sort of simulation that unifies perception, action, and cognition is involved in language comprehension, but talking about it at the level of neural simulation seems like the wrong level of abstraction. For me, it’s more helpful to think of it in terms of something like a mental model, a la &lt;a href=&quot;/quals/embodied%20language/2016/01/03/Matlock2004.html&quot;&gt;Matlock&lt;/a&gt;, &lt;a href=&quot;/quals/embodied%20language/2016/01/03/Bergen2007.html&quot;&gt;Bergen et al.&lt;/a&gt;, or &lt;a href=&quot;/quals/mental%20imagery/2016/01/01/Grush2004.html&quot;&gt;Grush&lt;/a&gt;. I don’t think these accounts are at odds with one another: you can still discuss motor simulation at the level of a mental model or emulator, but you don’t need to get into the tricky business of trying to interpret what it &lt;em&gt;actually&lt;/em&gt; means to observe correlations of neural activity.&lt;/p&gt;

&lt;p&gt;I do like the distinction that Fischer &amp;amp; Zwaan make between communicative and referential motor resonance. I would say that communicative motor resonance has more to do with perception—inferring the underlying motor process that is producing the auditory sensations. On the other hand, the referential motor resonance is more about cognition—constructing a mental simulation of percepts and actions for the purposes of comprehension and interpretation. (I am purposefully not using the terms “embodied” or “grounded” here: while I think it is incredibly useful for the mind to construct models of how perception and action work, I don’t think it is a requirement for &lt;em&gt;all&lt;/em&gt; of cognition to ground out in the real world).&lt;/p&gt;

&lt;p&gt;Overall, I’m a bit dissatisfied with the account in this article: the overarching hypothesis is that “motor simulation is required for language comprehension” but Fischer &amp;amp; Zwaan don’t really make an attempt to explain &lt;em&gt;why&lt;/em&gt; that’s the case. Simply imitating someone else’s action if you don’t know what the action is &lt;em&gt;for&lt;/em&gt; doesn’t help you understand it, and I think the same is true for the motor simulation account as well. I agree motor simulation is real, and believe it is involved in comprehension, but I want an account for why it helps and what it brings to the table. The argument that I’d make is that we construct mental models/simulations from which we can make predictions, inferences, etc., and taking actions within the model (i.e. imagining those actions) is the way in which we manipulate the model or test out our predictions. It’s not simply an imitation of the motor action, but a way of identifying and choosing hypotheses about what information a sentence is conveying.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 15:11:01 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Fischer2008.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Fischer2008.html</guid>
        
        
        <category>Embodied language</category>
        
      </item>
    
      <item>
        <title>Spatial and linguistic aspects of visual imagery in sentence comprehension</title>
        <description>&lt;p&gt;&lt;span id=&quot;Bergen2007&quot;&gt;Bergen, B. K., Lindsay, S., Matlock, T., &amp;amp; Narayanan, S. (2007). Spatial and linguistic aspects of visual imagery in sentence comprehension. &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;31&lt;/i&gt;(5), 733–64. doi:10.1080/03640210701530748&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Bergen et al. examine some of the ways that language triggers the use of visual mental imagery. In particular, they look at the Perky effect, which they describe as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In a seminal study, Perky (1910) asked participants to imagine seeing an object (such as a banana or leaf) while they were looking at a blank screen. At the same time, unbeknownst to them, an actual image of the same object was projected on the screen, starting below the threshold for conscious perception, but with progressively greater and greater illumination. Perky found that many participants continued to believe that they were still just imagining the stimulus and failed to recognize that there was actually a real, projected image even at levels where the projected image was perfectly perceptible to participants not simultaneously performing imagery. (pg. 736)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The Perky effect can be explained based on competition for the same set of resources: visual perception uses some of the same resources as mental imagery, and so if mental imagery is engaged, it can interfere with our ability to perceive things. This paradigm can be extended to measure how mental imagery is being used in language comprehension, by having people listen to a sentence just before performing a visual perception task.&lt;/p&gt;

&lt;p&gt;Using the above paradigm, Bergen et al. find that nouns and verbs that are associated with concrete motion or spatial locations (e.g. as in &lt;em&gt;The ceiling shook&lt;/em&gt; or &lt;em&gt;The ball dropped&lt;/em&gt;) do invoke mental imagery in the relevant location of the visual field. In contraast, metaphorical motion does not produce this effect (e.g. &lt;em&gt;The market fell&lt;/em&gt;), nor do abstract verbs that are associated with up/down but which do not literally mean up/down (e.g. &lt;em&gt;The quantity dwindled&lt;/em&gt;). Their conclusion is that concrete sentences evoke visual mental imagery, but metaphorical and abstract sentences—even if they do involve some form of motion of spatial location—do not engage imagery (or at least not in the same way).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In all experiments, the task was to listen to a sentence, after which a circle or square would appear in one portion of the screen (up, down, left, right). Participants had to judge whether the object was a circle or square. The hypothesis was that if mental imagery was being used in a particular portion of the visual field when the sentence was read, then it would take participants longer to judge the object if that object were in the same part of the visual field than if it was not.&lt;/p&gt;

&lt;p&gt;Experiment 1 looked at up/down verbs (e.g. &lt;em&gt;dropped&lt;/em&gt;, &lt;em&gt;rose&lt;/em&gt;). They did find that sentences with up verbs resulted in longer processing when the object was in the upper part of the screen than when it was in the lower part of the screen (5/5), and that sentences with down verbs resulted in longer proceessing when the object was in the lower part of the screen than when it was in the upper part of the screen (3/5).&lt;/p&gt;

&lt;p&gt;Experiment 2 looked at up/down nouns (e.g. &lt;em&gt;ceiling&lt;/em&gt;, &lt;em&gt;cellar&lt;/em&gt;). They found essentially the same results as before: sentences with up nouns resulted in longer processing times when the shape was in the upper part of the screen (3/5), and sentences with down nouns resulted in longer processing times when the shape was in the lower part of the screen (4/5).&lt;/p&gt;

&lt;p&gt;Experiment 3 looked at metaphorical motion (e.g. &lt;em&gt;The market fell&lt;/em&gt;), but did not find the effect either for metaphorical up verbs (2/5) or for metaphorical down verbs (2/5).&lt;/p&gt;

&lt;p&gt;Experiment 4 looked at abstract verbs with up/down connotations (e.g. &lt;em&gt;dwindled&lt;/em&gt;, &lt;em&gt;increased&lt;/em&gt;) and again did not find the effect either for abstract up verbs (3/5) or for abstract down verbs (2/5).&lt;/p&gt;

&lt;p&gt;Experiment 5 looked at axis (horizontal versus vertical) effects, rather than directional (up versus down) effects. They also did not find an effect here.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;First—wow, the Perky effect is awesome! I have heard variants of that before (e.g. it is hard to read and listen to language simultaneously) but I hadn’t heard of people actually conflating mental imagery and true perceptions.&lt;/p&gt;

&lt;p&gt;This paradigm used by Bergen et al. is really neat, though the fact that they didn’t find the expected effect for all of the individual sentences (even if they found it overall) is somewhat less compelling. It would be interesting to use eyetracking with these experiments to see if people are actually attending to the portion of the field that is hypothesized, particularly in the case of the abstract/metaphorical sentences—not finding saccades in those cases would strengthen their hypothesis that people are not actually constructing a concrete visual scene in the same way.&lt;/p&gt;

&lt;p&gt;On a different note, given that these types of sentences do seem to evoke mental imagery, it would be interesting to try to reverse &lt;a href=&quot;http://arxiv.org/abs/1411.4555&quot;&gt;some of&lt;/a&gt; the &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/deepimagesent/&quot;&gt;image captioning&lt;/a&gt; algorithms that are coming out of the machine learning literature in order to produce a type of simulated mental imagery, which perhaps could then augmented as more information comes in—so the first sentence might retrieve an image, which is then parsed into components, and as more sentences are heard, they modify the current scene to make it consistent with the linguistic description. Then, that scene could be used to do further reasoning about what is being said—e.g. verifying whether a description about it is true or not.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 11:17:02 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Bergen2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Bergen2007.html</guid>
        
        
        <category>Embodied language</category>
        
      </item>
    
  </channel>
</rss>
