<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 04 Jan 2016 00:39:52 -0800</pubDate>
    <lastBuildDate>Mon, 04 Jan 2016 00:39:52 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Embodied language: a review of the role of the motor system in language comprehension</title>
        <description>&lt;p&gt;&lt;span id=&quot;Fischer2008&quot;&gt;Fischer, M. H., &amp;amp; Zwaan, R. A. (2008). Embodied language: a review of the role of the motor system in language comprehension. &lt;i&gt;Quarterly Journal Of Experimental Psychology&lt;/i&gt;, &lt;i&gt;61&lt;/i&gt;(6), 825–850. doi:10.1080/17470210701623605&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This is a review article by Fischer &amp;amp; Zwaan about whether and how the motor system is engaged in language comprehension. They are primarily interested in understanding how internal “simulations” by the motor system of an action affects, or is affected by, language describing the same action—a phenomena they term &lt;em&gt;motor resonance&lt;/em&gt;. They don’t give a precise definition of it, beyond that it is the “possibility that language comprehension may incorporate, and possibly even require as an essential component, some activity of the motor system that could be characterized as ‘motor resonance’” (pg. 826).&lt;/p&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan first review a set of theories regarding the relationship between perception and action. These include the &lt;em&gt;two-visual-pathways theory&lt;/em&gt; (which holds that there are two separate pathways in the brain, one for perception and one for action), the &lt;em&gt;theory of event coding&lt;/em&gt; or TEC (which holds that action and perception use the same underlying representations and thus compete for cognitive resources of those representations), &lt;em&gt;mirror neurons&lt;/em&gt; (which are neurons that are active when we engage in an action, and when we see others engaging in the same action), and &lt;em&gt;motor cognition&lt;/em&gt; (which is based on the idea that “we simulate our own as well as other people’s behavior as part of understanding it”, pg. 831).&lt;/p&gt;

&lt;p&gt;Next, Fischer &amp;amp; Zwaan discuss what mechanisms are involved in action simulation. The first mechanism they discuss is computation of affordances, i.e., that “the motor system spontaneously uses object information to compute possible actions in the light of one’s current posture and to select favourable responses” (pg. 831). Similarly, they discuss evidence that planning an action to a particular location can facilitate perception in that area. The second mechanism they discuss is motor resonance during action observation, which seems to be the idea that people use their motor system to make predictions about the actions of others during observation of those actions. The third mechanism is the time course of motor simulation, which they seem to suggest is involved specifically with forward prediction (and therefore facilitates judgements that involve a prediction, as opposed to a reverse inference or something unrelated). However, Fischer &amp;amp; Zwaan also state that “viewing the result of an action activates the processes that would bring about that result”, which is really an inference, not a forward simulation.&lt;/p&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan now turn to discussing how the motor system is involved in actual language comprehension. They make the distinction between &lt;em&gt;communicative motor resonance&lt;/em&gt; (i.e., motor simulation of speech production) versus &lt;em&gt;referential motor resonance&lt;/em&gt; (i.e., motor simulation of the action described by the language). They discuss evidence for communicative motor resonance in lower level phonological processing, and evidence for referential motor resonance in lexical access (i.e. the semantic meaning of individual words) as well as full sentence comprehension (in which information from multiple words must be integrated) and also discourse comprehension (which is, as they note, more ecologically valid as it is how we interpret language in real-world contexts).&lt;/p&gt;

&lt;p&gt;They close with three questions. First, the &lt;em&gt;association question&lt;/em&gt; is whether activation of the motor system co-occurs when performing a cognitive task (they state that they think the evidence reviewed in this article supports an answer of “yes” to this question). Second, the &lt;em&gt;necessity question&lt;/em&gt; is whether the activation of the motor system is required for language comprehension. Third, the &lt;em&gt;sufficiency question&lt;/em&gt; is whether activation of the motor system is sufficient for language comprehension.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Fischer &amp;amp; Zwaan seem to use the term “simulation” here to mean neural activation (in the mirror neuron sense) of the same motor regions that are used for actually executing actions. This view, to me, isn’t that useful, though—talking about simulation as &lt;em&gt;neural&lt;/em&gt; simulation is either tautological or unrealistic, depending on how you view it. On the one hand, if we recognize actions in other people as the same types of actions we ourselves make, it would be incredibly surprising if there were no shared neural activity. On the other hand, if it is more than just a shared representation and is actually involved in &lt;em&gt;producing&lt;/em&gt; the action, then it doesn’t make much sense why motor simulation wouldn’t also produce the action.&lt;/p&gt;

&lt;p&gt;I don’t doubt that some sort of simulation that unifies perception, action, and cognition is involved in language comprehension, but talking about it at the level of neural simulation seems like the wrong level of abstraction. For me, it’s more helpful to think of it in terms of something like a mental model, a la &lt;a href=&quot;/quals/embodied%20language/2016/01/03/Matlock2004.html&quot;&gt;Matlock&lt;/a&gt;, &lt;a href=&quot;/quals/embodied%20language/2016/01/03/Bergen2007.html&quot;&gt;Bergen et al.&lt;/a&gt;, or &lt;a href=&quot;/quals/mental%20imagery/2016/01/01/Grush2004.html&quot;&gt;Grush&lt;/a&gt;. I don’t think these accounts are at odds with one another: you can still discuss motor simulation at the level of a mental model or emulator, but you don’t need to get into the tricky business of trying to interpret what it &lt;em&gt;actually&lt;/em&gt; means to observe correlations of neural activity.&lt;/p&gt;

&lt;p&gt;I do like the distinction that Fischer &amp;amp; Zwaan make between communicative and referential motor resonance. I would say that communicative motor resonance has more to do with perception—inferring the underlying motor process that is producing the auditory sensations. On the other hand, the referential motor resonance is more about cognition—constructing a mental simulation of percepts and actions for the purposes of comprehension and interpretation. (I am purposefully not using the terms “embodied” or “grounded” here: while I think it is incredibly useful for the mind to construct models of how perception and action work, I don’t think it is a requirement for &lt;em&gt;all&lt;/em&gt; of cognition to ground out in the real world).&lt;/p&gt;

&lt;p&gt;Overall, I’m a bit dissatisfied with the account in this article: the overarching hypothesis is that “motor simulation is required for language comprehension” but Fischer &amp;amp; Zwaan don’t really make an attempt to explain &lt;em&gt;why&lt;/em&gt; that’s the case. Simply imitating someone else’s action if you don’t know what the action is &lt;em&gt;for&lt;/em&gt; doesn’t help you understand it, and I think the same is true for the motor simulation account as well. I agree motor simulation is real, and believe it is involved in comprehension, but I want an account for why it helps and what it brings to the table. The argument that I’d make is that we construct mental models/simulations from which we can make predictions, inferences, etc., and taking actions within the model (i.e. imagining those actions) is the way in which we manipulate the model or test out our predictions. It’s not simply an imitation of the motor action, but a way of identifying and choosing hypotheses about what information a sentence is conveying.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 15:11:01 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Fischer2008.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Fischer2008.html</guid>
        
        
        <category>Embodied language</category>
        
      </item>
    
      <item>
        <title>Spatial and linguistic aspects of visual imagery in sentence comprehension</title>
        <description>&lt;p&gt;&lt;span id=&quot;Bergen2007&quot;&gt;Bergen, B. K., Lindsay, S., Matlock, T., &amp;amp; Narayanan, S. (2007). Spatial and linguistic aspects of visual imagery in sentence comprehension. &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;31&lt;/i&gt;(5), 733–64. doi:10.1080/03640210701530748&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Bergen et al. examine some of the ways that language triggers the use of visual mental imagery. In particular, they look at the Perky effect, which they describe as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In a seminal study, Perky (1910) asked participants to imagine seeing an object (such as a banana or leaf) while they were looking at a blank screen. At the same time, unbeknownst to them, an actual image of the same object was projected on the screen, starting below the threshold for conscious perception, but with progressively greater and greater illumination. Perky found that many participants continued to believe that they were still just imagining the stimulus and failed to recognize that there was actually a real, projected image even at levels where the projected image was perfectly perceptible to participants not simultaneously performing imagery. (pg. 736)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The Perky effect can be explained based on competition for the same set of resources: visual perception uses some of the same resources as mental imagery, and so if mental imagery is engaged, it can interfere with our ability to perceive things. This paradigm can be extended to measure how mental imagery is being used in language comprehension, by having people listen to a sentence just before performing a visual perception task.&lt;/p&gt;

&lt;p&gt;Using the above paradigm, Bergen et al. find that nouns and verbs that are associated with concrete motion or spatial locations (e.g. as in &lt;em&gt;The ceiling shook&lt;/em&gt; or &lt;em&gt;The ball dropped&lt;/em&gt;) do invoke mental imagery in the relevant location of the visual field. In contraast, metaphorical motion does not produce this effect (e.g. &lt;em&gt;The market fell&lt;/em&gt;), nor do abstract verbs that are associated with up/down but which do not literally mean up/down (e.g. &lt;em&gt;The quantity dwindled&lt;/em&gt;). Their conclusion is that concrete sentences evoke visual mental imagery, but metaphorical and abstract sentences—even if they do involve some form of motion of spatial location—do not engage imagery (or at least not in the same way).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In all experiments, the task was to listen to a sentence, after which a circle or square would appear in one portion of the screen (up, down, left, right). Participants had to judge whether the object was a circle or square. The hypothesis was that if mental imagery was being used in a particular portion of the visual field when the sentence was read, then it would take participants longer to judge the object if that object were in the same part of the visual field than if it was not.&lt;/p&gt;

&lt;p&gt;Experiment 1 looked at up/down verbs (e.g. &lt;em&gt;dropped&lt;/em&gt;, &lt;em&gt;rose&lt;/em&gt;). They did find that sentences with up verbs resulted in longer processing when the object was in the upper part of the screen than when it was in the lower part of the screen (5/5), and that sentences with down verbs resulted in longer proceessing when the object was in the lower part of the screen than when it was in the upper part of the screen (3/5).&lt;/p&gt;

&lt;p&gt;Experiment 2 looked at up/down nouns (e.g. &lt;em&gt;ceiling&lt;/em&gt;, &lt;em&gt;cellar&lt;/em&gt;). They found essentially the same results as before: sentences with up nouns resulted in longer processing times when the shape was in the upper part of the screen (3/5), and sentences with down nouns resulted in longer processing times when the shape was in the lower part of the screen (4/5).&lt;/p&gt;

&lt;p&gt;Experiment 3 looked at metaphorical motion (e.g. &lt;em&gt;The market fell&lt;/em&gt;), but did not find the effect either for metaphorical up verbs (2/5) or for metaphorical down verbs (2/5).&lt;/p&gt;

&lt;p&gt;Experiment 4 looked at abstract verbs with up/down connotations (e.g. &lt;em&gt;dwindled&lt;/em&gt;, &lt;em&gt;increased&lt;/em&gt;) and again did not find the effect either for abstract up verbs (3/5) or for abstract down verbs (2/5).&lt;/p&gt;

&lt;p&gt;Experiment 5 looked at axis (horizontal versus vertical) effects, rather than directional (up versus down) effects. They also did not find an effect here.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;First—wow, the Perky effect is awesome! I have heard variants of that before (e.g. it is hard to read and listen to language simultaneously) but I hadn’t heard of people actually conflating mental imagery and true perceptions.&lt;/p&gt;

&lt;p&gt;This paradigm used by Bergen et al. is really neat, though the fact that they didn’t find the expected effect for all of the individual sentences (even if they found it overall) is somewhat less compelling. It would be interesting to use eyetracking with these experiments to see if people are actually attending to the portion of the field that is hypothesized, particularly in the case of the abstract/metaphorical sentences—not finding saccades in those cases would strengthen their hypothesis that people are not actually constructing a concrete visual scene in the same way.&lt;/p&gt;

&lt;p&gt;On a different note, given that these types of sentences do seem to evoke mental imagery, it would be interesting to try to reverse &lt;a href=&quot;http://arxiv.org/abs/1411.4555&quot;&gt;some of&lt;/a&gt; the &lt;a href=&quot;http://cs.stanford.edu/people/karpathy/deepimagesent/&quot;&gt;image captioning&lt;/a&gt; algorithms that are coming out of the machine learning literature in order to produce a type of simulated mental imagery, which perhaps could then augmented as more information comes in—so the first sentence might retrieve an image, which is then parsed into components, and as more sentences are heard, they modify the current scene to make it consistent with the linguistic description. Then, that scene could be used to do further reasoning about what is being said—e.g. verifying whether a description about it is true or not.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 11:17:02 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Bergen2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Bergen2007.html</guid>
        
        
        <category>Embodied language</category>
        
      </item>
    
      <item>
        <title>Fictive motion as cognitive simulation</title>
        <description>&lt;p&gt;&lt;span id=&quot;Matlock2004&quot;&gt;Matlock, T. (2004). Fictive motion as cognitive simulation. &lt;i&gt;Memory And Cognition&lt;/i&gt;, &lt;i&gt;32&lt;/i&gt;(8), 1389–1400. doi:10.3758/BF03206329&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Matlock asks whether mental simulation is involved in processing sentences that involve motion words but which do not actually describe literal motion (the term for this type of motion is &lt;em&gt;fictive motion&lt;/em&gt;). In a series of four quite elegant experiments, Matlock shows that people take longer to respond whether a sentence involving fictive motion was related to a story when the story described long/slow/difficult travel as opposed to short/fast/easy travel. The results suggest that people may be constructing a mental model of the scenario described by the story, and that then to understand the target sentence they simulate from this mental model. When the model actually involves motion that that is longer, slower, or more difficult, people accordingly run simulations that mimic those properties, and thus take a longer time to respond.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;Experiment 1 used stories involving long-distance vs. short-distance travel. For example, the story might be about someone driving across the desert to visit a relative, and the desert is either large or small. The target sentence would then be something like “the road crosses the desert”. In the control study, they instead used target sentences like “the road is in the desert”. In this case they found participants took about 400ms longer to judge the target sentence in the long-distance stories.&lt;/p&gt;

&lt;p&gt;Experiment 2 used stories involving slow vs. fast motion. For example, the story might be about someone walking or running along a path. The target sentence would then be something like “the path follows the creek”. In the control study, they instead used target sentences like “the path is next to the creek”. In this case they found participants again took about 400ms longer to judge the target sentences in the slow-motion stories.&lt;/p&gt;

&lt;p&gt;Experiment 3 used stories involving difficult vs. easy terrain. For example, the story might be about someone driving along a coastline which is either jagged and hilly or straight and flat. The target sentence would then be something like “a road runs along the peninsula”. In the control study, they instead used target sentences like “there is a road along the peninsula”. In this case they found participants took about 350ms longer to judge the target sentences in the rough terrain stories.&lt;/p&gt;

&lt;p&gt;Experiment 4 used stories that didn’t actually involve motion at all—for example, a story describing an earthquake fault along terrain that is either difficult or easy. The target sentence would then be something like “an earthquake fault runs across the valley”. There was no control experiment here, but I would expect the results to be similar. They found participants took about 200ms longer to judge the target sentences in the difficult terrain stories.&lt;/p&gt;

&lt;p&gt;In all the experiments there were “filler” scenarios that did not involve fictive motion but did involve regular motion.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;It’s not surprising to me that people might use something like mental simulation to help them interpret sentences, but I am surprised that the motion words have as strong of an effect as they do: i.e., that “the road crosses the desert” takes longer to read for a long-distance scenario than a short-distance scenario, while “the road is in the desert” is read at pretty much the same speed. I’m really curious to think about how this type of mental simulation serves interpretation of language from a computational point of view. Is it something like: we construct this mental model, and then when asked if “the road crosses the desert” is true, we look at our mental model and verify (perhaps visually) that it does in fact cross the desert (which would then take longer for longer distances)? Whereas, for “the road is in the desert”, all that is required is to verify that the road is in the desert, which should take the same amount of time regardless of how big the desert is?&lt;/p&gt;

&lt;p&gt;I do wonder whether these results apply in the general case of interpreting language. For one, participants were explicitly told to imagine the scenario as they read it (and the first sentence of each story was of the form “imagine a road”). If they weren’t prompted to use imagery, would there still be an effect? Also, if the background story were described in much less detail, would there be an effect?&lt;/p&gt;

&lt;p&gt;I also wonder if this is an effect just of something like a priming effect, versus being explicitly about running a mental simulation. In particular, they didn’t test reaction times on target sentences that were irrelevant to the scenario. For example, if the sentence was “the river crosses the road” (when the story was about a road and didn’t mention a river), would people still take longer in the long-distance condition? If so, then that might point to something more like a priming effect that is engaged when the person reads a motion word like “crosses”, rather than them running a mental simulation (because there is no reason for a mental simulation about a river crossing a road to take longer, as it is irrelevant to the long-distance aspects of the story).&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 08:57:21 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Matlock2004.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/embodied%20language/2016/01/03/Matlock2004.html</guid>
        
        
        <category>Embodied language</category>
        
      </item>
    
      <item>
        <title>On discriminative vs. generative classifiers: a comparison of logistic regression and naive Bayes</title>
        <description>&lt;p&gt;&lt;span id=&quot;Ng2002&quot;&gt;Ng, A. Y., &amp;amp; Jordan, M. I. (2002). On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes. &lt;i&gt;Advances In Neural Information Processing Systems&lt;/i&gt;, &lt;i&gt;14&lt;/i&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Ng &amp;amp; Jordan give a nice analysis of generative vs. discriminative classifiers, and show that there are actually two modes where either type of model might be preferred. They focus in particular on logistic regression vs. naive Bayes, but say that their analysis should be extendable to other generative-discriminative pairs of models.&lt;/p&gt;

&lt;p&gt;There are two key results in their analysis:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The asymptotic error of a generative linear classifier is greater than the asymptotic error of a discriminative linear classifier (which converges to the best linear classifier overall).&lt;/li&gt;
  &lt;li&gt;The number of training examples required for a discriminative linear classifier to reach its asymptotic error is $O(n)$, where $n$ is the &lt;a href=&quot;https://en.wikipedia.org/wiki/VC_dimension&quot;&gt;VC dimension&lt;/a&gt;. In contrast, the number of examples for a generative linear classifier is $O(\log{n})$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They show these propositions to be empirically true across a number of experiments on the UCI datasets.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The results from Ng &amp;amp; Jordan suggest that generative classifiers may be more useful in situations where there are small amounts of data. However, if there is a lot of data available, it may be more useful to use a discriminative classifier because it will ultimately have less error. The intuition behind this (I think?) has to do with the fact that the generative classifier needs to make assumptions about the distributions for both the likelihood and the prior, while the discriminative model does not. In some cases, if the assumptions are correct, then the generative classifier should have the same asymptotic error as the discriminative classifier. If my intuition about this is correct, then the question is essentially: can you estimate how incorrect your assumptions are, and then use that estimate (combined with knowledge about how much data you have) to determine whether to train a generative vs. discriminative classifier?&lt;/p&gt;

&lt;p&gt;Of course, there are also other reasons to potentially use a generative model besides just faster asymptotic error. In many cases, it may be necessary to be able to invert the model (i.e. sometimes you may need $p(x\vert y)$, and sometimes you may need $p(y\vert x)$). Intuitively, it seems like if you need to do this, it is going to be more efficient to train a generative model (from which you can compute both $p(y\vert x)$ and $p(x\vert y)$) rather than training multiple discriminative models.&lt;/p&gt;

&lt;p&gt;One question I have about this is regarding training discriminative models with generative models—if you learn a generative model first, and then use samples from it to train a discriminative model, how does that affect the error of the discriminative model? Can the discriminative model only do as well as the generative model, in that case? I want to say the answer is yes, but perhaps combined with true data from the world (in addition to samples from the generative model) the discriminative model could eventually achieve a lower error.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 07:45:06 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/03/Ng2002.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/03/Ng2002.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</title>
        <description>&lt;p&gt;&lt;span id=&quot;Clark2013&quot;&gt;Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. &lt;i&gt;The Behavioral And Brain Sciences&lt;/i&gt;, &lt;i&gt;36&lt;/i&gt;(3), 181–204. doi:10.1017/S0140525X12000477&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this BBS article, Clark lays out a grand unified theory of cognition based on the idea that our brains are predictive machines. At the high levels of processing, our brains construct hypotheses/predictions of our percepts, and at the low levels of processing, our sensory systems compute error signals between what is predicted and what is actually sensed. He begins with the following assumption about what the brain is trying to accomplish:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How, simply on the basis of patterns and changes in its own internal states, is [the brain] to alter and adapt its resources so as to tune itself to act as a useful node… for the origination of adaptive responses? Notice how different this conception is to ones in which the problem is posed as one of establishing a mapping relation between environmental and inner states. The task is not to find such a mapping but to infer the nature of the signal source (the world) from just the varying input signal itself. (pg. 183)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clark next defines his overarching theory as that of “action-oriented predictive processing”, and quotes from Hawkins &amp;amp; Blakeslee (2004):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As strange as it sounds, when your own behavior is involved, your predictions not only precede sensation, they determine sensation. Thinking of going to the next pattern in a sequence causes a cascading prediction of what you should experience next. As the cascading predition unfolds, it generates the motor commands necessary to fulfil the prediction. Thinking, predicting, and doing are all part of the same unfolding of sequences moving down the cortical hierarchy. (pg. 186)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, action is just a way to bring the world into alignment with the brain’s predictions.&lt;/p&gt;

&lt;p&gt;Clark goes on to describe a large body of work that supports these views, particularly from the realm of neuroscience and perceptual science. He also explains how the action-oriented predictive processing scheme unifies the levels of analysis, in that it is a neural implementation of what are actually generative Bayesian models.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I agree with a lot of what Clark says in this article, though there are a few points that I disagree with.&lt;/p&gt;

&lt;p&gt;First, “predictive” is &lt;em&gt;not&lt;/em&gt; the same as “generative” (though all generative models are predictive, in a sense). Discriminative models are also predictive, in the sense that you receive an input $x$ and predict an output $y$. Generative models, by contrast, can jointly predict $x$ and $y$ simultaneously, making it possible to predict $y$ from $x$ but also the other way around (predicting $x$ from $y$). Thus, generative models have more predictive power in the sense that they can be used to predict more relationships between the data, but that doesn’t mean discriminative models aren’t predictive. Clark uses “predictive” and “generative” interchangeably, but I think that is a mistake.&lt;/p&gt;

&lt;p&gt;I definitely agree that having a generative model is incredibly powerful because it allows us to make &lt;em&gt;a priori&lt;/em&gt; predictions in the absence of data. However, generative models can be more difficult to learn, and while they may be overall more general, that may come with the cost of a loss of precision for specific prediction tasks. Thus, discriminative models are important, too, and it is almost certainly incorrect to say that the brain never makes use of simple mappings.&lt;/p&gt;

&lt;p&gt;I’m not sure I agree with Clark’s characterization of action. I do like the idea that our minds make predictions, and then we use action to make those predictions come true, but I think it is important to make a distinction between predictions of the generative process in the world (i.e., what is going to happen next in the absence of action) versus predictions of how the default process might change &lt;em&gt;if&lt;/em&gt; we were to act on it.&lt;/p&gt;

&lt;p&gt;I am also not sure if I agree with the idea that the brain is trying to reduce &lt;em&gt;error&lt;/em&gt;, especially since “error” is not a well-defined term. Clark seems to use it in the sense of reducing entropy, but there’s a lot of other ways it could be used. By definition, any type of learning system is trying to optimize some objective function—the “error”—so while technically true, this  isn’t really that new of a concept, so I don’t feel that it provides all that much explanatory power.&lt;/p&gt;

&lt;p&gt;The more important component of Clark’s argument is that of the large part generative models probably play. Yet, Clark doesn’t go into details about how those models are actually constructed, beyond referring to a few things like the Helmholtz machine. That is where I think the truly difficult problems lie—determining &lt;em&gt;what&lt;/em&gt; the generative models are, &lt;em&gt;how&lt;/em&gt; we construct them in the first place, and &lt;em&gt;when&lt;/em&gt; a generative model is the thing that’s constructed as opposed to a discriminative model. There are particularly difficult questions to be answered in domains where the models must be able to handle incredibly high-dimensional data and parse it into complex, structured models. Saying the brain constructs generative models is a useful starting point (and it’s a starting point that I agree with), but it really doesn’t say anything about what the actual structure of those models is.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 06:05:44 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/03/Clark2013.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/03/Clark2013.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>The role of generative knowledge in object perception</title>
        <description>&lt;p&gt;&lt;span id=&quot;Battaglia2012&quot;&gt;Battaglia, P. W., Kersten, D., &amp;amp; Schrater, P. R. (2012). The Role of Generative Knowledge in Object Perception. In J. Trommershauser, K. P. Körding, &amp;amp; M. S. Landy (Eds.), &lt;i&gt;Sensory Cue Integration&lt;/i&gt;. Oxford University Press.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this chapter, Battaglia et al. describe the differing roles of the &lt;em&gt;generative process&lt;/em&gt;, people’s &lt;em&gt;generative knowledge&lt;/em&gt;, and how they related to perception (particularly for object perception). They first define several challenges/observations regarding perception:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The mapping from the world to our senses is often not invertible (e.g. recovering 3D shape from 2D data)&lt;/li&gt;
  &lt;li&gt;Many sensory cues are not actually measurements of the relevant property we’re interested in, but provide only “auxiliary” information&lt;/li&gt;
  &lt;li&gt;Sensory cues vary in quality relative to each other, depending on external and internal factors (e.g. fog, cataracts), and as a function of the world state.&lt;/li&gt;
  &lt;li&gt;Objects’ spatial and material properties follow highly predictable patterns&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Next, they define the &lt;em&gt;sensory generative process&lt;/em&gt; to be the true process in the world that generates our sensations. In contrast, &lt;em&gt;sensory generative knowledge&lt;/em&gt; is people’s assumptions about how the sensory generative process works. In some cases, the generative process and generative knowledge may be the same, though not necessarily. If generative knowledge is at least a good approximation to the generative process, though, it can provide important information about how to interpret our sensations (addressing the challenges listed previously). Battaglia et al. make the distinction between being &lt;em&gt;subjectively optimal&lt;/em&gt; (in which people make optimal use of their generative knowledge, but the knowledge does not match the generative process), &lt;em&gt;objectively optimal&lt;/em&gt; (in which people make optimal use of their generative knowledge, which matches the generative process), and &lt;em&gt;suboptimal&lt;/em&gt; (in which people do not make optimal use of their generative knowledge).&lt;/p&gt;

&lt;p&gt;Battaglia et al. also describe how a Bayesian observer model can account for several basic phenomena in perception: performing basic Bayesian inference, combining multiple cues, discounting nuisance information based on prior knowledge, and explaining away nuisance information based on auxiliary cues. In particular, discounting and explaining away rely heavily on generative knowledge, while basic Bayes and cue combination could theoretically be learned just via a discriminative mapping.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This chapter makes two important points:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Our internal models aren’t necessarily accurate imitations of the true generative process, but that doesn’t mean that people don’t make optimal use of the knowledge they have. Hence, when constructing models of cognition, it is important to be explicit about what the generative process is in the world that is generating people’s observations, and what generative knowledge we as scientists think people actually have.&lt;/li&gt;
  &lt;li&gt;Having structured generative knowledge is important, because it makes it easier to interpret ambiguous sensations into reliable perceptions. For simpler things, such as cue combination, we don’t necessarily need to have a full generative model since a simple discriminative model can often be sufficient. But, for reasoning about more complex systems, and to be able to explain phenomena like discounting or explaining away, generative knowledge is crucially important.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 14:18:13 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/02/Battaglia2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/02/Battaglia2012.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>The Helmholtz machine</title>
        <description>&lt;p&gt;&lt;span id=&quot;Dayan1995&quot;&gt;Dayan, P., Hinton, G. E., Neal, R. M., &amp;amp; Zemel, R. S. (1995). The Helmholtz machine. &lt;i&gt;Neural Computation&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;(5), 889–904. doi:10.1162/neco.1995.7.5.889&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Dayan et al. propose a method for learning about the underlying structure in data using self-supervised learning in a neural network. Specifically, they construct the network to have bottom-up &lt;em&gt;recognition&lt;/em&gt; weights and top-down &lt;em&gt;generative&lt;/em&gt; weights. The network is then trained according to a &lt;em&gt;wake-sleep algorithm&lt;/em&gt;, where the generative weights are trained during the “wake” phase and the recognition weights are trained during the “sleep” phase by simulating training examples from the generative model.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;The recognition probability of unit $j$ in layer $\ell$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_j^\ell(\phi,\mathbf{s}^{\ell-1})=\sigma\left(\sum_i s_i^{\ell-1}\phi_{i,j}^{\ell-1,\ell}\right)&lt;/script&gt;

&lt;p&gt;where $\sigma$ is the sigmoid function and $\phi$ are the recognition weights. As mentioned earlier, the recognition weights are trained during the sleep phase by simulating training data from the generative model.&lt;/p&gt;

&lt;p&gt;The generative probability of unit $j$ in layer $\ell$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_j^\ell(\theta,\mathbf{s}^{\ell+1})=\sigma\left(\sum_i s_i^{\ell+1}\theta_{i,j}^{\ell+1,\ell}\right)&lt;/script&gt;

&lt;p&gt;where $s$ are the unit activities. The generative model is trained by presenting data to input units and then activating units according to $q_j^\ell$. Then, the generative weights are updated to minimized the KL-divergence between the actual activations and the generative probabilities $p_j^\ell$.&lt;/p&gt;

&lt;p&gt;The wake and sleep phases are computed iteratively and over time the weights should converge such that $Q(d)=P(\theta,d)$.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The Helmholtz machine is a very cool idea, in that it is an unsupervised way of (approximately) learning highly complex structures by jointly training a recognition and generative model. This can be thought of as a way of implementing the EM algorithm in a neural network; it can also be interpreted as a particular type of autoencoder (e.g. if it is a one-layer Helmholtz machine, it is a folded-over autoencoder).&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 10:25:21 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/02/Dayan1995.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/02/Dayan1995.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>Learning compound multi-step controllers under unknown dynamics</title>
        <description>&lt;p&gt;&lt;span id=&quot;Han2015&quot;&gt;Han, W., Levine, S., &amp;amp; Abbeel, P. (2015). Learning Compound Multi-Step Controllers under Unknown Dynamics. &lt;i&gt;Proceedings Of the 28th IEEE/RSJ International Conference on Intelligent Robots and Systems&lt;/i&gt;. Retrieved from http://rll.berkeley.edu/reset_controller/reset_controller.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Han et al. build on the work of &lt;a href=&quot;/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Levine2015.html&quot;&gt;Levine et al.&lt;/a&gt; to learn controllers for tasks involving multiple steps. This is a difficult problem for traditional approaches to control, which assume that tasks are stationary (i.e., that the initial conditions are the same), which isn’t usually the case for compound tasks because the initial state of each step depends on the end state of the previous step. To address this, Han et al. formulate a way to learn both forward and &lt;em&gt;reverse&lt;/em&gt; controllers simultaneously. Then, in the compound task, if a forward controller fails, the reverse controller can be used to reset the movement and try again. Training compound controllers in this way is much more effective than trying to learn one global controller.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Training the forward and reset controllers is actually pretty straightfoward. Han et al. run the forward controller and reset controllers in sequence $N$ times, and use these samples to update both controllers according to the task-specific loss function (for the forward controller) or a reset cost (for the reset controller).&lt;/p&gt;

&lt;p&gt;Training the compound controller follows largely the same idea. Each forward controller is executed in sequence. If a controller fails, then the corresponding reverse controller is used to undo the action, and the forward controller is run again. This is repeated until the controller succeeds. After $N$ samples are collected for a controller, it is refit according to those samples. These $N$ samples count as one “iteration”, and all controllers are trained for $K$ iterations.&lt;/p&gt;

&lt;p&gt;As in the previous work, the individual linear-Gaussian controllers can also be used to train a more general parameterized policy (such as a neural network). The benefit of training both forward and reset controllers, though, means that the system can autonomously train the neural network by running the forward controllers, using those samples as training data for the network, and resetting automatically using the reset controller.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This way of training compound controllers is exciting, because it could potentially be integrated with higher-level planning algorithms to determine how to accomplish more sophisticated and complex tasks. This brings up an interesting distinction in the idea of simulation—that you can potentially have simulations at different levels of abstraction, as well as at different levels of granularity. In this case, the robot can learn a dynamics model for multiple motion primitives (the individual linear-Gaussian controllers) but it might also need to be able to learn a higher-level (perhaps more qualitative) form of simulation in order to reason about how to accomplish the task in the first place. In the case of screwing in a bolt, the robot might need high-level qualitative knowledge about how the task works (first need to pick up the wrench, then bring it to the bolt, then turn it because bolts need to be turned to go further into the hole, then repeat, etc.), but as shown by this work, it also needs low-level knowledge about the dynamics of the task in order to actually execute the subparts of the action.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 08:22:37 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Han2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Han2015.html</guid>
        
        
        <category>Planning under uncertain dynamics</category>
        
      </item>
    
      <item>
        <title>Learning contact-rich manipulation skills with guided policy search</title>
        <description>&lt;p&gt;&lt;span id=&quot;Levine2015&quot;&gt;Levine, S., Wagener, N., &amp;amp; Abbeel, P. (2015). Learning Contact-Rich Manipulation Skills with Guided Policy Search. &lt;i&gt;Proceedings Of the IEEE International Conference on Robotics and Automation&lt;/i&gt;. Retrieved from http://arxiv.org/abs/1501.05611v1&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper builds on previous work (Levine &amp;amp; Abbeel, NIPS 2013) that learns policies for object manipulation using a two step process. The first step is to learn local linear-Gaussian controllers for a few specific training examples. Then, the linear-Gaussian controllers are used to train parameters for a more complex policy (e.g., using a neural network) using a method called &lt;em&gt;guided policy search&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;linear-gaussian-controllers&quot;&gt;Linear-Gaussian controllers&lt;/h2&gt;

&lt;p&gt;First, they assume linear-Gaussian dynamics, i.e.:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{x}_{t+1}\vert \mathbf{x}_t,\mathbf{u}_t)=\mathcal{N}(f_{\mathbf{x}t}\mathbf{x}_t+f_{\mathbf{u}t}\mathbf{u}_t, \mathbf{F}_t)&lt;/script&gt;

&lt;p&gt;They can fit these dynamics using samples collected under the previous version of the controller, and from there compute the linear-Gaussian controller:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{u}_t\vert \mathbf{x}_t)=\mathcal{N}(\hat{\mathbf{u}}_t+\mathbf{k}_t+\mathbf{K}_t(\mathbf{x}_t-\hat{\mathbf{x}}_t),Q_{\mathbf{u}, \mathbf{u}t}^{-1})&lt;/script&gt;

&lt;p&gt;where $Q_{\mathbf{u}, \mathbf{u}t}$ is the Hessian of the $Q$-function, $\mathbf{k}_t=-Q_{\mathbf{u},\mathbf{u}t}^{-1}Q_{\mathbf{u}t}$ and $\mathbf{K}_t=-Q_{\mathbf{u},\mathbf{u}t}^{-1}Q_{\mathbf{u},\mathbf{x}t}$ and (I think?) where $\hat{\mathbf{u}}_t$ and $\hat{\mathbf{x}}_t$ are the mean state and control from the samples. The $Q$-function is computed using a dynamic programming algorithm, which I won’t go into the details of.&lt;/p&gt;

&lt;p&gt;The controller is then updated subject to the constraint that the KL-divergence between the trajectory distribution $p(\tau)=\prod_t p(\mathbf{x}_{t+1}\vert \mathbf{x}_t,\mathbf{u}_t)p(\mathbf{u}_t\vert \mathbf{x}_t)$ and the old trajectory distribution is not more than a threshold $\epsilon$. They solve this optimization using dual gradient descent, which I also won’t go into the details of here.&lt;/p&gt;

&lt;p&gt;Levine et al. minimize the number of samples needed by also estimating a Gaussain mixture model prior on the global dynamics, and adaptively adjust both the step size $\epsilon$ and sample count according to an estimate of the additional cost at each iteration due to unmodeled changes in the dynamics.&lt;/p&gt;

&lt;h2 id=&quot;guided-policy-search&quot;&gt;Guided policy search&lt;/h2&gt;

&lt;p&gt;Given the learned linear-Gaussian dynamics and controller, they can be used to train a more sophisticated policy with large numbers of parameters (e.g., a neural network). Rather than directly computing a policy, supervised learning is used to learn the policy using trajectories sampled from the linear-Gaussian controllers (this is kind of a form of learning-by-demonstration, I think, where the demonstrations are a combination of the actual trajectories run using the linear-Gaussian controller on the real robot, and synthesized trajectories from the linear-Gaussian controllers). The trajectories are then reoptimized as well to better match the state distribution of the policy, i.e.:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\theta,p(\tau)}E_{p(\tau)}[\ell(\tau)]\ \mathrm{s.t.}\ D_{\mathrm{KL}}(p(\mathbf{x}_t)\pi_\theta(\mathbf{u}_t\vert \mathbf{x}_t)\vert\vert p(\mathbf{x}_t, \mathbf{u}_t))=0\ \forall t&lt;/script&gt;

&lt;p&gt;where $p(\tau)$ is the trajectory distribution (obtained from the linear-Gaussian dynamics) and $\pi_\theta$ is the parameterized policy. They solve this using another (different) application of dual gradient descent.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a cool way to combine deep learning with a more structured modeling approach. Essentially, the linear-Gaussian controllers are learning a generative model of the dynamics for specific motions, from which an optimal policy can be extracted with relatively little computation. The generative model is then used to train the discriminative model (the neural network). It’s an interesting contrast, though, because I generally think of generative models as being &lt;em&gt;more&lt;/em&gt; general, whereas in this case it’s actually that there are multiple generative models, each of which is local and therefore relatively fragile. But by combining them together a more powerful and flexible discriminative model can be trained.&lt;/p&gt;

&lt;p&gt;Overall, it seems to me that this approach is quite similar similar to the approach taken by &lt;a href=&quot;/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Paraschos2015.html&quot;&gt;Paraschos et al.&lt;/a&gt;, at least in terms of using linear-Gaussian dynamics to learn local (or primitive) motion policies. This paper, though, goes the next step and shows how then those local motion policies can be combined in order to train a more sophisticated and general policy.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 04:56:38 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Levine2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Levine2015.html</guid>
        
        
        <category>Planning under uncertain dynamics</category>
        
      </item>
    
      <item>
        <title>The simulation heuristic</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kahneman1981&quot;&gt;Kahneman, D., &amp;amp; Tversky, A. (1981). &lt;i&gt;The simulation heuristic&lt;/i&gt;. Retrieved from http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA099504&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kahneman &amp;amp; Tversky discuss how people construct mental simulations of hypothetical scenarios, and in particular, counterfactual scenarios. They argue that people rely on a “simulation heuristic”, which is defined as constructing scenarios in order to estimate the probability of events.&lt;/p&gt;

&lt;p&gt;In the introduction, Kahneman &amp;amp; Tversky provide a nice definition of what they mean by simulation (pg. 1-2):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Our starting point is a common introspection: there appear to be many situations in which questions about events are answered by an operation that resembles the running of a simulation model. The simulation can be constrained and controlled in several ways: the starting conditions for a ‘run’ can be left at their realistic default value, or modified to assume some special contingency; the outcomes can be left unsecified, or else a target state may be set, with the task of finding a path to that state from the initial conditions. A simulation does not necessarily produce a single story, which starts at the beginning and ends with a definite outcome. Rather, we construe the output of simulation as an assessment of the ease with which the model could produce different outcomes, given its initial conditions and operating parameters. Thus, we suggest that mental simulation yields a measure of the propensity of one’s model of the situation to generate various outcomes, much as the propensities of a statistical model can be assessed by Monte Carlo techniques.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They also give a few use cases that seem to involve simulation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prediction&lt;/li&gt;
  &lt;li&gt;Assessing the probability of a specified event&lt;/li&gt;
  &lt;li&gt;Assessing conditioned probabilities&lt;/li&gt;
  &lt;li&gt;Counterfactual assessments&lt;/li&gt;
  &lt;li&gt;Assessments of causality&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In this paper, they focus primarily on counterfactual assessments (i.e., “if only…” scenarios). The first type of task that they focus on is that involving regret. The example scenario is that of two people who miss their flight, but by different margins (5 minutes vs. 30 minutes), and ask participants to ask who feels worse. Almost universally people respond that the person who missed their flight by 5 minutes feels worse than the other person. Kahneman and Tversky argue that this is because it is easier to imagine making it to the airport 5 minutes earlier than it is to make it to the airport 30 minutes earlier.&lt;/p&gt;

&lt;p&gt;The second type of task they focus on involves people actually producing alternate scenarios that could have prevented something happening (for example, someone dying in a car crash). There were two conditions; one in which the driver left work early (“time” version), and one in which they took an unusual route (“route” version). In the time version, participants came up with alterations to the scenario like “he should have left at a different time” (26%), “he should have crossed the intersection more quickly” (31%), or “the other driver shouldn’t have been driving” (29%). In the route version, the majority of participants said “he shouldn’t have taken the different route” (51%), with some also saying “he should have crossed the intersection more quickly” (22%) or “the other driver shouldn’t have been driving” (20%).&lt;/p&gt;

&lt;p&gt;The route version involves an unusal element (the route), and correspondingly, participants are vastly more likely to undo this element in their counterfactual simulation than the participants in the time version. Kahneman and Tversky explain this as one of three types of changes that can occur:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Downhill change (increases probability, decreases surprise)&lt;/li&gt;
  &lt;li&gt;Uphill change (decreases probability, increases surprise)&lt;/li&gt;
  &lt;li&gt;Horizontal change (arbitrary value is changed, no change in probability or surprise)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In particular, the change in the route version is an example of a downhill change; thus, Kahneman and Tversky argue that when constructing simulations, people are biased towards scenarios that are more probable and less surprising. Moreover, people are biased towards making alterations to the scenario that are related to the main object or character (e.g., the behavior of the original driver, and not the other driver).&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Kahneman and Tversky characterize the simulation heuristic as being biased towards downhill changes. I wonder if this could be explained by something like a stochastic search in the space of scenarios (e.g. something like Monte-Carlo tree search), where people are trying to maximize something like the posterior probability of the scenario given the alternate outcome. Due to the prior, this would result in a bias towards less surprising scenarios (e.g. undoing the change in route). This bias isn’t necessarily a bad thing, though. The prior is a well-motivated component, because you need something to constrain the space of possible scenarios—the narrative structure of the situation still needs to be coherent, and the structure of the prior ensures that.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 02:21:27 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/memory%20and%20imagination/2016/01/02/Kahneman1981.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/memory%20and%20imagination/2016/01/02/Kahneman1981.html</guid>
        
        
        <category>Memory and imagination</category>
        
      </item>
    
  </channel>
</rss>
