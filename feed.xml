<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 11 Jan 2016 16:54:34 -0800</pubDate>
    <lastBuildDate>Mon, 11 Jan 2016 16:54:34 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Optimal predictions in everyday cognition: the wisdom of individuals or crowds?</title>
        <description>&lt;p&gt;&lt;span id=&quot;Mozer2008&quot;&gt;Mozer, M. C., Pashler, H., &amp;amp; Homaei, H. (2008). Optimal predictions in everyday cognition: the wisdom of individuals or crowds? &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;32&lt;/i&gt;(7), 1133–1147. Retrieved from http://onlinelibrary.wiley.com/doi/10.1080/03640210802353016/abstract&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Mozer et al. question whether the conclusion of &lt;a href=&quot;/quals/probabilistic%20models%20of%20cognition/2015/11/08/Griffiths2006.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt;—that people have accurate knowledge of full prior distributions—is justified. In particular, they argue that the same results can be obtained with a model that only uses a few ($k$) samples from the prior. They show that a simple heuristic model (termed Min$k$) actually outperforms the full Bayesian model, and also matches the inter-participant variance better than the Bayesian model.&lt;/p&gt;

&lt;p&gt;Mozer et al. suggest that their Min$k$ model is perhaps more like an algorithmic-level theory, while the fully Bayesian model is a computational-level theory. However, they caution that by focusing purely on the computational level, important factors such as variability and processing time might be missed.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Mozer et al. suggest a few different models. First, the model from Griffiths &amp;amp; Tenenbaum (the &lt;em&gt;G&amp;amp;T-Bayesian&lt;/em&gt; model) predicts the median of the posterior:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(t_{\mathrm{total}}\vert t_\mathrm{cur})\propto p(t_\mathrm{cur}\vert t_\mathrm{total})p(t_\mathrm{total})&lt;/script&gt;

&lt;p&gt;The &lt;em&gt;Min$k$&lt;/em&gt; model retrieves $k$ samples from memory and predicts the minimum sample that is greater than $t_\mathrm{cur}$. If no samples are greater than $t_\mathrm{cur}$, then the model predicts $(1+g)t_\mathrm{cur}$, where $g$ is guess proportion of the query.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;GT$k$Guess&lt;/em&gt; model is similar to Min$k$, except that instead of returning the minimum, it returns the median of the posterior computed with an empirical prior estimated via $k$ samples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(t_\mathrm{total})=\frac{1}{k}\sum_{i=1}^k \delta_{s_i,t_\mathrm{total}}&lt;/script&gt;

&lt;p&gt;If the prior estimate is less than $t_\mathrm{total}$, then the GT$k$Guess model performs the same guess with $g$ as the Min$k$ model.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;GT$k$Smooth&lt;/em&gt; model doesn’t guess, but instead uses something like a kernel density estimate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(t_\mathrm{total})\propto \sum_{i=1}^k \exp\left(\frac{(t_\mathrm{total}-s_i)^2}{2\sigma^2}\right)&lt;/script&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think this is actually a really important point—average participant judgments can’t necessarily be used to say something about the knowledge of individuals. I do think, though, that starting at the computational level and working down can still be revealing. In this case, the computational level analysis reveals that at least in &lt;em&gt;aggregate&lt;/em&gt; people’s judgments reflect true statistical regularities in the world. It could have been the other way around, and a computational level analysis allows us to first look at the high-level picture. Given that, we can then ask questions like the one that Mozer et al. ask here: what is actually the reason why people’s judgments reflect regularities? Is it that each participant has accurate knowledge of the prior, or that knowledge across people is distributed according to the prior? Mozer et al. show that it is the latter.&lt;/p&gt;

&lt;p&gt;I would be interested to see how well an analysis based on mechanisms for sampling would do. For example, would a model that uses rejection sampling also give a consistent algorithmic-level account? I would hypothesize that it would give the same results. You could also measure response time to see if people take longer to respond in cases where they are unlikely to get samples from the prior that are greater than $t_\mathrm{total}$—i.e., they have to make more rejections, and so it takes them longer to respond.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 08:27:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Mozer2008.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Mozer2008.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Judgment under uncertainty: heuristics and biases</title>
        <description>&lt;p&gt;&lt;span id=&quot;Tversky1974&quot;&gt;Tversky, A., &amp;amp; Kahneman, D. (1974). Judgment under Uncertainty: Heuristics and Biases. &lt;i&gt;Science&lt;/i&gt;, &lt;i&gt;185&lt;/i&gt;(4157), 1124–1131. doi:10.1126/science.185.4157.1124&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Tversky &amp;amp; Kahneman give an overview of three commonly used heuristics that lead to systematic biases: representativeness, availability, and anchoring.&lt;/p&gt;

&lt;h2 id=&quot;representativeness&quot;&gt;Representativeness&lt;/h2&gt;

&lt;p&gt;The representativeness heuristic was discussed in &lt;a href=&quot;/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html&quot;&gt;Kahneman &amp;amp; Tversky&lt;/a&gt;, so I won’t go into too much detail about it here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insensitivity to prior probabilities&lt;/li&gt;
  &lt;li&gt;Insensitivity to sample size&lt;/li&gt;
  &lt;li&gt;Misconceptions of chance — e.g. HTHTTH is more representative than HHHTTT&lt;/li&gt;
  &lt;li&gt;Insensitivity to predictability&lt;/li&gt;
  &lt;li&gt;The illusion of validity — confidence is a function based more on degree of representativeness rather than accuracy&lt;/li&gt;
  &lt;li&gt;Misconceptions of regression&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;availability&quot;&gt;Availability&lt;/h2&gt;

&lt;p&gt;The availability heuristic states that people will estimate things like probabilities by seeing how many items from the distribution come to mind. Thus, things that come to mind more easily will be estimated to have higher probability, even if they don’t actually have higher probability.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Biases due to the retrievability of instances — e.g. things like fame or salience will make things more available, and thus be considered more numerous&lt;/li&gt;
  &lt;li&gt;Biases due to the effectiveness of a search set — e.g. judging whether more words start with ‘r’ or have ‘r’ as the third letter&lt;/li&gt;
  &lt;li&gt;Biases of imaginability — e.g. intuitively computing “10 choose $k$” for $2\leq k\leq 8$. People are better able to imagine disjoint sets for $k=2$ than $k=8$, so they say that “10 choose 2” is higher than “10 choose 8”, even though they are the same&lt;/li&gt;
  &lt;li&gt;Illusory correlations — overestimating the frequency of co-occurring events (e.g. believing that patients who exhibit suspiciousness draw characters with shifty eyes, because “suspiciousness” and “eyes” have a strong association, even if they might not predict anything true about the diagnosis)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;anchoring-and-adjustment&quot;&gt;Anchoring and Adjustment&lt;/h2&gt;

&lt;p&gt;The anchoring heuristic states that people estimate values by starting at one value (the &lt;em&gt;anchor&lt;/em&gt;) and adjusting either up or down. Usually, people fail to fully adjust, and thus their estimates are biased towards the anchor.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insufficient adjustments — e.g. in multiplying $8\times 7\times 6\times 5\times 4\times 3\times 2\times 1$ vs. multiplying $1\times 2\times 3\times 4\times 5\times 6\times 7\times 8$, people will report higher estimates for the first product and lower estimates for the second product&lt;/li&gt;
  &lt;li&gt;Biases in the evaluation of conjunctive and disjunctive events — e.g. overestimation that things with high probability will co-occur, underestimation that at least one thing with low probability will occur. The anchors in this case are the probabilities which are then insufficiently adjusted, leading to overestimates of high probability things and underestimates of low probability things.&lt;/li&gt;
  &lt;li&gt;Anchoring in the assessment of subjective probability distributions — e.g. starting at current (mean) estimate and adjusting upward to get to the 90th percentile&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Each of these heuristics is quite telling about the way that people think. Although they’ve been taken in the past that people are just “irrational”, I think they’re more interesting when thought about in the context of how people are actually processing information. The case is clearer in cases like the availability heuristic, for example: it seems plausible that whatever algorithm is used for retrieving things from memory will bias us when we try to objectively estimate the probabilities of things. Generally, the point of memory seems to be to provide us with information that is going to be &lt;em&gt;relevant&lt;/em&gt;, which means there will be an effect of both frequency and utility—the combination of which would manifest itself as “availability”.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 07:27:51 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Tversky1974.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Tversky1974.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>On the psychology of prediction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kahneman1973&quot;&gt;Kahneman, D., &amp;amp; Tversky, A. (1973). On the psychology of prediction. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;80&lt;/i&gt;(4), 237–251. doi:10.1037/h0034747&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kahneman &amp;amp; Tversky discuss the &lt;em&gt;representativeness&lt;/em&gt; heuristic, in which people seem to evaluate evidence based on how representative it is of an outcome rather than based on the posterior probability of the outcome given the evidence. The present results from several experiments illustrating this effect, and demonstrating its robustness to changes in wording, reliability of evidence, and prior probability.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In the first experiment, participants were given vignettes such as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tom W. is of high intelligence, although lacking in true creativity. He has a need for order and clarity, and for neat and tidy systems in which every detail finds its appropriate place. His writing is rather dull and mechanical, occasionally enlivened by somewhat corny puns and by flashes of imagination of the sci-fi type. He has a strong drive for competence. He seems to have little feel and little sympathy for other people and does not enjoy interacting with others. Self-centered, he nonetheless has a deep moral sense.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Participants in one group rated the base rates of graduate student disciplines (and did not see the vignette). In another group, participants rated how similar the vignette description was to the average graduate student in each discipline. In the third group, participants rated the likelihood that Tom W. was a graduate student in each discipline. Overwhelmingly, participants in the third group ignored the base rate evidence and gave responses similar to the second group. In a follow-up experiment, they had two groups which were told the reliability of predictions of “students like themselves”. This reliability did have an effect on their likelihood ratings, though it did not bring the ratings any close to the base rate. In contrast, if particpants were given no information at all, they reverted to using the base rate.&lt;/p&gt;

&lt;p&gt;In another experiment, they focused on manipulating the base rate: for example, giving descriptions and asking how likely the description was of an engineer or a lawyer, where the number of engineers and lawyers was manipulated. However, changing this proportion did not seem to have an effect: people still seemed to report what was essentially the likelihood, rather than the posterior. Interestingly, when given &lt;em&gt;worthless&lt;/em&gt; information (as opposed to &lt;em&gt;no&lt;/em&gt; information), people judged the probability at 50%, rather than reverting the the prior. (Side note: if you gave people worthless information and they had three categories to choose from, would they give the probability as 50% still or 33%?)&lt;/p&gt;

&lt;p&gt;Kahneman &amp;amp; Tversky also looked at “prediction of outcome” (e.g. predicting GPA from a description) versus “evaluation of inputs” (e.g. rating impressions from a description) versus “translation” (e.g. translating an input variable on one scale to another scale). According to the representativeness heuristic, predictions and evaluations should be essentially the same; according to statistics, however, the predictions are more unreliable and thus should exhibit regression to the mean. Similarly, the representativeness heuristic predicts that predictions are no more regressive than translations, even though statistics predicts otherwise.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The results presented by Kahneman &amp;amp; Tversky are incredibly compelling: despite knowing what the correct calculation to do is, it &lt;em&gt;still&lt;/em&gt; feels more “natural” to respond in the manner that they equate with representativeness. &lt;a href=&quot;http://web.mit.edu/cocosci/Papers/cogsci01_final.pdf&quot;&gt;Tenenbaum &amp;amp; Griffiths&lt;/a&gt; (2001) cast representativeness in a different light from the notion of “similarity” discussed by Kahneman &amp;amp; Tversky, defining representativeness as being a measure of how good of an example something is. Formally, they define it as the log ratio of the likelihood of the current hypothesis to all other hypotheses:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(d, h_i)=\log\frac{P(d\vert h_i)}{\sum_{h_j\neq h_i} P(d\vert h_j)P(h_j\vert \bar{h}_i)}&lt;/script&gt;

&lt;p&gt;where $\mathcal{H}$ is the set of all hypotheses under consideration and $P(h_j\vert \bar{h}_i)$ is the prior probability of $h_j$ given that $h_i$ is not the true explanation of $d$ (i.e. $P(h_j)/(1-P(h_i))$). I don’t know if this particular model has been applied specifically to the Kahneman &amp;amp; Tversky experiments here, though. I think you probably couldn’t apply them directly because the report similarity ranks rather than real likelihood scores.&lt;/p&gt;

&lt;p&gt;I do find it interesting that people are interpreting the phrasing of the problem to be asking for something along the lines of “representativeness” rather than “posterior probability”. Even if you can quantify what “representativeness” means, it doesn’t explain &lt;em&gt;why&lt;/em&gt; people interpret the question that way (and not the way it is meant to be interpreted).&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 05:39:59 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Theory learning as a stochastic search in a language of thought</title>
        <description>&lt;p&gt;&lt;span id=&quot;Ullman2012&quot;&gt;Ullman, T. D., Goodman, N. D., &amp;amp; Tenenbaum, J. B. (2012). Theory learning as stochastic search in the language of thought. &lt;i&gt;Cognitive Development&lt;/i&gt;, &lt;i&gt;27&lt;/i&gt;(4). doi:10.1016/j.cogdev.2012.07.005&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Ullman et al. suggest an algorithm-level approach for theory learning. In contrast to &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Griffiths2009.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt; and &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Kemp2010.html&quot;&gt;Kemp et al.&lt;/a&gt;, which proposed computational-level models for theory learning, Ullman et al. build on previous work (i.e., they still use a hierarchical Bayesian model) and demonstrate that MCMC-like search procedure can account for similar learning dynamics as are found in children.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Ullman et al. express theories using Horn clauses, which are logical expressions of the form $r\leftarrow (f\wedge g\wedge\ldots{}\wedge s\wedge t)$ where each term is a predicate such as $f(X)$ or $s(X, Y)$, where $X$ and $Y$ are entities in the domain.&lt;/p&gt;

&lt;p&gt;A theory $T$ is produced from a &lt;em&gt;univeresal theory grammar&lt;/em&gt; $U$, which is a probabilistic context-free Horn clause grammar (PHCG). This grammar includes what they call &lt;em&gt;law templates&lt;/em&gt; which encode assumptions for what types of laws are good. For example, one template is $F(X,Y)\leftarrow F(X,Z)\wedge F(Z,Y)$ which captures transitivity. The $F$ could then be replaced with something like $\mathrm{interacts}(\cdot{})$.&lt;/p&gt;

&lt;p&gt;Together, the grammar and the templates specify the prior $p(T\vert U)$ over theories. A particular theory—for example, in the magnetism domain—might have core predicates $f(X)$ and $g(X)$, surface predicates of $interacts(X, Y)$, and laws such as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathrm{interacts}(X, Y)&amp;\leftarrow f(X)\wedge f(Y)\\
\mathrm{interacts}(X, Y)&amp;\leftarrow f(X)\wedge g(Y)\\
\mathrm{interacts}(X, Y)&amp;\leftarrow \mathrm{interacts}(Y, X)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Given a theory, a model $M$ can be produced which fits the data. For example, in the magnetism domain, the model might assign magnets to $f(X)$ and magnetic objects to $g(X)$ (and everything else unassigned). The prior on models $p(M\vert T)$ is a beta prior (per binary fact).&lt;/p&gt;

&lt;p&gt;Finally, the data $D$ are actual observations, for example, “this object interacts with this other object”. The likelihood $p(D\vert M, T)$ is Bernoulli.&lt;/p&gt;

&lt;p&gt;Thus the full model is something like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
T\vert U &amp;\sim \mathrm{PHCG}(U)\\
M^i &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
D^{ij} &amp;\sim \mathrm{Bernoulli}(M^i)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the posterior is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(T\vert D, U)\propto P(T\vert U)\sum_M p(D\vert M, T)p(M\vert T)&lt;/script&gt;

&lt;p&gt;To approximate the sum, they use Gibbs sampling. To search in the space of theories, they use a modified version of MCMC where the acceptance ratio is annealed, so that changes are less likely to be made as time goes on. They propose new theories at each step of the chain with the following possible modifications:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add or delete a predicate from a law&lt;/li&gt;
  &lt;li&gt;Change one predicate to an alternative of the same or different form within a law&lt;/li&gt;
  &lt;li&gt;Add or delete a whole law&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This approach of performing a stochastic search in theory space seems generally plausible as a mechanism for how children learn theories. However, I think there are a few things here that aren’t fully satisfing:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The MCMC chain takes many iterations to run: for example, 1600 in the magnetism case. I realize you can’t exactly map inference (“thinking”) time directly to MCMC steps, but it seems like a lot. If this is a mechanistic account, then it’s essentially saying that children propose 1600 different theories before they reach the final one.&lt;/li&gt;
  &lt;li&gt;Children presumably don’t do inference over the entire dataset at once; I would expect there to be some sort of online learning going on. Ullman et al. do investigate the effect of varying amounts of data on the learning process, but they use separate datasets rather than looking at an online method.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Related to the last point, I suppose there are two options for what people do: either they use some memoryless inference procedure, or they do inference over a set of data, but I would expect that to not include &lt;em&gt;everything&lt;/em&gt; they’ve ever encountered. There will be some effect of memory (perhaps remembering data points that were surprising, and more recent data points).&lt;/p&gt;

&lt;p&gt;I wonder if, rather than using a random MCMC algorithm, an active search or Bayesian optimization type of algorithm might work better here in terms of sampling efficiency.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 03:10:05 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/11/Ullman2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/11/Ullman2012.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>A probabilistic model of theory formation</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kemp2010&quot;&gt;Kemp, C., Tenenbaum, J. B., Niyogi, S., &amp;amp; Griffiths, T. L. (2010). A probabilistic model of theory formation. &lt;i&gt;Cognition&lt;/i&gt;, &lt;i&gt;114&lt;/i&gt;(2), 165–196. doi:10.1016/j.cognition.2009.09.003&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kemp et al. describe a method for clustering entities based on  relations, rather than features. They argue that this clustering algorithm can be thought of as a model of theory formation in people, reflecting how people learn and organize concepts. In particular, they focus on the idea of &lt;em&gt;framework theories&lt;/em&gt;, which “specify the fundamental concepts that exist in a domain and the possible relationships between these concepts” (pg. 166). They ask three fundamental questions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;What are theories?&lt;/strong&gt; Framework theories are “represented as a probabilistic model which includes a set of categories and a matrix of parameters specifying relationships between those categories” (pg. 166)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How are theories used to make inductive inferences?&lt;/strong&gt; “Each of our theories specifies the relationships between categories that are possible or likely, and predictions about unobserved relationships between entities are guided by inductive inferences about their category assignments” (pg. 166)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How are theories acquired?&lt;/strong&gt; Kemp et al. consider the case of acquiring both the concepts and causal laws of a theory simultaneously. “Given a formal characterization of a theory, we can set up a space of possible theories and define a prior distribution over this space” (pg. 167)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They apply their model (described below) to several different real-world datasets: clustering animals and features, clustering medical terms and predicates, and recovering kinship categories. They also run two experiments in which they show how their model matches the way that people learn causal theories of physical relationships.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, Kemp et al. had people play with a set of blocks. Each block either was a member of category $A$ or category $B$, and depending on the condition, the relationship between $A$ and $B$ was either: that $A$ blocks caused $B$ blocks to light up; that $A$ blocks caused $B$ blocks to light up and vice versa; that $A$ blocks caused $B$ blocks to light up with $p=0.5$; and that $A$ blocks caused $B$ blocks to light up with $p=0.5$ and vice versa. Note that &lt;em&gt;specific&lt;/em&gt; blocks were deterministic, $p$ was used to determine which pairs of blocks would affect each other. Participants got experience playing with the blocks and were periodically asked to predict what would happen if a new block was touched to an old block. They then saw the new block touch a different old block, and were asked the same question.&lt;/p&gt;

&lt;p&gt;Experiment 2 was similar to Experiment 1, except that only the relationship where $A$ blocks caused $B$ blocks to light up and vice versa was used. After getting experience with the blocks, participants were shown three new blocks, $x$, $y$, and $z$, where either $x$ and $z$ were in the same category and $y$ was in a different category, or where $y$ and $z$ were in the same category and $x$ was in a different category. By seeing that $x$ interacts with $y$ and that $z$ interacts with $y$, participants should (and do) infer that $x$ and $z$ are in the same category—but they shouldn’t know which one. After seeing $z$ interact (or not interact) with an old block, participants should (and do) infer which categories $x$ and $y$ are in.&lt;/p&gt;

&lt;p&gt;The model matches people’s behavior in both of these experiments. In contrast, particularly in Experiment 2, a purely feature-based categorization model (which doesn’t take into account relations) does not appropriately generalize.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Let the observed data include $m$ relations ($R$) over $n$ types. Then, a relational system is characterized by a pair $(z, \eta)$, where $z$ is a partition of entities into categories and $\eta$ is a matrix of parameters specifying how the categories interact with each other for a relation $R$ (a.k.a. a &lt;em&gt;category graph&lt;/em&gt; where the edge from $A$ to $B$ has weight $\eta(A,B)$). Then, the theory that best characterizes the data is given by the MAP estimate of:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z,\eta\vert R)\propto p(R\vert \eta,z)p(\eta\vert z)p(z)&lt;/script&gt;

&lt;p&gt;where $p(R\vert \eta,z)$ is the probability of observing the relations given categories and parameters, $p(\eta\vert z)$ is the probability of the parameters given the partition, and $p(z)$ is the prior over category assignments. More formally:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z\vert\gamma &amp;\sim \mathrm{CRP}(\gamma)\\
\eta(A, B)\vert\alpha,\beta &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
R(i, j)\vert z, \eta &amp;\sim \mathrm{Bernoulli}(\eta(z_i, z_j))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In this case, there is one relation and one type (though multiple entities within that type); however, in the general case, there are multiple relations with $m$ dimensions and $n$ types. If $T^i$ is the $i$th type, then let $z^i$ be the partition into category assignments for $T^i$. If $R^j$ is the $j$th relation, then let $\eta^j$ be a separate parameter matrix for each $R^j:T^{d_1}\times T^{d_2}\times \ldots{}\times T^{d_m}$, where $d_k$ is the type corresponding to the $k$th dimension. The full general specification is then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z^i\vert\gamma &amp;\sim \mathrm{CRP}(\gamma)\\
\eta^j(z_{x_1}^{d_1},z_{x_2}^{d_2}\ldots{},z_{x_m}^{d_m})\vert\alpha,\beta &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
R^j(x_1,x_2,\ldots{},x_m)\vert z^1,\ldots{},z^n, \eta^j &amp;\sim \mathrm{Bernoulli}(\eta^j(z_{x_1}^{d_1},z_{x_2}^{d_2}\ldots{},z_{x_m}^{d_m}))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;To fit the model, they first integrate out $\eta$ and find the best category assignments $z$ using a hill-climbing procedure or MCMC. Given $z$, the MAP parameter assignments $\eta$ can be computed analytically. Similarly, unobserved relations between specific entities can be predicted given $z$.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This framework seems to be a generalization of the one described by &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Griffiths2009.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt; — for example, Kemp et al.’s experiments here are very similar to the “blicket detector” paradigm. However, Griffiths &amp;amp; Tenenbaum only consider there to be blickets and non-blickets. In this case, the number of types is left unspecified. Actually, it seems not so much that this is a generalization of Griffiths &amp;amp; Tenenbaum, but that it’s a more abstract sort of learning. This clusters entities and relations into an ontology that could be used by the framework described by Griffiths &amp;amp; Tenenbaum, but it’s not necessarily a &lt;em&gt;causal&lt;/em&gt; ontology.&lt;/p&gt;

&lt;p&gt;I am less clear on exactly how this type of framework for theories relates to that discussed in the other paper by &lt;a href=&quot;/quals/theory%20learning/2016/01/09/Kemp2007.html&quot;&gt;Kemp et al.&lt;/a&gt;. In that paper, they discuss how to parse objects into different ontological kinds, which is essentially a type of feature-based clustering. I suppose that model is something that is meant to be applied a more domain-specific level—e.g. perhaps within a single category of the kind that is discovered by the present paper. It would be nice to see a tighter coupling between these different types of ontologies and ways of learning theories.&lt;/p&gt;

</description>
        <pubDate>Sun, 10 Jan 2016 11:32:56 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Kemp2010.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Kemp2010.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>Theory-based causal induction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Griffiths2009&quot;&gt;Griffiths, T. L., &amp;amp; Tenenbaum, J. B. (2009). Theory-based causal induction. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;116&lt;/i&gt;(4), 661–716. doi:10.1037/a0017201&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Griffiths &amp;amp; Tenenbaum outline a probabilistic framework for causal induction that allows inference to be done over variables, graphs, and theories. The framework has three main components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ontology of entities, properties, and relations (i.e., “how entities are differentiated on the basis of their causal properties”, pg. 663)&lt;/li&gt;
  &lt;li&gt;Plausible relations between entities (i.e., what types of relationships should be considered in a given domain)&lt;/li&gt;
  &lt;li&gt;Functional forms of relations (i.e., the direction in which causes act, and how causes combine)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These three components are implemented in a probabilistic graphical model as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ontology = variables&lt;/li&gt;
  &lt;li&gt;Plausible relations = graph (the &lt;em&gt;structure&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;Functional form = probability distribution (the &lt;em&gt;parameterization&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Given an existing ontology, learning the plausible relations is a type of &lt;em&gt;structure learning&lt;/em&gt;, and learning the functional form is a type of &lt;em&gt;parameter estimation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Griffiths &amp;amp; Tenenbaum note, however, that knowledge of ontologies, plausible relations, and functional forms itself is not something that can be expressed through the framework of graphical models:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Knowledge about the ontology, plausibility, and functional form of causal relationships should influence the prior, likelihood, and hypothesis space for Bayesian inference. However, expressing this knowledge requires going beyond the representational capacities of causal graphical models. Although this knowledge can be &lt;em&gt;instantiated&lt;/em&gt; in a causal graphical model, it generalizes over a set of such models, and thus cannot be &lt;em&gt;expressed&lt;/em&gt; in any one model. (pg. 669)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They argue that this type of knowledge is akin to the notion of an &lt;em&gt;intuitive theory&lt;/em&gt;, and draw the relationship between theories and grammars; causal structures and syntactic structures; and data and sentences. To formalize theories in their framework, they make use of &lt;em&gt;hierarchical Bayesian models&lt;/em&gt; which allow inference to be performed both at the level of the theory as well as the lower levels of causal structures and relationships.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;types-of-functional-forms&quot;&gt;Types of functional forms&lt;/h2&gt;

&lt;h3 id=&quot;noisy-or&quot;&gt;Noisy-OR&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(e^+\vert c; w_0, w_1)=1-(1-w_0)(1-w_1)^c&lt;/script&gt;

&lt;p&gt;where $e^+$ is the presence of the effect, $c$ is the presence/absence of the cause $w_0$ is the probability of the effect in the absence of the cause, and $w_1$ is the probability of the effect given a single cause.&lt;/p&gt;

&lt;h3 id=&quot;noisy-and-not&quot;&gt;Noisy-AND-NOT&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(e^+\vert c; w_0, w_1)=w_0(1-w_1)^c&lt;/script&gt;

&lt;h3 id=&quot;generic&quot;&gt;Generic&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(e^+\vert c^-)&amp;=w_0\\
p(e^+\vert c^+)&amp;=w_1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;continuous-noisy-or&quot;&gt;Continuous Noisy-OR&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t)=\sum_i w_i\delta(t, t_i)&lt;/script&gt;

&lt;p&gt;where $w_i$ and $t_i$ are the weight and time associated with the $i$th cause.&lt;/p&gt;

&lt;h2 id=&quot;medical-contingency-data&quot;&gt;Medical contingency data&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for contingency data.&lt;/p&gt;

&lt;h3 id=&quot;ontology&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Chemical}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Gene}$ ($N_G\sim P_G$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Mouse}$ ($N_M\sim P_M$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(\mathrm{Chemical}, \mathrm{Mouse}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Expressed}(\mathrm{Gene}, \mathrm{Mouse}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(C, M) \rightarrow \mathrm{Expressed}(G, M)$, true for all $M$ with probability $p$ for each $C$, $G$ pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(C, M) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Expressed}(G, M) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, noisy-AND-NOT, or generic functional form, where $w_0$ and $w_1$ are drawn from uniform distributions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;blicket-detection&quot;&gt;Blicket detection&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for small amounts of data.&lt;/p&gt;

&lt;h3 id=&quot;ontology-1&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Block}$ ($N_B\sim P_B$)
    &lt;ul&gt;
      &lt;li&gt;$\mathrm{Blicket}$ ($p$)&lt;/li&gt;
      &lt;li&gt;$\mathrm{NonBlicket}$ ($1-p$)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\mathrm{Detector}$ ($N_D\sim P_D$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(\mathrm{Block}, \mathrm{Detector}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{Detector}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-1&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(B, D, T) \rightarrow \mathrm{Active}(D, T)$ for all $T$ for any $D$ if $B$ is a $\mathrm{Blicket}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-1&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(B, D, T) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(D, T) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, where $w_0=\epsilon$ and $w_1=1-\epsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the &lt;em&gt;deterministic detector&lt;/em&gt; theory, $\epsilon=0$. In the &lt;em&gt;probabilistic dector&lt;/em&gt; theory, $\epsilon&amp;gt;0$.&lt;/p&gt;

&lt;h2 id=&quot;stick-ball-machine&quot;&gt;Stick ball machine&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes.&lt;/p&gt;

&lt;h3 id=&quot;ontology-2&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Ball}$ ($N_B\sim P_B$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H=\inf$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Moves}(\mathrm{Ball}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{HiddenCause}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-2&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Moves}(B_1, T) \rightarrow \mathrm{Moves}(B_2, T)$, true for all $T$ with probability $p$ for each $B_1\neq B_2$ pair&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(H, T) \rightarrow \mathrm{Moves}(B, T)$, each $B$ has an edge from some $H$ with probability $q$. The particular $H$ is chosn according to a Chinese Restaurant Process (i.e. based on number of edges)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-2&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Active}(H, T) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Moves}(B_1, T) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, where $w_0=0$ and $w_i=\omega$ for the $i^{th}$ cause (either $B_2$ or some $H$)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lemur-colonies&quot;&gt;Lemur colonies&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes in a spatial domain.&lt;/p&gt;

&lt;h3 id=&quot;ontology-3&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Colony}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H\in [0, 1]$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Location}(\mathrm{Colony}) \rightarrow \mathcal{R}\subset\mathbb{R}^2$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Nexus}(\mathrm{HiddenCause}) \rightarrow \mathcal{R}\subset\mathbb{R}^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-3&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Nexus}(H) \rightarrow \mathrm{Location}(C)$, true with probability $p$ for each $H$, $C$ pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-3&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Nexus}(H) \sim \mathrm{Uniform}(\mathcal{R})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Location}(C) \sim \mathcal{N}(\mathrm{Nexus}(H),\Sigma)$ if $\mathrm{Nexus}(H) \rightarrow \mathrm{Location}(C)$, otherwise $\mathcal{N}((0, 0), \inf)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;exploding-cans&quot;&gt;Exploding cans&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes in a spatiotemporal domain.&lt;/p&gt;

&lt;h3 id=&quot;ontology-4&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Can}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H=\inf$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(\mathrm{Can}) \rightarrow \mathbb{R}+$ (time)&lt;/li&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(\mathrm{HiddenCause}) \rightarrow \mathbb{R}+$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Location}(\mathrm{Can}) \rightarrow \mathbb{R}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-4&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(C_1) \rightarrow \mathrm{ExplosionTime}(C_2)$, true with probability 1 for each $C_1\neq C_2$ pair&lt;/li&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(H)\rightarrow \mathrm{ExplosionTime}(C)$, each $C$ has an edge from some $H$ with probability 1. The particular $H$ is chosn according to a Chinese Restaurant Process (i.e. based on number of edges)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-4&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(H)\sim \mathrm{Exponential}(\alpha)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(C_1)\sim \mathrm{Exponential}(\lambda(t))$ for $\lambda(t)$ from a continuous noisy-OR with $w_i=\omega$ for either times $t_i=\mathrm{ActivationTime}(H)$ or $t_i=\mathrm{ExplosionTime}(C_2)+\vert\mathrm{Location}(C_2)-\mathrm{Location}(C_1)\vert/\mu$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cross-domain-causal-induction&quot;&gt;Cross-domain causal induction&lt;/h2&gt;

&lt;h3 id=&quot;ontology-5&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Cause}$ ($N_C\sim P_C$)
    &lt;ul&gt;
      &lt;li&gt;$\mathrm{InDomain}$&lt;/li&gt;
      &lt;li&gt;$\mathrm{OutDomain}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\mathrm{Effect}$ ($N_E\sim P_E$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(\mathrm{Cause}, \mathrm{Trial})\rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{Effect}, \mathrm{Trial})\rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-5&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(C, T)\rightarrow \mathrm{Active}(E, T)$, true for all $T$ with probability $p$ for each $C$, $E$ pair when $C$ is an $\mathrm{InDomain}$ cause, and with probability $q$ for each $C$, $E$ pair where $C$ is an $\mathrm{OutDomain}$ cause.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-5&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(C, T)\sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(E, T)\sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR with $w_0=\epsilon$ and $w_i=1-\epsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The framework outlined by Griffiths &amp;amp; Tenenbaum is an extremely rich and flexible framework for causal reasoning across a wide range of domains. It would be really interesting to try to apply this to even more complex physical domains—for example, can this framework be extended to explain some of the results from Michotte experiments, such as the launching effect? It’s not immediately clear to me how you would do this, but my guess is that it would be somewhat similar to the exploding can example, which includes both spatial and temporal causes.&lt;/p&gt;
</description>
        <pubDate>Sun, 10 Jan 2016 07:15:53 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Griffiths2009.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Griffiths2009.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>Learning overhypotheses with hierarchical Bayesian models</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kemp2007&quot;&gt;Kemp, C., Perfors, A., &amp;amp; Tenenbaum, J. B. (2007). Learning overhypotheses with hierarchical Bayesian models. &lt;i&gt;Developmental Science&lt;/i&gt;, &lt;i&gt;10&lt;/i&gt;(3), 307–321. doi:10.1111/j.1467-7687.2007.00585.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kemp et al. formulate a hierarchical Bayesian model (HBM) for learning overhypotheses and demonstrate how it can account for two kinds of empirical results (the shape bias, and grouping into ontological kinds).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;The model is parameterized as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbf{z} &amp;\sim \mathrm{CRP}(\gamma)\\
\alpha^k &amp;\sim \mathrm{Exponential}(\lambda)\\
\mathbf{\beta}^k &amp;\sim \mathrm{Dirichlet}(1)\\
\mathbf{\theta}^i &amp;\sim \mathrm{Dirichlet}(\alpha^{z_i}\mathbf{\beta}^{z_i})\\
\mathbf{y}^i\ \vert\ n^i &amp;\sim \mathrm{Multinomial}(\mathbf{\theta}^i)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mathbf{y}^i$ is the data (e.g. counts of features in category $i$), $n^i$ is the number of observations for category $i$, $\mathbf{\theta}^i$ specifies the distribution of features in category $i$, $\alpha^k$ is the degree of uniformity for features across categories within the ontological kind $z_i$, $\mathbf{\beta}^k$ is the distribution of features across all categories within the ontological kind $z_i$, and $\mathbf{z}$ is the partition (assignment) of categories to ontological kinds.&lt;/p&gt;

&lt;p&gt;Given training data $\mathbf{y}$, the model can be fit to reflect the distribution of features within categories and across categories within a particular ontological kind, and also to reflect the assignment of categories to ontological kinds. Given a test exemplar $T$ (with known category) and three choice objects, the model needs to produce the probability that each choice object belongs to the same category as the exemplar. It’s not entirely clear to me what they specifically compute to get this. One possibility would be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(c_C=i|i)\propto \int p(C|\mathbf{\theta}^i)p(\mathbf{\theta}^i|\mathbf{y}^i)\ \mathrm{d}\mathbf{\theta}^i&lt;/script&gt;

&lt;p&gt;where $c_C$ is the category of the choice object, $C$ are the features of the choice object, and $i$ is the category of the exemplar.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a nice use of a HBM to show how theories/overhypotheses might be implemented in a computational-level framework. I do wonder thought how much of the heavy lifting is being done by the fact that they’ve only included features that could be relevant and they’ve discretized the feature domains. In the real world, things are going to be much more ambiguous than that. First, how does the child determine what features are relevant in the first place (why pay attention to overall shape, rather than ear shape)? Second, what if some dimensions (e.g. size) are continuous values rather than discrete values? In the continuous case, modeling the data as feature counts will not longer work.&lt;/p&gt;

&lt;p&gt;It would be interesting to see if this type of framework could be applied as a way of classifying objects based on how they should be simulated. For example, this might be applicable to first grouping things ontologically (fluids, rigid bodies, soft bodies) which might determine the overall class of simulation that needs to be run. Then individual categories within each ontological kind might determine the types of parameters that need to be put into the simulation (e.g. mass, coefficient of friction, etc.). I’m not sure this is exactly the right framework for making those sorts of approximations but it might be a good place to start for at least looking at how we make the necessary decisions for setting up a simulation.&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 11:05:17 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/09/Kemp2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/09/Kemp2007.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>Thought experiments</title>
        <description>&lt;p&gt;&lt;span id=&quot;Brown2014&quot;&gt;Brown, J. R., &amp;amp; Fehige, Y. (2014). Thought Experiments. In E. N. Zalta (Ed.), &lt;i&gt;The Stanford Encyclopedia of Philosophy&lt;/i&gt; (Fall 2014). Retrieved from http://plato.stanford.edu/archives/fall2014/entries/thought-experiment/&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This entry in the Stanford Encyclopedia of Philosophy gives an outline of what thought experiments are, the history of thought experiments, and the main schools of thought regarding whether thought experiments are useful and how they work.&lt;/p&gt;

&lt;p&gt;First, while the entry does not give a specific definition of thought experiments, it does describe some common features of them: visualizing a situation in our imagination, “running” it, seeing what happens, and drawing a conclusion. Thought experiments do not necessarily result in some objective truth (e.g., following the thought experiment about throwing a spear through the edge of the universe, we now know of topologies where the space could in fact be simultaneously finite and unbounded, like a circle). Thought experiments frequently cannot be run as real experiments “for physical, technological, ethical, or financial reasons… but this needn’t be a defining condition of thought experiments”.&lt;/p&gt;

&lt;p&gt;One possible taxonomy for thought experiments categorizes them firstly as either &lt;em&gt;constructive&lt;/em&gt; or &lt;em&gt;destructive&lt;/em&gt;. Destructive thought experiments may illustrate a contradiction in a theory (e.g. Galileo’s falling objects), show that the theory is in contradiction with other beliefs (unrelated to the theory) that we hold (e.g. Schroedinger’s cat), undermine a premise of the though experiment itself (e.g. Thomson’s violinist), or slightly modify the original version of the thought experiment in order to produce an outcome which calls the conclusions from the original thought experiment into question. Constructive thought experiments provide positive support for a theory by illustrating the implications of a theory’s claims (e.g. Newton’s cannonball).&lt;/p&gt;

&lt;p&gt;The first stage of philosophical investigation of thought experiments began in the 18th and 19th centuries (Novalis, Hans-Christian Orsted). It was again revived in the beginning of the 20th century, marking the 2nd stage (Duhem, Mach, Meinong), and then again in the first part of the second half of the 20th century (Koyre, Kuhn, Popper), marking the 3rd stage. The current investigation is the 4th stage (Brown, Norton).&lt;/p&gt;

&lt;p&gt;There are a number of prominent views regarding thought experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Skeptical objection&lt;/strong&gt; (e.g. Duhem, Wilkes) — a denial that thought experiments are useful, and that they are no substitute for a real experiment. Most skeptical objections are specific to particular fields, rather than to thought experiments as a whole.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intuition&lt;/strong&gt; (e.g. Brown) — one version of the intuition account is &lt;em&gt;Platonic&lt;/em&gt; intuition, in which the claim is that what is determined in a thought experiment is “a priori (though still fallible) knowledge of nature, since there are no new data involved, nor is the conclusion derived from old data”. The other version is &lt;em&gt;naturalistic&lt;/em&gt; intuition, though it’s not entirely clear to me what this is.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Argument&lt;/strong&gt; (e.g. Norton) — thought experiments are really just inductive or deductive arguments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conceptual constructivism&lt;/strong&gt; (e.g. Van Dyck, Gendler, Camilleri, Kuhn) — thought experiments enable conceptual change; i.e. they “[help] us to re-conceptualize the world in a new way”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Experimentalism&lt;/strong&gt; (e.g. Mach, Sorenson, Buzzoni) — thought experiments are just experiments which generate “uncontrollable images of facts acquired in past experiences with the world”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mental model&lt;/strong&gt; — thought experiments are the manipulation of a non-propositional mental model, rather than a physical model. The mental model account ties into the idea of “literary fiction as thought experiments”. The idea is that fiction brings us to construct a hypothetical scenario and use our imagination to let it play out.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One more recent development in the understanding of thought experiments is the question of whether computer simulations can be thought experiments.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;There is something interesting about thought experiments—and mental simulation in general—in that we can imagine scenarios which are physically impossible and which may violate many natural laws. At the same time, as some of Schwartz’s experiments show, we can’t always use imagery in ways that violate natural laws. So why is it that we can in some cases, but cannot in others? There is something interesting going on between the interplay of actual perceptual or motor imagery (which presumably must approximately conform to experience with the world as it is goverened by our sensory modalities rather than higher level cognition) and the capacity for abstract of symbolic thought. I can &lt;em&gt;conceptualize&lt;/em&gt; the idea of gravity going in the opposite direction, and even run thought experiments regarding what the consequences would be of that. But I can’t do things like &lt;em&gt;visualize&lt;/em&gt; a glass with water in it rotating upside down without the water spilling out. So it seems to me that certain types of thought experiments—particularly ones which violate physical laws—are &lt;em&gt;not&lt;/em&gt; using information from low level sensorimotor processes. Or at least not using the full information the same way. So where does that information come from, then? Do we construct more abstract models about how the world works initially based on low-level information, but which are conceptual/abstract enough to be manipulated during the course of a thought experiment?&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 10:21:27 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Brown2014.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Brown2014.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>The role of imagistic simulation in scientific thought experiments</title>
        <description>&lt;p&gt;&lt;span id=&quot;Clement2009&quot;&gt;Clement, J. J. (2009). The Role of Imagistic Simulation in Scientific Thought Experiments. &lt;i&gt;Topics In Cognitive Science&lt;/i&gt;, &lt;i&gt;1&lt;/i&gt;(4), 686–710. doi:10.1111/j.1756-8765.2009.01031.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Clement poses the &lt;em&gt;fundamental paradox of thought experiments&lt;/em&gt; as being “How can findings that carry conviction result from a new experiment conducted entirely within the head?” (pg. 687). He attempts to provide a resolution to this paradox based on the idea of “imagistic simulation” (a.k.a. mental simulation, mental imagery, etc.) with evidence provided through a case study of a single expert subject (S2). S2 is posed with the following “spring problem”:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A weight is hung on a spring (Fig. 1). The original spring is replaced with a spring made of the same kind of wire, with the same number of coils, but with coils that are twice as wide in diameter. Will the spring stretch from its natural length more, less, or the same amount under the same weight? (Assume the mass of the spring is negligible). Why do you think so? (pg. 689)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clement specifies his definition of “thought experiment” (TE) as being “the act of considering an untested, concrete system (the ‘experiment’ or case) and attempting to predict aspects of its behavior” (pg. 690-1). He isolates a number of TEs produced by S2, and analyzes the use of imagistic simulation in those TEs. He finds that S2 spontaneously engaged in “personal action projection (spontaneously redescribing a system action in terms of a human action) consistent with the use of kinesthetic imagery, depictive getures (gestures that depict objects, forces, locations, or movements of entities), and imagery reports” (pg. 694). Some of these are characteristic of using static imagery, but others are characteristic of &lt;em&gt;dynamic&lt;/em&gt; imagery.&lt;/p&gt;

&lt;p&gt;To explain the use of dynamic imagery, Clement appeals to the idea of &lt;a href=&quot;https://en.wikipedia.org/wiki/Motor_program#Schmidt.E2.80.99s_schema_theory&quot;&gt;motor schema theory&lt;/a&gt; in which imagistic simulations are driven by the use of motor programs/schema. Specifically, he identifies four possible components to an imagistic simulation which allow such simulations to apply to situations which have not previously been encountered:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Applying a schema to a use outside its normal domain&lt;/li&gt;
  &lt;li&gt;Converting implicit knowledge into explicit knowledge&lt;/li&gt;
  &lt;li&gt;Including spatial reasoning&lt;/li&gt;
  &lt;li&gt;Combining multiple schemas into a &lt;em&gt;compound simulation&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Additionally, Clement argues that imagistic simulations are used to generate “enhanced” imagery:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…the main source of conviction in the simulations is the tapping of implicit knowledge embedded in motor schemas and its conversion into explicit knowledge. The extreme case makes differences in implicit expectations larger and more ‘perceivable’ in this case. (pg. 698)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clement argues that this formulation of imagistic simulation resolves the TE paradox:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To address the TE paradox, the idea of perceptual motor schemas running imagistic simulations, and the four more specific sources of conviction within imagistic simulations… can account for ways that a TE can &lt;em&gt;feel&lt;/em&gt; empirical (via the inspection of imagery) or necessary (via confident schema extension or spatial reasoning). Yet these processes actually involve a considerable amount of nonformal reasoning and inference that goes beyond prior observations. (pg. 704)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In discussing the case study with S2, Clement also touches on the distinction between evaluative (disconfirmatory/confirmatory) and generative thought experiments.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I like this characterization of what thought experiments (and perhaps, even, mental simulations more generally) are; I think Clement is right in tying the use of thought experiments to action &lt;em&gt;and&lt;/em&gt; perception.&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 07:35:52 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Clement2009.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Clement2009.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>&quot;What if...&quot;: The use of conceptual simulations in scientific reasoning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Trickett2007&quot;&gt;Trickett, S. B., &amp;amp; Trafton, J. G. (2007). “What if…”: The Use of Conceptual Simulations in Scientific Reasoning. &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;31&lt;/i&gt;(5), 843–875. doi:10.1080/03640210701530771&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Trickett &amp;amp; Trafton experimentally explore the use of &lt;em&gt;conceptual simulations&lt;/em&gt; by expert scientists when reasoning about problems in their domain of expertise. They have two main hypotheses: that conceptual simulations are a core strategy used in scientific reasoning, and that they are used in particular to reason about situations in which there are high levels of uncertainty (e.g. partial knowledge, violation of expectation, etc.). They define conceptual simulation as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…a three-step process that consists of first, visualizing some situation; second, carrying out one or more operations on it; and third, seeing what happens. The third part of the process—seeing what happens—is crucial. It distinguishes “what if” thinking from purely imagining because during this third phase &lt;em&gt;causal reasoning&lt;/em&gt; occurs to the results of the manipulation(s) of the second phase.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In their experiments, Trickett &amp;amp; Trafton found that scientists do spontaneously use conceptual simulation and that they use it in cases where their expectations are violated (i.e. they have more uncertainty):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The research shows how conceptual simulation helps resolve uncertainty: conceptual simulation facilitates reasoning about hypotheses by generating an altered representation under the purported conditions expressed in the hypothesis and providing a source of comparison with the actual data, in the process of alignment by similarity detection. (pg. 866)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Additionally, the results of these experiments combined with other results from the literature suggest that conceptual simulations are used in situations where there the answer truly is unknown. In other cases, people can rely on background knowledge, existing models, etc.:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Frequently, studies of experts employ problems that are well-understood for an expert and that can be solved by recalling either this very problem (i.e., by model-based search) or another that shares the same deep structure (i.e., by analogy; cf. Chi et al., 1981). In contrast, our studies show experts reasoning about problems for which neither they nor anyone else knows the answer. (pg. 867)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That is, conceptual simulation is a type of model &lt;em&gt;construction&lt;/em&gt; (pg. 866).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, Trickett &amp;amp; Trafton performed an &lt;em&gt;in vivo&lt;/em&gt; study of scientists across several different domains of science. The scientists were filmed while they analyzed their own data, either individually (with a verbal protocol) or collaboratively. The utterances of the scientists were coded for instances of conceptual simulation, for hypotheses, for and for other scientific reasoning strategies (data focus, empirical test, consult a colleague, tie-in with theory and domain knowledge, analogy, or alignment). They found that data focus was the most commonly used strategy. The next most frequently used strategies were tie-in with theory, alignment, and conceptual simulation; these were used at approximately the same frequency.&lt;/p&gt;

&lt;p&gt;In analyzing the relationship between strategies, they found that conceptual simulations were almost always followed by a process of alignment, which was then usually either the end of the chain of reasoning, or which was followed by a return to data focus. Trickett &amp;amp; Trafton hypothesized that this sequence of conceptual simulation followed by alignment was used “to link the internal (result of the conceptual simulation) and external (phenomena in the data) representations” (pg. 858).&lt;/p&gt;

&lt;p&gt;They additionally coded the data for hypotheses that were generated either based on evidence that violated expectations or which was consistent with expectations. They found that conceptual simulation more frequently followed violation of expectation hypotheses than those that did not have a violation of expectation, suggesting that the scientists used conceptual simulation in situations where they were more uncertain.&lt;/p&gt;

&lt;p&gt;To causally test the previous hypothesis (that conceptual simulations are used in situations with higher uncertainty), Trickett &amp;amp; Trafton ran a second experiment. In Experiment 2, they recruited expert cognitive psychologists and gave them different scenarios and results of phenomena they were familiar with. The results were either consistent with the given scenario (Expectation Confirmation, EC) or inconsistent (Expectation Violation, EV). The scientists were instructed to engage in a process of explaining the data, and again were recorded doing so. Consistent with the results of Experiment 1, they found that conceptual simulations were used more frequently in the EV condition than in the EC condition, at a rate of approximately 2:1.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;These types of “conceptual simulations”, as well as thought experiments like those described by &lt;a href=&quot;/quals/thought%20experiments/2016/01/08/Gendler1998.html&quot;&gt;Gendler&lt;/a&gt;, are really fascinating in that they seem to be qualitatively a very different sort of simulation than, for example, motor simulation or even certain types of mental imagery (like that which is used in language understanding). I think a relevant question is: are such types of conceptual simulations drawing on the same types of simulation processes that serve lower levels of reasoning? I would expect the answer to be “sometimes”, but I don’t have a good intuition for why certain low-level simulations would be available for high-level conceptual reasoning (e.g. imagery) while others wouldn’t (e.g. accurate simulation of physics via the motor system).&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 04:33:40 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Trickett2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Trickett2007.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
  </channel>
</rss>
