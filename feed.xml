<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 10 Jan 2016 17:18:08 -0800</pubDate>
    <lastBuildDate>Sun, 10 Jan 2016 17:18:08 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Theory-based causal induction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Griffiths2009&quot;&gt;Griffiths, T. L., &amp;amp; Tenenbaum, J. B. (2009). Theory-based causal induction. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;116&lt;/i&gt;(4), 661–716. doi:10.1037/a0017201&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Griffiths &amp;amp; Tenenbaum outline a probabilistic framework for causal induction that allows inference to be done over variables, graphs, and theories. The framework has three main components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ontology of entities, properties, and relations (i.e., “how entities are differentiated on the basis of their causal properties”, pg. 663)&lt;/li&gt;
  &lt;li&gt;Plausible relations between entities (i.e., what types of relationships should be considered in a given domain)&lt;/li&gt;
  &lt;li&gt;Functional forms of relations (i.e., the direction in which causes act, and how causes combine)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These three components are implemented in a probabilistic graphical model as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ontology = variables&lt;/li&gt;
  &lt;li&gt;Plausible relations = graph (the &lt;em&gt;structure&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;Functional form = probability distribution (the &lt;em&gt;parameterization&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Given an existing ontology, learning the plausible relations is a type of &lt;em&gt;structure learning&lt;/em&gt;, and learning the functional form is a type of &lt;em&gt;parameter estimation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Griffiths &amp;amp; Tenenbaum note, however, that knowledge of ontologies, plausible relations, and functional forms itself is not something that can be expressed through the framework of graphical models:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Knowledge about the ontology, plausibility, and functional form of causal relationships should influence the prior, likelihood, and hypothesis space for Bayesian inference. However, expressing this knowledge requires going beyond the representational capacities of causal graphical models. Although this knowledge can be &lt;em&gt;instantiated&lt;/em&gt; in a causal graphical model, it generalizes over a set of such models, and thus cannot be &lt;em&gt;expressed&lt;/em&gt; in any one model. (pg. 669)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They argue that this type of knowledge is akin to the notion of an &lt;em&gt;intuitive theory&lt;/em&gt;, and draw the relationship between theories and grammars; causal structures and syntactic structures; and data and sentences. To formalize theories in their framework, they make use of &lt;em&gt;hierarchical Bayesian models&lt;/em&gt; which allow inference to be performed both at the level of the theory as well as the lower levels of causal structures and relationships.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;types-of-functional-forms&quot;&gt;Types of functional forms&lt;/h2&gt;

&lt;h3 id=&quot;noisy-or&quot;&gt;Noisy-OR&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(e^+\vert c; w_0, w_1)=1-(1-w_0)(1-w_1)^c&lt;/script&gt;

&lt;p&gt;where $e^+$ is the presence of the effect, $c$ is the presence/absence of the cause $w_0$ is the probability of the effect in the absence of the cause, and $w_1$ is the probability of the effect given a single cause.&lt;/p&gt;

&lt;h3 id=&quot;noisy-and-not&quot;&gt;Noisy-AND-NOT&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(e^+\vert c; w_0, w_1)=w_0(1-w_1)^c&lt;/script&gt;

&lt;h3 id=&quot;generic&quot;&gt;Generic&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(e^+\vert c^-)&amp;=w_0\\
p(e^+\vert c^+)&amp;=w_1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;continuous-noisy-or&quot;&gt;Continuous Noisy-OR&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t)=\sum_i w_i\delta(t, t_i)&lt;/script&gt;

&lt;p&gt;where $w_i$ and $t_i$ are the weight and time associated with the $i$th cause.&lt;/p&gt;

&lt;h2 id=&quot;medical-contingency-data&quot;&gt;Medical contingency data&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for contingency data.&lt;/p&gt;

&lt;h3 id=&quot;ontology&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Chemical}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Gene}$ ($N_G\sim P_G$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Mouse}$ ($N_M\sim P_M$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(\mathrm{Chemical}, \mathrm{Mouse}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Expressed}(\mathrm{Gene}, \mathrm{Mouse}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(C, M) \rightarrow \mathrm{Expressed}(G, M)$, true for all $M$ with probability $p$ for each $C$, $G$ pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(C, M) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Expressed}(G, M) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, noisy-AND-NOT, or generic functional form, where $w_0$ and $w_1$ are drawn from uniform distributions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;blicket-detection&quot;&gt;Blicket detection&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for small amounts of data.&lt;/p&gt;

&lt;h3 id=&quot;ontology-1&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Block}$ ($N_B\sim P_B$)
    &lt;ul&gt;
      &lt;li&gt;$\mathrm{Blicket}$ ($p$)&lt;/li&gt;
      &lt;li&gt;$\mathrm{NonBlicket}$ ($1-p$)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\mathrm{Detector}$ ($N_D\sim P_D$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(\mathrm{Block}, \mathrm{Detector}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{Detector}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-1&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(B, D, T) \rightarrow \mathrm{Active}(D, T)$ for all $T$ for any $D$ if $B$ is a $\mathrm{Blicket}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-1&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(B, D, T) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(D, T) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, where $w_0=\epsilon$ and $w_1=1-\epsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the &lt;em&gt;deterministic detector&lt;/em&gt; theory, $\epsilon=0$. In the &lt;em&gt;probabilistic dector&lt;/em&gt; theory, $\epsilon&amp;gt;0$.&lt;/p&gt;

&lt;h2 id=&quot;stick-ball-machine&quot;&gt;Stick ball machine&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes.&lt;/p&gt;

&lt;h3 id=&quot;ontology-2&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Ball}$ ($N_B\sim P_B$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H=\inf$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Moves}(\mathrm{Ball}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{HiddenCause}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-2&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Moves}(B_1, T) \rightarrow \mathrm{Moves}(B_2, T)$, true for all $T$ with probability $p$ for each $B_1\neq B_2$ pair&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(H, T) \rightarrow \mathrm{Moves}(B, T)$, each $B$ has an edge from some $H$ with probability $q$. The particular $H$ is chosn according to a Chinese Restaurant Process (i.e. based on number of edges)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-2&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Active}(H, T) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Moves}(B_1, T) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, where $w_0=0$ and $w_i=\omega$ for the $i^{th}$ cause (either $B_2$ or some $H$)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lemur-colonies&quot;&gt;Lemur colonies&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes in a spatial domain.&lt;/p&gt;

&lt;h3 id=&quot;ontology-3&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Colony}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H\in [0, 1]$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Location}(\mathrm{Colony}) \rightarrow \mathcal{R}\subset\mathbb{R}^2$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Nexus}(\mathrm{HiddenCause}) \rightarrow \mathcal{R}\subset\mathbb{R}^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-3&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Nexus}(H) \rightarrow \mathrm{Location}(C)$, true with probability $p$ for each $H$, $C$ pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-3&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Nexus}(H) \sim \mathrm{Uniform}(\mathcal{R})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Location}(C) \sim \mathcal{N}(\mathrm{Nexus}(H),\Sigma)$ if $\mathrm{Nexus}(H) \rightarrow \mathrm{Location}(C)$, otherwise $\mathcal{N}((0, 0), \inf)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;exploding-cans&quot;&gt;Exploding cans&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes in a spatiotemporal domain.&lt;/p&gt;

&lt;h3 id=&quot;ontology-4&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Can}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H=\inf$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(\mathrm{Can}) \rightarrow \mathbb{R}+$ (time)&lt;/li&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(\mathrm{HiddenCause}) \rightarrow \mathbb{R}+$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Location}(\mathrm{Can}) \rightarrow \mathbb{R}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-4&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(C_1) \rightarrow \mathrm{ExplosionTime}(C_2)$, true with probability 1 for each $C_1\neq C_2$ pair&lt;/li&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(H)\rightarrow \mathrm{ExplosionTime}(C)$, each $C$ has an edge from some $H$ with probability 1. The particular $H$ is chosn according to a Chinese Restaurant Process (i.e. based on number of edges)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-4&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(H)\sim \mathrm{Exponential}(\alpha)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(C_1)\sim \mathrm{Exponential}(\lambda(t))$ for $\lambda(t)$ from a continuous noisy-OR with $w_i=\omega$ for either times $t_i=\mathrm{ActivationTime}(H)$ or $t_i=\mathrm{ExplosionTime}(C_2)+\vert\mathrm{Location}(C_2)-\mathrm{Location}(C_1)\vert/\mu$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cross-domain-causal-induction&quot;&gt;Cross-domain causal induction&lt;/h2&gt;

&lt;h3 id=&quot;ontology-5&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Cause}$ ($N_C\sim P_C$)
    &lt;ul&gt;
      &lt;li&gt;$\mathrm{InDomain}$&lt;/li&gt;
      &lt;li&gt;$\mathrm{OutDomain}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\mathrm{Effect}$ ($N_E\sim P_E$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(\mathrm{Cause}, \mathrm{Trial})\rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{Effect}, \mathrm{Trial})\rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-5&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(C, T)\rightarrow \mathrm{Active}(E, T)$, true for all $T$ with probability $p$ for each $C$, $E$ pair when $C$ is an $\mathrm{InDomain}$ cause, and with probability $q$ for each $C$, $E$ pair where $C$ is an $\mathrm{OutDomain}$ cause.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-5&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(C, T)\sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(E, T)\sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR with $w_0=\epsilon$ and $w_i=1-\epsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The framework outlined by Griffiths &amp;amp; Tenenbaum is an extremely rich and flexible framework for causal reasoning across a wide range of domains. It would be really interesting to try to apply this to even more complex physical domains—for example, can this framework be extended to explain some of the results from Michotte experiments, such as the launching effect? It’s not immediately clear to me how you would do this, but my guess is that it would be somewhat similar to the exploding can example, which includes both spatial and temporal causes.&lt;/p&gt;
</description>
        <pubDate>Sun, 10 Jan 2016 07:15:53 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Griffiths2009.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Griffiths2009.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>Learning overhypotheses with hierarchical Bayesian models</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kemp2007&quot;&gt;Kemp, C., Perfors, A., &amp;amp; Tenenbaum, J. B. (2007). Learning overhypotheses with hierarchical Bayesian models. &lt;i&gt;Developmental Science&lt;/i&gt;, &lt;i&gt;10&lt;/i&gt;(3), 307–321. doi:10.1111/j.1467-7687.2007.00585.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kemp et al. formulate a hierarchical Bayesian model (HBM) for learning overhypotheses and demonstrate how it can account for two kinds of empirical results (the shape bias, and grouping into ontological kinds).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;The model is parameterized as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbf{z} &amp;\sim \mathrm{CRP}(\gamma)\\
\alpha^k &amp;\sim \mathrm{Exponential}(\lambda)\\
\mathbf{\beta}^k &amp;\sim \mathrm{Dirichlet}(1)\\
\mathbf{\theta}^i &amp;\sim \mathrm{Dirichlet}(\alpha^{z_i}\mathbf{\beta}^{z_i})\\
\mathbf{y}^i\ \vert\ n^i &amp;\sim \mathrm{Multinomial}(\mathbf{\theta}^i)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mathbf{y}^i$ is the data (e.g. counts of features in category $i$), $n^i$ is the number of observations for category $i$, $\mathbf{\theta}^i$ specifies the distribution of features in category $i$, $\alpha^k$ is the degree of uniformity for features across categories within the ontological kind $z_i$, $\mathbf{\beta}^k$ is the distribution of features across all categories within the ontological kind $z_i$, and $\mathbf{z}$ is the partition (assignment) of categories to ontological kinds.&lt;/p&gt;

&lt;p&gt;Given training data $\mathbf{y}$, the model can be fit to reflect the distribution of features within categories and across categories within a particular ontological kind, and also to reflect the assignment of categories to ontological kinds. Given a test exemplar $T$ (with known category) and three choice objects, the model needs to produce the probability that each choice object belongs to the same category as the exemplar. It’s not entirely clear to me what they specifically compute to get this. One possibility would be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(c_C=i|i)\propto \int p(C|\mathbf{\theta}^i)p(\mathbf{\theta}^i|\mathbf{y}^i)\ \mathrm{d}\mathbf{\theta}^i&lt;/script&gt;

&lt;p&gt;where $c_C$ is the category of the choice object, $C$ are the features of the choice object, and $i$ is the category of the exemplar.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a nice use of a HBM to show how theories/overhypotheses might be implemented in a computational-level framework. I do wonder thought how much of the heavy lifting is being done by the fact that they’ve only included features that could be relevant and they’ve discretized the feature domains. In the real world, things are going to be much more ambiguous than that. First, how does the child determine what features are relevant in the first place (why pay attention to overall shape, rather than ear shape)? Second, what if some dimensions (e.g. size) are continuous values rather than discrete values? In the continuous case, modeling the data as feature counts will not longer work.&lt;/p&gt;

&lt;p&gt;It would be interesting to see if this type of framework could be applied as a way of classifying objects based on how they should be simulated. For example, this might be applicable to first grouping things ontologically (fluids, rigid bodies, soft bodies) which might determine the overall class of simulation that needs to be run. Then individual categories within each ontological kind might determine the types of parameters that need to be put into the simulation (e.g. mass, coefficient of friction, etc.). I’m not sure this is exactly the right framework for making those sorts of approximations but it might be a good place to start for at least looking at how we make the necessary decisions for setting up a simulation.&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 11:05:17 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/09/Kemp2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/09/Kemp2007.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>Thought experiments</title>
        <description>&lt;p&gt;&lt;span id=&quot;Brown2014&quot;&gt;Brown, J. R., &amp;amp; Fehige, Y. (2014). Thought Experiments. In E. N. Zalta (Ed.), &lt;i&gt;The Stanford Encyclopedia of Philosophy&lt;/i&gt; (Fall 2014). Retrieved from http://plato.stanford.edu/archives/fall2014/entries/thought-experiment/&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This entry in the Stanford Encyclopedia of Philosophy gives an outline of what thought experiments are, the history of thought experiments, and the main schools of thought regarding whether thought experiments are useful and how they work.&lt;/p&gt;

&lt;p&gt;First, while the entry does not give a specific definition of thought experiments, it does describe some common features of them: visualizing a situation in our imagination, “running” it, seeing what happens, and drawing a conclusion. Thought experiments do not necessarily result in some objective truth (e.g., following the thought experiment about throwing a spear through the edge of the universe, we now know of topologies where the space could in fact be simultaneously finite and unbounded, like a circle). Thought experiments frequently cannot be run as real experiments “for physical, technological, ethical, or financial reasons… but this needn’t be a defining condition of thought experiments”.&lt;/p&gt;

&lt;p&gt;One possible taxonomy for thought experiments categorizes them firstly as either &lt;em&gt;constructive&lt;/em&gt; or &lt;em&gt;destructive&lt;/em&gt;. Destructive thought experiments may illustrate a contradiction in a theory (e.g. Galileo’s falling objects), show that the theory is in contradiction with other beliefs (unrelated to the theory) that we hold (e.g. Schroedinger’s cat), undermine a premise of the though experiment itself (e.g. Thomson’s violinist), or slightly modify the original version of the thought experiment in order to produce an outcome which calls the conclusions from the original thought experiment into question. Constructive thought experiments provide positive support for a theory by illustrating the implications of a theory’s claims (e.g. Newton’s cannonball).&lt;/p&gt;

&lt;p&gt;The first stage of philosophical investigation of thought experiments began in the 18th and 19th centuries (Novalis, Hans-Christian Orsted). It was again revived in the beginning of the 20th century, marking the 2nd stage (Duhem, Mach, Meinong), and then again in the first part of the second half of the 20th century (Koyre, Kuhn, Popper), marking the 3rd stage. The current investigation is the 4th stage (Brown, Norton).&lt;/p&gt;

&lt;p&gt;There are a number of prominent views regarding thought experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Skeptical objection&lt;/strong&gt; (e.g. Duhem, Wilkes) — a denial that thought experiments are useful, and that they are no substitute for a real experiment. Most skeptical objections are specific to particular fields, rather than to thought experiments as a whole.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intuition&lt;/strong&gt; (e.g. Brown) — one version of the intuition account is &lt;em&gt;Platonic&lt;/em&gt; intuition, in which the claim is that what is determined in a thought experiment is “a priori (though still fallible) knowledge of nature, since there are no new data involved, nor is the conclusion derived from old data”. The other version is &lt;em&gt;naturalistic&lt;/em&gt; intuition, though it’s not entirely clear to me what this is.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Argument&lt;/strong&gt; (e.g. Norton) — thought experiments are really just inductive or deductive arguments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conceptual constructivism&lt;/strong&gt; (e.g. Van Dyck, Gendler, Camilleri, Kuhn) — thought experiments enable conceptual change; i.e. they “[help] us to re-conceptualize the world in a new way”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Experimentalism&lt;/strong&gt; (e.g. Mach, Sorenson, Buzzoni) — thought experiments are just experiments which generate “uncontrollable images of facts acquired in past experiences with the world”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mental model&lt;/strong&gt; — thought experiments are the manipulation of a non-propositional mental model, rather than a physical model. The mental model account ties into the idea of “literary fiction as thought experiments”. The idea is that fiction brings us to construct a hypothetical scenario and use our imagination to let it play out.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One more recent development in the understanding of thought experiments is the question of whether computer simulations can be thought experiments.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;There is something interesting about thought experiments—and mental simulation in general—in that we can imagine scenarios which are physically impossible and which may violate many natural laws. At the same time, as some of Schwartz’s experiments show, we can’t always use imagery in ways that violate natural laws. So why is it that we can in some cases, but cannot in others? There is something interesting going on between the interplay of actual perceptual or motor imagery (which presumably must approximately conform to experience with the world as it is goverened by our sensory modalities rather than higher level cognition) and the capacity for abstract of symbolic thought. I can &lt;em&gt;conceptualize&lt;/em&gt; the idea of gravity going in the opposite direction, and even run thought experiments regarding what the consequences would be of that. But I can’t do things like &lt;em&gt;visualize&lt;/em&gt; a glass with water in it rotating upside down without the water spilling out. So it seems to me that certain types of thought experiments—particularly ones which violate physical laws—are &lt;em&gt;not&lt;/em&gt; using information from low level sensorimotor processes. Or at least not using the full information the same way. So where does that information come from, then? Do we construct more abstract models about how the world works initially based on low-level information, but which are conceptual/abstract enough to be manipulated during the course of a thought experiment?&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 10:21:27 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Brown2014.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Brown2014.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>The role of imagistic simulation in scientific thought experiments</title>
        <description>&lt;p&gt;&lt;span id=&quot;Clement2009&quot;&gt;Clement, J. J. (2009). The Role of Imagistic Simulation in Scientific Thought Experiments. &lt;i&gt;Topics In Cognitive Science&lt;/i&gt;, &lt;i&gt;1&lt;/i&gt;(4), 686–710. doi:10.1111/j.1756-8765.2009.01031.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Clement poses the &lt;em&gt;fundamental paradox of thought experiments&lt;/em&gt; as being “How can findings that carry conviction result from a new experiment conducted entirely within the head?” (pg. 687). He attempts to provide a resolution to this paradox based on the idea of “imagistic simulation” (a.k.a. mental simulation, mental imagery, etc.) with evidence provided through a case study of a single expert subject (S2). S2 is posed with the following “spring problem”:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A weight is hung on a spring (Fig. 1). The original spring is replaced with a spring made of the same kind of wire, with the same number of coils, but with coils that are twice as wide in diameter. Will the spring stretch from its natural length more, less, or the same amount under the same weight? (Assume the mass of the spring is negligible). Why do you think so? (pg. 689)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clement specifies his definition of “thought experiment” (TE) as being “the act of considering an untested, concrete system (the ‘experiment’ or case) and attempting to predict aspects of its behavior” (pg. 690-1). He isolates a number of TEs produced by S2, and analyzes the use of imagistic simulation in those TEs. He finds that S2 spontaneously engaged in “personal action projection (spontaneously redescribing a system action in terms of a human action) consistent with the use of kinesthetic imagery, depictive getures (gestures that depict objects, forces, locations, or movements of entities), and imagery reports” (pg. 694). Some of these are characteristic of using static imagery, but others are characteristic of &lt;em&gt;dynamic&lt;/em&gt; imagery.&lt;/p&gt;

&lt;p&gt;To explain the use of dynamic imagery, Clement appeals to the idea of &lt;a href=&quot;https://en.wikipedia.org/wiki/Motor_program#Schmidt.E2.80.99s_schema_theory&quot;&gt;motor schema theory&lt;/a&gt; in which imagistic simulations are driven by the use of motor programs/schema. Specifically, he identifies four possible components to an imagistic simulation which allow such simulations to apply to situations which have not previously been encountered:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Applying a schema to a use outside its normal domain&lt;/li&gt;
  &lt;li&gt;Converting implicit knowledge into explicit knowledge&lt;/li&gt;
  &lt;li&gt;Including spatial reasoning&lt;/li&gt;
  &lt;li&gt;Combining multiple schemas into a &lt;em&gt;compound simulation&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Additionally, Clement argues that imagistic simulations are used to generate “enhanced” imagery:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…the main source of conviction in the simulations is the tapping of implicit knowledge embedded in motor schemas and its conversion into explicit knowledge. The extreme case makes differences in implicit expectations larger and more ‘perceivable’ in this case. (pg. 698)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clement argues that this formulation of imagistic simulation resolves the TE paradox:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To address the TE paradox, the idea of perceptual motor schemas running imagistic simulations, and the four more specific sources of conviction within imagistic simulations… can account for ways that a TE can &lt;em&gt;feel&lt;/em&gt; empirical (via the inspection of imagery) or necessary (via confident schema extension or spatial reasoning). Yet these processes actually involve a considerable amount of nonformal reasoning and inference that goes beyond prior observations. (pg. 704)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In discussing the case study with S2, Clement also touches on the distinction between evaluative (disconfirmatory/confirmatory) and generative thought experiments.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I like this characterization of what thought experiments (and perhaps, even, mental simulations more generally) are; I think Clement is right in tying the use of thought experiments to action &lt;em&gt;and&lt;/em&gt; perception.&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 07:35:52 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Clement2009.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Clement2009.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>&quot;What if...&quot;: The use of conceptual simulations in scientific reasoning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Trickett2007&quot;&gt;Trickett, S. B., &amp;amp; Trafton, J. G. (2007). “What if…”: The Use of Conceptual Simulations in Scientific Reasoning. &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;31&lt;/i&gt;(5), 843–875. doi:10.1080/03640210701530771&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Trickett &amp;amp; Trafton experimentally explore the use of &lt;em&gt;conceptual simulations&lt;/em&gt; by expert scientists when reasoning about problems in their domain of expertise. They have two main hypotheses: that conceptual simulations are a core strategy used in scientific reasoning, and that they are used in particular to reason about situations in which there are high levels of uncertainty (e.g. partial knowledge, violation of expectation, etc.). They define conceptual simulation as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…a three-step process that consists of first, visualizing some situation; second, carrying out one or more operations on it; and third, seeing what happens. The third part of the process—seeing what happens—is crucial. It distinguishes “what if” thinking from purely imagining because during this third phase &lt;em&gt;causal reasoning&lt;/em&gt; occurs to the results of the manipulation(s) of the second phase.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In their experiments, Trickett &amp;amp; Trafton found that scientists do spontaneously use conceptual simulation and that they use it in cases where their expectations are violated (i.e. they have more uncertainty):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The research shows how conceptual simulation helps resolve uncertainty: conceptual simulation facilitates reasoning about hypotheses by generating an altered representation under the purported conditions expressed in the hypothesis and providing a source of comparison with the actual data, in the process of alignment by similarity detection. (pg. 866)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Additionally, the results of these experiments combined with other results from the literature suggest that conceptual simulations are used in situations where there the answer truly is unknown. In other cases, people can rely on background knowledge, existing models, etc.:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Frequently, studies of experts employ problems that are well-understood for an expert and that can be solved by recalling either this very problem (i.e., by model-based search) or another that shares the same deep structure (i.e., by analogy; cf. Chi et al., 1981). In contrast, our studies show experts reasoning about problems for which neither they nor anyone else knows the answer. (pg. 867)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That is, conceptual simulation is a type of model &lt;em&gt;construction&lt;/em&gt; (pg. 866).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, Trickett &amp;amp; Trafton performed an &lt;em&gt;in vivo&lt;/em&gt; study of scientists across several different domains of science. The scientists were filmed while they analyzed their own data, either individually (with a verbal protocol) or collaboratively. The utterances of the scientists were coded for instances of conceptual simulation, for hypotheses, for and for other scientific reasoning strategies (data focus, empirical test, consult a colleague, tie-in with theory and domain knowledge, analogy, or alignment). They found that data focus was the most commonly used strategy. The next most frequently used strategies were tie-in with theory, alignment, and conceptual simulation; these were used at approximately the same frequency.&lt;/p&gt;

&lt;p&gt;In analyzing the relationship between strategies, they found that conceptual simulations were almost always followed by a process of alignment, which was then usually either the end of the chain of reasoning, or which was followed by a return to data focus. Trickett &amp;amp; Trafton hypothesized that this sequence of conceptual simulation followed by alignment was used “to link the internal (result of the conceptual simulation) and external (phenomena in the data) representations” (pg. 858).&lt;/p&gt;

&lt;p&gt;They additionally coded the data for hypotheses that were generated either based on evidence that violated expectations or which was consistent with expectations. They found that conceptual simulation more frequently followed violation of expectation hypotheses than those that did not have a violation of expectation, suggesting that the scientists used conceptual simulation in situations where they were more uncertain.&lt;/p&gt;

&lt;p&gt;To causally test the previous hypothesis (that conceptual simulations are used in situations with higher uncertainty), Trickett &amp;amp; Trafton ran a second experiment. In Experiment 2, they recruited expert cognitive psychologists and gave them different scenarios and results of phenomena they were familiar with. The results were either consistent with the given scenario (Expectation Confirmation, EC) or inconsistent (Expectation Violation, EV). The scientists were instructed to engage in a process of explaining the data, and again were recorded doing so. Consistent with the results of Experiment 1, they found that conceptual simulations were used more frequently in the EV condition than in the EC condition, at a rate of approximately 2:1.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;These types of “conceptual simulations”, as well as thought experiments like those described by &lt;a href=&quot;/quals/thought%20experiments/2016/01/08/Gendler1998.html&quot;&gt;Gendler&lt;/a&gt;, are really fascinating in that they seem to be qualitatively a very different sort of simulation than, for example, motor simulation or even certain types of mental imagery (like that which is used in language understanding). I think a relevant question is: are such types of conceptual simulations drawing on the same types of simulation processes that serve lower levels of reasoning? I would expect the answer to be “sometimes”, but I don’t have a good intuition for why certain low-level simulations would be available for high-level conceptual reasoning (e.g. imagery) while others wouldn’t (e.g. accurate simulation of physics via the motor system).&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 04:33:40 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Trickett2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Trickett2007.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>Galileo and the indispensability of scientific thought experiment</title>
        <description>&lt;p&gt;&lt;span id=&quot;Gendler1998&quot;&gt;Gendler, T. S. (1998). Galileo and the Indispensability of Scientific Thought Experiment. &lt;i&gt;The British Journal For the Philosophy of Science&lt;/i&gt;, &lt;i&gt;49&lt;/i&gt;(3), 397–424.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Gendler argues that thought experiments are important and justified for the use of scientific inquiry, using Galileo’s thought experiment demonstrating that two objects with different masses fall at the same rate. The thought experiment goes, imagine that two objects with different weights are strapped together. Because the lighter object falls slower than the heavier object, it must slow the heavier object down, and thus the speed that they fall together must be somewhere in between the speed of the heavy object alone and the light object alone. But, together, they also have a greater mass, meaning that together they should fall faster than the heavy object alone. Thus, for there to not be a contradiction, the objects must actually fall at the same speed.&lt;/p&gt;

&lt;p&gt;Gendler addresses the claim that thought experiments are just another form of deductive/inductive reasoning from a set of explicit premises, and explains what they add over and above pure argumentation. Specifically, she argues against the &lt;em&gt;Elimination Thesis&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The Elimination Thesis&lt;/strong&gt;: Any conclusion reached by a (successful) scientific thought experiment will also be demonstrable by a non-thought-experimental argument. (pg. 398)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;She breaks the Elimination Thesis down into two separate claims:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The Dispensibility Thesis&lt;/strong&gt;: Any good scientific thought experiment can be replaced, without loss of demonstrative force, by a non-thought-experimental argument. (pg. 401)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The Derivativity Thesis&lt;/strong&gt;: The justificatory force of any good scientific thought experiment can only be explained by the fact that it can be replaced, without loss of demonstrative force, by a non-thought-experimental argument. (pg. 401)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, Gendler rephrases the thought experiment in terms of a few propositions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1) Natural speed is mediative [averaging].&lt;/li&gt;
  &lt;li&gt;(2) Weight is additive.&lt;/li&gt;
  &lt;li&gt;(3) Natural speed is not directly proportional to weight.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, she goes about disproving the Dispensibility Thesis by giving an example set of alternate premises that an Aristotelian might adhere to, given their existing belief that objects fall at the same weight:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(4) Natural speed is not physically determinate for strapped-bodies.&lt;/li&gt;
  &lt;li&gt;(5) Weight is not physically determinate for strapped-bodies.&lt;/li&gt;
  &lt;li&gt;(6) Natural speed and weight are mediative for strapped-bodies that are &lt;em&gt;united&lt;/em&gt; (two objects). Natural speed and weight are additive for strapped-bodies that are &lt;em&gt;unified&lt;/em&gt; (one object).&lt;/li&gt;
  &lt;li&gt;(7) Natural speed and weight for strapped-bodies are determined by a &lt;em&gt;degree of connectedness&lt;/em&gt; ($C$) such that the speed/weight of $B_1$-strapped-to-$B_2$ where $B_1$ has $w_1$ and $B_2$ has $w_2$ will be: $C(w_1+w_2)+(1-C)(w_1+w_2)/2$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is, the Aristotelian takes this “degree of connectedness” as being a key relevant property, and thus is able to preserve the idea that speed and weight are proportional. Of course, this claim seems a bit ridiculous, and that is because, as Gendler claims, the thought experiment is revealing to us tacit assumptions that we did not realize we had:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(8) Natural speed and weight are physically determined.&lt;/li&gt;
  &lt;li&gt;(9) Entification [number of objects/entities that something counts as] is not physically determined.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, it is important to note that “prior to the thought experiment, the Aristotelian is explicitly committed to the &lt;em&gt;negation&lt;/em&gt; of (3), and this background commitment serves as a filter through which apparently contrary evidence will inevitably be reinterpreted” (pg. 409). Thus, it does not necessarily follow that anyone given (1) and (2) would reach the conclusion of (3)—particularly if they already thought (3) was false, they would look for ways of explaining away or disproving (1) and (2) so that (3) would remain false.&lt;/p&gt;

&lt;p&gt;Next, Gendler moves on to disproving the Derivativity Thesis by showing that through the thought experiment knowledge has been gained, both in the sense that something new has been learned, and that this new knowledge is justified.&lt;/p&gt;

&lt;p&gt;The first part of this argument (that something new has been learned) is a straightforward claim: presumably, altering one’s beliefs so that they &lt;em&gt;negate&lt;/em&gt; a piece of knowledge counts as having learned something “new”, as it is not simply a combination of previous beliefs. But even more importantly, the Aristotelian is led to think of speed as a different type of concept entirely: rather than being a derivative of weight, it is something else. As Gendler puts it, “it brings the Aristotelian to recognize the inadequacy of his conceptual framework for dealing with phenomena which—through the contemplation of this imaginary case—he comes to recognize as always having been part of his world.” (pg. 412)&lt;/p&gt;

&lt;p&gt;The second part of the argument (that the knowledge is justified) is trickier. Gendler asks, “Why should we think that our pre-theoretical beliefs about the structure of the physical world are reliable?” (pg. 414). She doesn’t really give an answer to this question, but does argue that we do have (implied veridical) knowledge of the world, but it is not accessible by argument alone: it requires something like a thought experiment to tap into it.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a somewhat different take on the notion of simulation than what I’ve been thinking about so far. Although Gendler doesn’t refer to thought experiments as simulations per se, they are related in the sense of being a mental reproduction of something in the world.&lt;/p&gt;

&lt;p&gt;I have more thoughts about this, but I’m too tired tonight to get them in a coherent enough form to write down. I will include them on my notes for the other papers in this topic (thought experiments) tomorrow.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jan 2016 12:31:41 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/08/Gendler1998.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/08/Gendler1998.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>Against simulation: the argument from error</title>
        <description>&lt;p&gt;&lt;span id=&quot;Saxe2005&quot;&gt;Saxe, R. (2005). Against simulation: the argument from error. &lt;i&gt;Trends In Cognitive Sciences&lt;/i&gt;, &lt;i&gt;9&lt;/i&gt;(4), 174–179. doi:10.1016/j.tics.2005.01.012&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Saxe, like &lt;a href=&quot;/quals/theory%20of%20mind/2016/01/08/Gopnik1992.html&quot;&gt;Gopnik &amp;amp; Wellman&lt;/a&gt;, argues in favor of folk psychological theories rather than simulation theory.&lt;/p&gt;

&lt;p&gt;Saxe specifically cautions against the use of the &lt;em&gt;mirror system&lt;/em&gt; as evidence for the simulation theory of mind. The mirror system consists of neurons which are active both when performing an action and watching someone else perform an action. However, the mirror system (e.g., right inferior parietal cortex, inferior frontal gyrus) is &lt;em&gt;not&lt;/em&gt; the same system as the one that is recruited when reasoning about true or false beliefs (bilateral temporo-parietal junction, right anterior superior temporal sulcus, medial prefrontal cortex, posterior cingulate).&lt;/p&gt;

&lt;p&gt;Saxe also recaps some of the behavioral and developmental evidence against the simulation theory, and gives some more recent examples as well. For example,  an adult and a child are sitting at a table with a circular dish of red and green beads, and a square dish of yellow beads. Both the child and the adult see that a bead from the square dish was put in the bag, but only the child knows the color of the bead (green). When the child is asked what color the adult thinks the bead is, children overwhelmingly say “red”. That is, they seem to think something along the lines of, “ignorance means you get it wrong”. Saxe gives an eloquent explanation of how this is inconsistent with the strong notion of simulation theory:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The ‘incorrect inputs’ defence does not work, though, for the systematic errors described above, such as young children’s conflation of ignorance and ‘being wrong’. Remember that children who know that the selected bead is green reported that the ignorant adult observer, ‘A’, thinks the bead is red. If the child were simulating A, she might accurately express A’s ignorance, or else she might assimilate A to the self and judge that A thinks the bead is green. Simulation Theory offers no account of children’s actual systematic error. It is not enough to say that they used incorrect inputs: the theory must explain why the inputs were wrong in just this way.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, Saxe suggest a compromise between theory theory and simulation theory. In this formulation, there is both a theory and a simulator, but that theory determines (or at least influences) what inputs are used in the simulator.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I find the idea that there is really something somewhere in between theory theory and simulation theory more plausible than either on their own, though I would expect the simulation theory not so much to be using the same decision making process that we use when make our own decisions, but a learned generative model of it. One way to think about it might be like this: at any given moment, there are many competing desires and urges governing our behavior (I need to study for quals because they are in less than two weeks, but I want to play video games, and yet I’m also a bit hungry, and also the apartment needs to be cleaned, and ooh, that looks shiny). These all combine in some way and one behavior is ultimately produced (I am studying for quals). In the moment, perhaps, I might be able to explain &lt;em&gt;my own&lt;/em&gt; behavior as being the input that has the highest weight (though I am doubtful this level of introspection exists). But what about explaining my past behavior? I no longer have access to all the inputs, as those presumably reside only in short term memory. Thus, I need to have some function that allows me to reason about &lt;em&gt;my own&lt;/em&gt; behavior with missing information. One possibility for this is to have a full generative (joint) model over actions, desires, beliefs, and perceptions. Then, with such a joint model, any subset of these variables can be conditioned on to produce predictions or inferences. Such a model can also be used to reason about other people, though in that case something additional is needed to choose the appropriate values of things to condition on. Furthermore, the structure of a generative model such as this needs to be learned, which is also where the notion of a theory comes in—a meta form of reasoning that guides the process of constructing, and then using, the model.&lt;/p&gt;

</description>
        <pubDate>Fri, 08 Jan 2016 09:45:02 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/08/Saxe2005.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/08/Saxe2005.html</guid>
        
        
        <category>Theory of mind</category>
        
      </item>
    
      <item>
        <title>Why the child&#39;s theory of mind really is a theory</title>
        <description>&lt;p&gt;&lt;span id=&quot;Gopnik1992&quot;&gt;Gopnik, A., &amp;amp; Wellman, H. M. (1992). Why the Child’s Theory of Mind Really Is a Theory. &lt;i&gt;Mind And Language&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;(1-2), 145–171. doi:10.1111/j.1468-0017.1992.tb00202.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Gopnik &amp;amp; Wellman lay out a wealth of developmental evidence supporting the theory theory, and describe precisely what they take the theory theory to be and what it would predict. They begin with a definition of “theoretical construct”:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Theoretical constructs are abstract entities postulated, or recruited from elsewhere, to provide a separate causal-explanatory level of analysis that accounts for evidential phenomena… Theoretical constructs need not be definitely unobservable, but they must be appeals to a set of entities removed from, and underlying, the evidential phenomena themselves… Theoretical constructs do not work independently, they work together in systems characterized by laws or structure. (pg. 146-7)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are two primary characteristics of theories: they are abstract, and they coherent. Because of these two properties, theories are able to generate predictions about a wide range of behavior, including situations that go beyond the data experienced so far. Theories may also produce incorrect predictions, in cases where they do not perfectly match the phenomena being described. Theories additionally provide &lt;em&gt;explanatory depth&lt;/em&gt; because, by definition, they “produce interpretations of evidence, not simply descriptions of evidence and generalizations about it”. Finally, theories can be modified in the face of new, conflicting evidence.&lt;/p&gt;

&lt;p&gt;Gopnik &amp;amp; Wellman describe theory change as follows. First, there is an initial theory. If evidence is encountered that conflicts with this theory, it will initially be ignored as noise, and eventually start to accumulate as ad-hoc auxiliary rules tacked on to the original theory. Eventually, as the original theory gets unweildy, if a new competing theory is encountered or developed, it may replace the old theory. Elements of the new theory may first appear in the auxiliary rules, but its full predictive power is not utilized until it fully subsumes the old theory.&lt;/p&gt;

&lt;p&gt;Gopnik &amp;amp; Wellman argue that this process of theory change is evident in the development of theory of mind in children, particularly between 2.5 and around 4 years of age. There are several stages of development:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;2-year-olds have some understanding of desires and perceptions. However, these concepts are relatively rudimentary and are “nonrepresentational”: desires are “drives towards object” and perceptions are an “awareness of objects”. This leads to simple causal rules like “if X can see an object, and X desires the object, then X will try to get the object”.&lt;/li&gt;
  &lt;li&gt;At 3, children begin to exhibit some (non-representational) understanding of beliefs, though it does not seem to have much of an effect on their behavior. While it seems that they typically have a notion of belief that directly reflects what is true in the world, there is some evidence that 3-year-olds can acknowledge the idea of a false belief. Even if they might be able to acknowledge the idea of a false belief, though, their theory of how the world works does not include beliefs as a factor in producing actions.&lt;/li&gt;
  &lt;li&gt;By 4 or 5 years, children seem to have developed a “representational theory of mind” in which desires, perceptions, beliefs, and pretenses are all included as &lt;em&gt;representations&lt;/em&gt; of reality, rather than being reflections of reality itself (i.e., &lt;em&gt;intentional&lt;/em&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Gopnik &amp;amp; Wellman next cite considerable evidence supporting this theory:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Explanations&lt;/strong&gt;: 2-year-old’s answers to questions like “why is she doing that?” tend to reflect desires (“she wants it”), while 3- and 4-year-old’s answers reflect beliefs (“she thinks it’s there”).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt;: With the initial desire-perception theory, children should be able to predict that desires will differ and that people will do things given that they have a desire. They should also be able to predict the perceptions of other, but not that things might be perceived &lt;em&gt;differently&lt;/em&gt; by multiple people. And, they should not be able to predict anything that relies on the notion that people have different beliefs that may not reflect the true world. Evidence for this comes from the false-belief task, but other types of tasks as well (appearance-reality tasks, questions about sources of beliefs, understanding pictorial representation systems).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interpretations&lt;/strong&gt;: Also with the initial desire-perception theory, children should initially ignore evidence that is counter to the theory. Indeed, children will misreport evidence they have just heard (e.g. someone saying “I think it is blue”, when it is white).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transitional phenomena&lt;/strong&gt;: Children seem to initially realize that concepts like perception and desire do not necessarily reflect the world (i.e., they &lt;em&gt;misrepresent&lt;/em&gt; what is true) earlier than they come to the same realization about beliefs. When pressed, 3-year-olds may beging to explain inconsistencies in terms of misrepresentation when those inconsistencies are pointed out to them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, Gopnik &amp;amp; Wellman turn to their critique of simulation theory. They focus on two main issues: first, “the centrality of your own mind in any understanding of the minds of others”, and second, “how development should proceed” (pg. 160). The first point is similar to the one made by &lt;a href=&quot;/quals/theory%20of%20mind/2016/01/07/Stich1992.html&quot;&gt;Stich &amp;amp; Nichols&lt;/a&gt; about how a simulation should give the same results for oneself and for others (provided the inputs are correct). For the second point, they argue that ST should predict a developmental trajectory in which children make errors on states that are “hard” to simulate, and that they should originally make “egocentric” errors that reflect an inability to modify their simulations to reflect other people’s states. They give empirical evidence based on these two issues:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;“Three-year-old children make false attributions to themselves, that exactly parallel their false attributions to others.”&lt;/em&gt; From a simulation point of view, it doesn’t make sense why children would make mistakes about their &lt;em&gt;own&lt;/em&gt; mental states if they were just reading off the results from whatever mechanism is used to run the simulations. Additionally, 3-year-olds are good at reporting their mental states in terms of desires and perceptions, but not beliefs. Why would the simulation account predict &lt;em&gt;a priori&lt;/em&gt; for this to be the case? What is special about beliefs in the simulation case that makes them harder to simulate?&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Three-year-old children make correct non-egocentric attributions to themselves and others for some mental states.”&lt;/em&gt; Children can report that other people have different perceptions and desires than their own, so in terms of the simulation account, they are clearly able to adjust their simulations to incorporate other people’s perceptions and desires. Why can they not do the same for beliefs?&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Children refer to only some mental states in their explanations, and refer to different mental states at different stages of their development.”&lt;/em&gt; Younger children tend to give explanations about other people’s behavior in terms of desires, while older children appeal to beliefs. And, in either of these scenarios, children are preferring beliefs in desires over “fears and fantasies, pains and sensations or any of a vast number of experientially available mental states”. What about simulation theory would predict that they answer in this way? Why is “she fears that kitty is lost” a dispreferred explanation to “she wants the kitty” or “she thinks the kitty is there”? In theory theory, desires and beliefs are core constructs, and thus make sense as the types of concepts that children appeal to.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Children’s understanding of other psychological phenomena changes in parallel with their understanding of false belief.”&lt;/em&gt; While three-year-olds can report their beliefs, they seem unable to report &lt;em&gt;how&lt;/em&gt; they got that information, even if it was just told to them. Additionally, three-year-olds seem to be unable to gauge the reliability of a source of information (e.g. distinguishing between someone who knows, vs. someone who is just guessing). Theory theory predicts these other types of behavior because knowing where information comes from and how reliable it is is related to the idea of seeing beliefs as representational—i.e., separate entities from what they correspond to in the world. It is not clear why simulation theory would &lt;em&gt;a priori&lt;/em&gt; predict these types of results.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;In reading this paper I had a bit of a small epiphany regarding why simulation seems to be such a loaded word, and why it seems to be such a complicated issue (not just in terms of theory of mind, but in general).&lt;/p&gt;

&lt;p&gt;Gopnik &amp;amp; Wellman seem to really be proposing what is a &lt;em&gt;computational-level&lt;/em&gt; analysis of theory of mind: we begin with theories based on our initial capabilities (i.e., perception) and urges (i.e., desires). The problem is then to update those theories as new information comes along. The specific changes that Gopnik &amp;amp; Wellman predict are really predictions about how a particular solution to that problem ends up playing out.&lt;/p&gt;

&lt;p&gt;The simulation theorists (&lt;a href=&quot;/quals/theory%20of%20mind/2016/01/07/Gordon1992.html&quot;&gt;Gordon&lt;/a&gt; and &lt;a href=&quot;/quals/theory%20of%20mind/2016/01/08/Goldman1992.html&quot;&gt;Goldman&lt;/a&gt;) are proposing a squarely &lt;em&gt;algorithmic-level&lt;/em&gt; account. It is highly mechanistic and focuses on the solution as a particular algorithm (the one that is already implemented by our brains), though it is vague on the specific representation of inputs and outputs (beliefs, desires, perceptions, decisionse—but it is not clear exactly what forms those take).&lt;/p&gt;

&lt;p&gt;In constrast, simulation &lt;em&gt;can&lt;/em&gt; actually be the (approximate) solution to a computational-level problem. In physical simulation, the computational-level problem can be expressed analytically as a differential equation, but it cannot be solved analytically. Numerical simulation, though an approximation, is the only known way to solve the problem short of setting up the actual physical situation in the real world. In probabilistic simulation, the story is similar. The computational-level problem can be expressed analytically, but often cannot be solve analytically. Monte-Carlo simulation is one particular class of methods for approximating the solution.&lt;/p&gt;

&lt;p&gt;Thus, sometimes simulation is the correct approach when it is the approximate solution to a computational-level problem. However, it is not &lt;em&gt;always&lt;/em&gt; the right answer, and is certainly not the right solution &lt;em&gt;a priori&lt;/em&gt;. In particular, I think it is a mistake to talk about simulation as being the right solution just on the basis of having a black-box mechanism at our disposal. Just because a tool might be available for use doesn’t make it the &lt;em&gt;right&lt;/em&gt; tool to use. I think instead of talking about whether simulation is the mechanism that is used by the mind in some particular case, the discussion should be about what the &lt;em&gt;best&lt;/em&gt; thing to use would be, and whether that happens to be simulation or not.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jan 2016 06:10:48 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/08/Gopnik1992.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/08/Gopnik1992.html</guid>
        
        
        <category>Theory of mind</category>
        
      </item>
    
      <item>
        <title>In defense of the simulation theory</title>
        <description>&lt;p&gt;&lt;span id=&quot;Goldman1992&quot;&gt;Goldman, A. I. (1992). In Defense of the Simulation Theory. &lt;i&gt;Mind And Language&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;(1-2), 104–119. doi:10.1111/j.1468-0017.1992.tb00200.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Goldman replies to &lt;a href=&quot;/quals/theory%20of%20mind/2016/01/07/Stich1992.html&quot;&gt;Stich &amp;amp; Nichols&lt;/a&gt;, conceding some of the points they make but also providing further evidence for simulation theory.&lt;/p&gt;

&lt;p&gt;Goldman begins by first pointing out that the simulation theory isn’t a substantial departure from previous approaches in cognitive science, as it falls under the umbrella of &lt;em&gt;knowledge-poor&lt;/em&gt; approaches (e.g. heuristics and biases) in contrast to &lt;em&gt;knowledge-rich&lt;/em&gt; approaches (e.g. rules and symbols). Thus, the simulation theory isn’t a “radical departure” from other paradigms in cognition.&lt;/p&gt;

&lt;p&gt;In the next section, Goldman points out that theory theory, as described by Stich &amp;amp; Nichols, depards substantially from other formulations of the theory theory. For example, he states that “in the philosophical literature it has been widely assumed that it should be easy to formulate the principles of folk psychology because they are &lt;em&gt;platitudes&lt;/em&gt;, i.e. truths that are obvious to everyone” (pg. 106). Additionally, Goldman discusses “the assumption that folk psychological platitudes are culturally produced and culturally transmitted” (pg. 106-7). Both of these claims seem dubious, and presumably Stich &amp;amp; Nichols woult not adhere to either of them. Thus, their definition of theory theory isn’t the same as everyone else’s. So, while criticisms of claims like those don’t necessarily “knock-down” the theory theory, “these arguments do cast doubt on some popular variations of the theory-theory theme, and highlight the difficulties that must be met by any detailed development of the theory-theory” (pg. 108).&lt;/p&gt;

&lt;p&gt;Next, Goldman discusses the relationship to simulations in other domains (i.e., mental imagery). Here he makes the distinction between &lt;em&gt;process-driven&lt;/em&gt; and &lt;em&gt;theory-driven simulation&lt;/em&gt;, and concedes that mental simulation may indeed be theory-driven; thus, it is necessary to show that the simulation theory is process driven, not just that it is a simulation. This leads into a discussion on introspection, which cannot on its own be used to discriminate between process- and theory-driven simulations. Goldman argues that, for the present purposes, the point of introspection isn’t to distinguish between the two, just to show that theory of mind may indeed involve some form of simulation (as a first step).&lt;/p&gt;

&lt;p&gt;Goldman next responds to Stich &amp;amp; Nichols point about simplicity, which was that the theory theory gets the control mechanism “for free” while simulation theory gets the database “for free”. He makes a good point that it isn’t entirely clear what “for free” means, and argues that simulation theory does indeed get the control “for free” by arguing that the process that interprets the output of the decision-making process needs to be present for any theory, and thus it would indeed be available to off-line simulation as well. (I think Goldman misses the point of what “control” means here, but I’ll get back to this in the takeaways section).&lt;/p&gt;

&lt;p&gt;Next, Goldman discusses additional evidence from autism for simulation theory. Specifically, that there is evidence that autistic children are perfectly capable at theorizing about mechanistic or behavioral processes (just not mentalistic ones), and that they do have a concept of desire, but that they have difficulty evaulating what is a “reasonable” desire based on the situation. He concludes that these pieces of evidence are easily explained by simulation theory, but not theory theory.&lt;/p&gt;

&lt;p&gt;He also discusses the developmental literature discussed by Stich &amp;amp; Nichols, and argues that the results they cite are contradicted by other studies and may be due to confusing task demands. He also suggests the possibility that younger children may understand beliefs, but do not classify those beliefs as being the same as &lt;em&gt;knowledge&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Finally, Goldman replies to Stich &amp;amp; Nichols’ arguments about cognitive penetrability, and in particular, the claim that simulations should predict the same impenetrable behaviors that people exhibit when they make predictions about others. Goldman argues that this only should be the case if the inputs to the simulation are &lt;em&gt;identical&lt;/em&gt; to when people are in the situation themselves, and only if the interpretation of those simulations is identical as well. He questions whether this would actually be the case in the examples cited by Stich &amp;amp; Nichols.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I like Goldman’s further characterization of simulation as being either &lt;em&gt;theory-driven&lt;/em&gt; or &lt;em&gt;process-driven&lt;/em&gt;. That is, the simulation is either driven from a theory (and therefore is something like a simulation of a mental model), or it is actually an execution of a real process in the brain. In the latter case I wonder if it is even really appropriate to call it “simulation”, as it’s not a &lt;em&gt;simulation&lt;/em&gt; of the process by which you would act, it &lt;em&gt;is&lt;/em&gt; the process by which you would act. A &lt;em&gt;simulation&lt;/em&gt; actually implies that it is a copy of a process that necessarily makes certain assumptions and simplifications. I guess it is a simulation in the sense that the real process is being used in the &lt;em&gt;context&lt;/em&gt; of a simulation (i.e., pretend inputs).&lt;/p&gt;

&lt;p&gt;In discussing the role of the controller, I think Goldman misses the point of why this is important. I made this point in my notes on &lt;a href=&quot;/quals/theory%20of%20mind/2016/01/07/Gordon1992.html&quot;&gt;Gordon&lt;/a&gt; as well: knowing what “pretend” inputs to feed the simulation is a nontrivial task. It cannot simply be a matter of trial-and-error. There has to be some additional mechanism guiding the decisions to change certain dimensions of the input rather than others, and what they get changed &lt;em&gt;to&lt;/em&gt;. This is related to the difficult problem of determining what information is relevant and what information is irrelevant; knowing the answer to these questions requires some sort of higher-level abstract knowledge (like a theory!).&lt;/p&gt;

&lt;p&gt;I do think Gordon brings up some good points regarding empathy. For example, people often feel physiological pain when they see someone else in pain (e.g. if you watch a video of someone cutting their finger with a knife while cutting vegetables, it &lt;em&gt;feels&lt;/em&gt; painful to see). Thus, it is not implausible that there is some amount of processing that goes on that engages the same systems that we use when where are in the situation ourselves. However, I don’t think this can be the only explanation. There are also cases where we understand someone else’s behavior while feeling relatively apathetic about it. If I simply read “John cut his finger”, I don’t feel physical pain, but if I see a video of the same thing happening, I do (though, of course, even that is still not as painful as actually experiencing it). But, I still understand how John is feeling in either case. Based on the arguments of Gordon and Goldman, I don’t see how just reading “John cut his finger” would be something that could be fed into the simulator without actually constructing the full scenario (i.e. putting it into a format that the simulation can handle), in which case it ought to predict that you would feel pain.&lt;/p&gt;

&lt;p&gt;This whole debate seems so black-and-white, though. Why does it have to be &lt;em&gt;only&lt;/em&gt; theory-theory, or &lt;em&gt;only&lt;/em&gt; simulation theory? Is it really so impossible for there to be some combination of using higher-order structured knowledge (i.e., a theory) in combination with reuse of existing perceptual, motor, or emotional systems (i.e., simulation)?&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jan 2016 03:29:43 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/08/Goldman1992.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/08/Goldman1992.html</guid>
        
        
        <category>Theory of mind</category>
        
      </item>
    
      <item>
        <title>Folk psychology: simulation or tacit theory?</title>
        <description>&lt;p&gt;&lt;span id=&quot;Stich1992&quot;&gt;Stich, S. P., &amp;amp; Nichols, S. (1992). Folk Psychology: Simulation or Tacit Theory? &lt;i&gt;Mind And Language&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;(1-2), 35–71. doi:10.1111/j.1468-0017.1992.tb00196.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Stich &amp;amp; Nichols argue against simulation theory in favor of theory theory. They first give an overview of their interpretation of what simulation theory is positing, which seems consistent with what I’ve read so far. They make a particular point about calling the simulation theory &lt;em&gt;off-line simulation&lt;/em&gt; of the decision-making system. That is, they argue that simulation theory says that to run a simulation, the decision-making system is taken off-line. It is fed pretend beliefs and desires, and it outputs a decision, which is then fed to another system which explains and interprets decisions in terms of their inputs. They also clarify what they mean by “theory”, which is (as they put it) in the “wide” sense: that a theory need not necessarily be symbolic/logical, but could take the form of other representations like a neural network as well.&lt;/p&gt;

&lt;p&gt;Stich &amp;amp; Nichols go through a number of arguments for the simulation theory, and give counter evidence to each one.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;What are the rules of the folk psychological theory?&lt;/em&gt; This is interpreting a “theory” in the narrow sense. And besides, not being able to specify the rules doesn’t make the theory invalid (example: language, folk physics).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Mental simulation models have been successful.&lt;/em&gt; “Simulation” used in the context of “mental simulation” is actually closer to a theory that is ust not rule/sentence like.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Children can’t possibly have as sophisticated a theory as theory theory suggests.&lt;/em&gt; Theory theory doesn’t necessarily posit a theory that is the same type of stuff that you learn in the classroom. Besides, children learn other very complicated things (language, folk physics)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Simulation theory is simpler than theory theory.&lt;/em&gt; Specifying the “control mechanism” for coming up with pretend beliefs and desires is non-trivial. Essentially, in theory-theory, you get the control for free, while in simulation theory, you get the “database” of knowledge for free. They are equally complex theories.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Sometimes we imagine how others behave by imagining ourselves in their situation.&lt;/em&gt; Imagery is not the same thing as simulation. For example, to imagine yourself walking through your house and counting the number of windows, it’s not just running an off-line simulation—the knowledge about the number of windows has to come from somewhere.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Simulation theory is supported by developmental evidence.&lt;/em&gt; It is consistent with the evidence; that doesn’t it predicts it. Theory theory is also consistent with the evidence.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Autistic children are poor at engaging in pretend play and at reasoning about theory of mind.&lt;/em&gt; As in the previous point, simulation might be able to account for this, but so can theory theory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After refuting the arguments of off-line simulation theory, Stich &amp;amp; Nichols present a few arguments in favor of theory theory:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;There is developmental data that simulation theory can’t explain.&lt;/em&gt; In particular, there is evidence that children have accurate knowledge about what other children have seen and yet still make incorrect judgments about their beliefs. That is, it seems like children have all the correct inputs that would be used to run an off-line simulation, yet they still come up with the wrong answer, but only for &lt;em&gt;other&lt;/em&gt; people—they give a different answer themselves.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Theory theory is cognitively penetrable.&lt;/em&gt; Simulation theory is not cognitive penetrable. In particular, because it’s a simulation of one’s own behavior, if there are any quirks in our own behavior that we don’t consciously know about, they should still show up in our predictions of other people’s behavior. However, if the theory posited by theory theory doesn’t include these things explicitly, then it will make the wrong predictions because it is cognitively penetrable. There are many examples of cases where people predict that they (or others) would behave in a certain way, but then actually behave differently themselves. Thus the can’t have just been running a simulation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I agree with pretty much all the points made by Stich &amp;amp; Nichols, though I do think they are a bit vague on the notion of what a theory is. Ideally, even if you can’t specify a set of rules, you should still be able to roughly sketch the mechanism by which it works, or by which it is acquired.&lt;/p&gt;

&lt;p&gt;I do like how Stich &amp;amp; Nichols explicitly contrast offline simulation with other forms of simulation. They state that they don’t think offline simulation is involved at all, which I would probably agree with—though I do think some form of “simulation” probably is used. Perhaps the better term to use here is “emulation” (ala &lt;a href=&quot;/quals/mental%20imagery/2016/01/01/Grush2004.html&quot;&gt;Grush&lt;/a&gt;), to imply that it is a simulation from a &lt;em&gt;model&lt;/em&gt;, not a simulation from &lt;em&gt;oneself&lt;/em&gt;. Such a model could easily be a component in a theory; thus, this notion isn’t at odds with theory theory.&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Jan 2016 11:25:00 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/07/Stich1992.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/07/Stich1992.html</guid>
        
        
        <category>Theory of mind</category>
        
      </item>
    
  </channel>
</rss>
