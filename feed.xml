<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 11 Jan 2016 14:34:25 -0800</pubDate>
    <lastBuildDate>Mon, 11 Jan 2016 14:34:25 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>On the psychology of prediction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kahneman1973&quot;&gt;Kahneman, D., &amp;amp; Tversky, A. (1973). On the psychology of prediction. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;80&lt;/i&gt;(4), 237–251. doi:10.1037/h0034747&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kahneman &amp;amp; Tversky discuss the &lt;em&gt;representativeness&lt;/em&gt; heuristic, in which people seem to evaluate evidence based on how representative it is of an outcome rather than based on the posterior probability of the outcome given the evidence. The present results from several experiments illustrating this effect, and demonstrating its robustness to changes in wording, reliability of evidence, and prior probability.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In the first experiment, participants were given vignettes such as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tom W. is of high intelligence, although lacking in true creativity. He has a need for order and clarity, and for neat and tidy systems in which every detail finds its appropriate place. His writing is rather dull and mechanical, occasionally enlivened by somewhat corny puns and by flashes of imagination of the sci-fi type. He has a strong drive for competence. He seems to have little feel and little sympathy for other people and does not enjoy interacting with others. Self-centered, he nonetheless has a deep moral sense.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Participants in one group rated the base rates of graduate student disciplines (and did not see the vignette). In another group, participants rated how similar the vignette description was to the average graduate student in each discipline. In the third group, participants rated the likelihood that Tom W. was a graduate student in each discipline. Overwhelmingly, participants in the third group ignored the base rate evidence and gave responses similar to the second group. In a follow-up experiment, they had two groups which were told the reliability of predictions of “students like themselves”. This reliability did have an effect on their likelihood ratings, though it did not bring the ratings any close to the base rate. In contrast, if particpants were given no information at all, they reverted to using the base rate.&lt;/p&gt;

&lt;p&gt;In another experiment, they focused on manipulating the base rate: for example, giving descriptions and asking how likely the description was of an engineer or a lawyer, where the number of engineers and lawyers was manipulated. However, changing this proportion did not seem to have an effect: people still seemed to report what was essentially the likelihood, rather than the posterior. Interestingly, when given &lt;em&gt;worthless&lt;/em&gt; information (as opposed to &lt;em&gt;no&lt;/em&gt; information), people judged the probability at 50%, rather than reverting the the prior. (Side note: if you gave people worthless information and they had three categories to choose from, would they give the probability as 50% still or 33%?)&lt;/p&gt;

&lt;p&gt;Kahneman &amp;amp; Tversky also looked at “prediction of outcome” (e.g. predicting GPA from a description) versus “evaluation of inputs” (e.g. rating impressions from a description) versus “translation” (e.g. translating an input variable on one scale to another scale). According to the representativeness heuristic, predictions and evaluations should be essentially the same; according to statistics, however, the predictions are more unreliable and thus should exhibit regression to the mean. Similarly, the representativeness heuristic predicts that predictions are no more regressive than translations, even though statistics predicts otherwise.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The results presented by Kahneman &amp;amp; Tversky are incredibly compelling: despite knowing what the correct calculation to do is, it &lt;em&gt;still&lt;/em&gt; feels more “natural” to respond in the manner that they equate with representativeness. &lt;a href=&quot;http://web.mit.edu/cocosci/Papers/cogsci01_final.pdf&quot;&gt;Tenenbaum &amp;amp; Griffiths&lt;/a&gt; (2001) cast representativeness in a different light from the notion of “similarity” discussed by Kahneman &amp;amp; Tversky, defining representativeness as being a measure of how good of an example something is. Formally, they define it as the log ratio of the likelihood of the current hypothesis to all other hypotheses:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(d, h_i)=\log\frac{P(d\vert h_i)}{\sum_{h_j\neq h_i} P(d\vert h_j)P(h_j\vert \bar{h}_i)}&lt;/script&gt;

&lt;p&gt;where $\mathcal{H}$ is the set of all hypotheses under consideration and $P(h_j\vert \bar{h}_i)$ is the prior probability of $h_j$ given that $h_i$ is not the true explanation of $d$ (i.e. $P(h_j)/(1-P(h_i))$). I don’t know if this particular model has been applied specifically to the Kahneman &amp;amp; Tversky experiments here, though. I think you probably couldn’t apply them directly because the report similarity ranks rather than real likelihood scores.&lt;/p&gt;

&lt;p&gt;I do find it interesting that people are interpreting the phrasing of the problem to be asking for something along the lines of “representativeness” rather than “posterior probability”. Even if you can quantify what “representativeness” means, it doesn’t explain &lt;em&gt;why&lt;/em&gt; people interpret the question that way (and not the way it is meant to be interpreted).&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 05:39:59 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/challenges%20for%20probabilistic%20models%20of%20cognition/2016/01/11/Kahneman1973.html</guid>
        
        
        <category>Challenges for probabilistic models of cognition</category>
        
      </item>
    
      <item>
        <title>Theory learning as a stochastic search in a language of thought</title>
        <description>&lt;p&gt;&lt;span id=&quot;Ullman2012&quot;&gt;Ullman, T. D., Goodman, N. D., &amp;amp; Tenenbaum, J. B. (2012). Theory learning as stochastic search in the language of thought. &lt;i&gt;Cognitive Development&lt;/i&gt;, &lt;i&gt;27&lt;/i&gt;(4). doi:10.1016/j.cogdev.2012.07.005&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Ullman et al. suggest an algorithm-level approach for theory learning. In contrast to &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Griffiths2009.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt; and &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Kemp2010.html&quot;&gt;Kemp et al.&lt;/a&gt;, which proposed computational-level models for theory learning, Ullman et al. build on previous work (i.e., they still use a hierarchical Bayesian model) and demonstrate that MCMC-like search procedure can account for similar learning dynamics as are found in children.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Ullman et al. express theories using Horn clauses, which are logical expressions of the form $r\leftarrow (f\wedge g\wedge\ldots{}\wedge s\wedge t)$ where each term is a predicate such as $f(X)$ or $s(X, Y)$, where $X$ and $Y$ are entities in the domain.&lt;/p&gt;

&lt;p&gt;A theory $T$ is produced from a &lt;em&gt;univeresal theory grammar&lt;/em&gt; $U$, which is a probabilistic context-free Horn clause grammar (PHCG). This grammar includes what they call &lt;em&gt;law templates&lt;/em&gt; which encode assumptions for what types of laws are good. For example, one template is $F(X,Y)\leftarrow F(X,Z)\wedge F(Z,Y)$ which captures transitivity. The $F$ could then be replaced with something like $\mathrm{interacts}(\cdot{})$.&lt;/p&gt;

&lt;p&gt;Together, the grammar and the templates specify the prior $p(T\vert U)$ over theories. A particular theory—for example, in the magnetism domain—might have core predicates $f(X)$ and $g(X)$, surface predicates of $interacts(X, Y)$, and laws such as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathrm{interacts}(X, Y)&amp;\leftarrow f(X)\wedge f(Y)\\
\mathrm{interacts}(X, Y)&amp;\leftarrow f(X)\wedge g(Y)\\
\mathrm{interacts}(X, Y)&amp;\leftarrow \mathrm{interacts}(Y, X)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Given a theory, a model $M$ can be produced which fits the data. For example, in the magnetism domain, the model might assign magnets to $f(X)$ and magnetic objects to $g(X)$ (and everything else unassigned). The prior on models $p(M\vert T)$ is a beta prior (per binary fact).&lt;/p&gt;

&lt;p&gt;Finally, the data $D$ are actual observations, for example, “this object interacts with this other object”. The likelihood $p(D\vert M, T)$ is Bernoulli.&lt;/p&gt;

&lt;p&gt;Thus the full model is something like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
T\vert U &amp;\sim \mathrm{PHCG}(U)\\
M^i &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
D^{ij} &amp;\sim \mathrm{Bernoulli}(M^i)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the posterior is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(T\vert D, U)\propto P(T\vert U)\sum_M p(D\vert M, T)p(M\vert T)&lt;/script&gt;

&lt;p&gt;To approximate the sum, they use Gibbs sampling. To search in the space of theories, they use a modified version of MCMC where the acceptance ratio is annealed, so that changes are less likely to be made as time goes on. They propose new theories at each step of the chain with the following possible modifications:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add or delete a predicate from a law&lt;/li&gt;
  &lt;li&gt;Change one predicate to an alternative of the same or different form within a law&lt;/li&gt;
  &lt;li&gt;Add or delete a whole law&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This approach of performing a stochastic search in theory space seems generally plausible as a mechanism for how children learn theories. However, I think there are a few things here that aren’t fully satisfing:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The MCMC chain takes many iterations to run: for example, 1600 in the magnetism case. I realize you can’t exactly map inference (“thinking”) time directly to MCMC steps, but it seems like a lot. If this is a mechanistic account, then it’s essentially saying that children propose 1600 different theories before they reach the final one.&lt;/li&gt;
  &lt;li&gt;Children presumably don’t do inference over the entire dataset at once; I would expect there to be some sort of online learning going on. Ullman et al. do investigate the effect of varying amounts of data on the learning process, but they use separate datasets rather than looking at an online method.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Related to the last point, I suppose there are two options for what people do: either they use some memoryless inference procedure, or they do inference over a set of data, but I would expect that to not include &lt;em&gt;everything&lt;/em&gt; they’ve ever encountered. There will be some effect of memory (perhaps remembering data points that were surprising, and more recent data points).&lt;/p&gt;

&lt;p&gt;I wonder if, rather than using a random MCMC algorithm, an active search or Bayesian optimization type of algorithm might work better here in terms of sampling efficiency.&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 03:10:05 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/11/Ullman2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/11/Ullman2012.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>A probabilistic model of theory formation</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kemp2010&quot;&gt;Kemp, C., Tenenbaum, J. B., Niyogi, S., &amp;amp; Griffiths, T. L. (2010). A probabilistic model of theory formation. &lt;i&gt;Cognition&lt;/i&gt;, &lt;i&gt;114&lt;/i&gt;(2), 165–196. doi:10.1016/j.cognition.2009.09.003&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kemp et al. describe a method for clustering entities based on  relations, rather than features. They argue that this clustering algorithm can be thought of as a model of theory formation in people, reflecting how people learn and organize concepts. In particular, they focus on the idea of &lt;em&gt;framework theories&lt;/em&gt;, which “specify the fundamental concepts that exist in a domain and the possible relationships between these concepts” (pg. 166). They ask three fundamental questions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;What are theories?&lt;/strong&gt; Framework theories are “represented as a probabilistic model which includes a set of categories and a matrix of parameters specifying relationships between those categories” (pg. 166)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How are theories used to make inductive inferences?&lt;/strong&gt; “Each of our theories specifies the relationships between categories that are possible or likely, and predictions about unobserved relationships between entities are guided by inductive inferences about their category assignments” (pg. 166)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How are theories acquired?&lt;/strong&gt; Kemp et al. consider the case of acquiring both the concepts and causal laws of a theory simultaneously. “Given a formal characterization of a theory, we can set up a space of possible theories and define a prior distribution over this space” (pg. 167)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They apply their model (described below) to several different real-world datasets: clustering animals and features, clustering medical terms and predicates, and recovering kinship categories. They also run two experiments in which they show how their model matches the way that people learn causal theories of physical relationships.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, Kemp et al. had people play with a set of blocks. Each block either was a member of category $A$ or category $B$, and depending on the condition, the relationship between $A$ and $B$ was either: that $A$ blocks caused $B$ blocks to light up; that $A$ blocks caused $B$ blocks to light up and vice versa; that $A$ blocks caused $B$ blocks to light up with $p=0.5$; and that $A$ blocks caused $B$ blocks to light up with $p=0.5$ and vice versa. Note that &lt;em&gt;specific&lt;/em&gt; blocks were deterministic, $p$ was used to determine which pairs of blocks would affect each other. Participants got experience playing with the blocks and were periodically asked to predict what would happen if a new block was touched to an old block. They then saw the new block touch a different old block, and were asked the same question.&lt;/p&gt;

&lt;p&gt;Experiment 2 was similar to Experiment 1, except that only the relationship where $A$ blocks caused $B$ blocks to light up and vice versa was used. After getting experience with the blocks, participants were shown three new blocks, $x$, $y$, and $z$, where either $x$ and $z$ were in the same category and $y$ was in a different category, or where $y$ and $z$ were in the same category and $x$ was in a different category. By seeing that $x$ interacts with $y$ and that $z$ interacts with $y$, participants should (and do) infer that $x$ and $z$ are in the same category—but they shouldn’t know which one. After seeing $z$ interact (or not interact) with an old block, participants should (and do) infer which categories $x$ and $y$ are in.&lt;/p&gt;

&lt;p&gt;The model matches people’s behavior in both of these experiments. In contrast, particularly in Experiment 2, a purely feature-based categorization model (which doesn’t take into account relations) does not appropriately generalize.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Let the observed data include $m$ relations ($R$) over $n$ types. Then, a relational system is characterized by a pair $(z, \eta)$, where $z$ is a partition of entities into categories and $\eta$ is a matrix of parameters specifying how the categories interact with each other for a relation $R$ (a.k.a. a &lt;em&gt;category graph&lt;/em&gt; where the edge from $A$ to $B$ has weight $\eta(A,B)$). Then, the theory that best characterizes the data is given by the MAP estimate of:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z,\eta\vert R)\propto p(R\vert \eta,z)p(\eta\vert z)p(z)&lt;/script&gt;

&lt;p&gt;where $p(R\vert \eta,z)$ is the probability of observing the relations given categories and parameters, $p(\eta\vert z)$ is the probability of the parameters given the partition, and $p(z)$ is the prior over category assignments. More formally:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z\vert\gamma &amp;\sim \mathrm{CRP}(\gamma)\\
\eta(A, B)\vert\alpha,\beta &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
R(i, j)\vert z, \eta &amp;\sim \mathrm{Bernoulli}(\eta(z_i, z_j))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In this case, there is one relation and one type (though multiple entities within that type); however, in the general case, there are multiple relations with $m$ dimensions and $n$ types. If $T^i$ is the $i$th type, then let $z^i$ be the partition into category assignments for $T^i$. If $R^j$ is the $j$th relation, then let $\eta^j$ be a separate parameter matrix for each $R^j:T^{d_1}\times T^{d_2}\times \ldots{}\times T^{d_m}$, where $d_k$ is the type corresponding to the $k$th dimension. The full general specification is then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z^i\vert\gamma &amp;\sim \mathrm{CRP}(\gamma)\\
\eta^j(z_{x_1}^{d_1},z_{x_2}^{d_2}\ldots{},z_{x_m}^{d_m})\vert\alpha,\beta &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
R^j(x_1,x_2,\ldots{},x_m)\vert z^1,\ldots{},z^n, \eta^j &amp;\sim \mathrm{Bernoulli}(\eta^j(z_{x_1}^{d_1},z_{x_2}^{d_2}\ldots{},z_{x_m}^{d_m}))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;To fit the model, they first integrate out $\eta$ and find the best category assignments $z$ using a hill-climbing procedure or MCMC. Given $z$, the MAP parameter assignments $\eta$ can be computed analytically. Similarly, unobserved relations between specific entities can be predicted given $z$.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This framework seems to be a generalization of the one described by &lt;a href=&quot;/quals/theory%20learning/2016/01/10/Griffiths2009.html&quot;&gt;Griffiths &amp;amp; Tenenbaum&lt;/a&gt; — for example, Kemp et al.’s experiments here are very similar to the “blicket detector” paradigm. However, Griffiths &amp;amp; Tenenbaum only consider there to be blickets and non-blickets. In this case, the number of types is left unspecified. Actually, it seems not so much that this is a generalization of Griffiths &amp;amp; Tenenbaum, but that it’s a more abstract sort of learning. This clusters entities and relations into an ontology that could be used by the framework described by Griffiths &amp;amp; Tenenbaum, but it’s not necessarily a &lt;em&gt;causal&lt;/em&gt; ontology.&lt;/p&gt;

&lt;p&gt;I am less clear on exactly how this type of framework for theories relates to that discussed in the other paper by &lt;a href=&quot;/quals/theory%20learning/2016/01/09/Kemp2007.html&quot;&gt;Kemp et al.&lt;/a&gt;. In that paper, they discuss how to parse objects into different ontological kinds, which is essentially a type of feature-based clustering. I suppose that model is something that is meant to be applied a more domain-specific level—e.g. perhaps within a single category of the kind that is discovered by the present paper. It would be nice to see a tighter coupling between these different types of ontologies and ways of learning theories.&lt;/p&gt;

</description>
        <pubDate>Sun, 10 Jan 2016 11:32:56 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Kemp2010.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Kemp2010.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>Theory-based causal induction</title>
        <description>&lt;p&gt;&lt;span id=&quot;Griffiths2009&quot;&gt;Griffiths, T. L., &amp;amp; Tenenbaum, J. B. (2009). Theory-based causal induction. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;116&lt;/i&gt;(4), 661–716. doi:10.1037/a0017201&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Griffiths &amp;amp; Tenenbaum outline a probabilistic framework for causal induction that allows inference to be done over variables, graphs, and theories. The framework has three main components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ontology of entities, properties, and relations (i.e., “how entities are differentiated on the basis of their causal properties”, pg. 663)&lt;/li&gt;
  &lt;li&gt;Plausible relations between entities (i.e., what types of relationships should be considered in a given domain)&lt;/li&gt;
  &lt;li&gt;Functional forms of relations (i.e., the direction in which causes act, and how causes combine)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These three components are implemented in a probabilistic graphical model as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ontology = variables&lt;/li&gt;
  &lt;li&gt;Plausible relations = graph (the &lt;em&gt;structure&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;Functional form = probability distribution (the &lt;em&gt;parameterization&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Given an existing ontology, learning the plausible relations is a type of &lt;em&gt;structure learning&lt;/em&gt;, and learning the functional form is a type of &lt;em&gt;parameter estimation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Griffiths &amp;amp; Tenenbaum note, however, that knowledge of ontologies, plausible relations, and functional forms itself is not something that can be expressed through the framework of graphical models:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Knowledge about the ontology, plausibility, and functional form of causal relationships should influence the prior, likelihood, and hypothesis space for Bayesian inference. However, expressing this knowledge requires going beyond the representational capacities of causal graphical models. Although this knowledge can be &lt;em&gt;instantiated&lt;/em&gt; in a causal graphical model, it generalizes over a set of such models, and thus cannot be &lt;em&gt;expressed&lt;/em&gt; in any one model. (pg. 669)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They argue that this type of knowledge is akin to the notion of an &lt;em&gt;intuitive theory&lt;/em&gt;, and draw the relationship between theories and grammars; causal structures and syntactic structures; and data and sentences. To formalize theories in their framework, they make use of &lt;em&gt;hierarchical Bayesian models&lt;/em&gt; which allow inference to be performed both at the level of the theory as well as the lower levels of causal structures and relationships.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;types-of-functional-forms&quot;&gt;Types of functional forms&lt;/h2&gt;

&lt;h3 id=&quot;noisy-or&quot;&gt;Noisy-OR&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(e^+\vert c; w_0, w_1)=1-(1-w_0)(1-w_1)^c&lt;/script&gt;

&lt;p&gt;where $e^+$ is the presence of the effect, $c$ is the presence/absence of the cause $w_0$ is the probability of the effect in the absence of the cause, and $w_1$ is the probability of the effect given a single cause.&lt;/p&gt;

&lt;h3 id=&quot;noisy-and-not&quot;&gt;Noisy-AND-NOT&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(e^+\vert c; w_0, w_1)=w_0(1-w_1)^c&lt;/script&gt;

&lt;h3 id=&quot;generic&quot;&gt;Generic&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(e^+\vert c^-)&amp;=w_0\\
p(e^+\vert c^+)&amp;=w_1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;continuous-noisy-or&quot;&gt;Continuous Noisy-OR&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda(t)=\sum_i w_i\delta(t, t_i)&lt;/script&gt;

&lt;p&gt;where $w_i$ and $t_i$ are the weight and time associated with the $i$th cause.&lt;/p&gt;

&lt;h2 id=&quot;medical-contingency-data&quot;&gt;Medical contingency data&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for contingency data.&lt;/p&gt;

&lt;h3 id=&quot;ontology&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Chemical}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Gene}$ ($N_G\sim P_G$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Mouse}$ ($N_M\sim P_M$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(\mathrm{Chemical}, \mathrm{Mouse}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Expressed}(\mathrm{Gene}, \mathrm{Mouse}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(C, M) \rightarrow \mathrm{Expressed}(G, M)$, true for all $M$ with probability $p$ for each $C$, $G$ pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Injected}(C, M) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Expressed}(G, M) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, noisy-AND-NOT, or generic functional form, where $w_0$ and $w_1$ are drawn from uniform distributions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;blicket-detection&quot;&gt;Blicket detection&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for small amounts of data.&lt;/p&gt;

&lt;h3 id=&quot;ontology-1&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Block}$ ($N_B\sim P_B$)
    &lt;ul&gt;
      &lt;li&gt;$\mathrm{Blicket}$ ($p$)&lt;/li&gt;
      &lt;li&gt;$\mathrm{NonBlicket}$ ($1-p$)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\mathrm{Detector}$ ($N_D\sim P_D$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(\mathrm{Block}, \mathrm{Detector}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{Detector}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-1&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(B, D, T) \rightarrow \mathrm{Active}(D, T)$ for all $T$ for any $D$ if $B$ is a $\mathrm{Blicket}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-1&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Contact}(B, D, T) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(D, T) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, where $w_0=\epsilon$ and $w_1=1-\epsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the &lt;em&gt;deterministic detector&lt;/em&gt; theory, $\epsilon=0$. In the &lt;em&gt;probabilistic dector&lt;/em&gt; theory, $\epsilon&amp;gt;0$.&lt;/p&gt;

&lt;h2 id=&quot;stick-ball-machine&quot;&gt;Stick ball machine&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes.&lt;/p&gt;

&lt;h3 id=&quot;ontology-2&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Ball}$ ($N_B\sim P_B$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H=\inf$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Moves}(\mathrm{Ball}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{HiddenCause}, \mathrm{Trial}) \rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-2&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Moves}(B_1, T) \rightarrow \mathrm{Moves}(B_2, T)$, true for all $T$ with probability $p$ for each $B_1\neq B_2$ pair&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(H, T) \rightarrow \mathrm{Moves}(B, T)$, each $B$ has an edge from some $H$ with probability $q$. The particular $H$ is chosn according to a Chinese Restaurant Process (i.e. based on number of edges)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-2&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Active}(H, T) \sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Moves}(B_1, T) \sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR, where $w_0=0$ and $w_i=\omega$ for the $i^{th}$ cause (either $B_2$ or some $H$)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lemur-colonies&quot;&gt;Lemur colonies&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes in a spatial domain.&lt;/p&gt;

&lt;h3 id=&quot;ontology-3&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Colony}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H\in [0, 1]$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Location}(\mathrm{Colony}) \rightarrow \mathcal{R}\subset\mathbb{R}^2$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Nexus}(\mathrm{HiddenCause}) \rightarrow \mathcal{R}\subset\mathbb{R}^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-3&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Nexus}(H) \rightarrow \mathrm{Location}(C)$, true with probability $p$ for each $H$, $C$ pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-3&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Nexus}(H) \sim \mathrm{Uniform}(\mathcal{R})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Location}(C) \sim \mathcal{N}(\mathrm{Nexus}(H),\Sigma)$ if $\mathrm{Nexus}(H) \rightarrow \mathrm{Location}(C)$, otherwise $\mathcal{N}((0, 0), \inf)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;exploding-cans&quot;&gt;Exploding cans&lt;/h2&gt;

&lt;p&gt;This case study shows how the framework can account for hidden causes in a spatiotemporal domain.&lt;/p&gt;

&lt;h3 id=&quot;ontology-4&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Can}$ ($N_C\sim P_C$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{HiddenCause}$ ($N_H=\inf$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(\mathrm{Can}) \rightarrow \mathbb{R}+$ (time)&lt;/li&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(\mathrm{HiddenCause}) \rightarrow \mathbb{R}+$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Location}(\mathrm{Can}) \rightarrow \mathbb{R}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-4&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(C_1) \rightarrow \mathrm{ExplosionTime}(C_2)$, true with probability 1 for each $C_1\neq C_2$ pair&lt;/li&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(H)\rightarrow \mathrm{ExplosionTime}(C)$, each $C$ has an edge from some $H$ with probability 1. The particular $H$ is chosn according to a Chinese Restaurant Process (i.e. based on number of edges)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-4&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{ActivationTime}(H)\sim \mathrm{Exponential}(\alpha)$&lt;/li&gt;
  &lt;li&gt;$\mathrm{ExplosionTime}(C_1)\sim \mathrm{Exponential}(\lambda(t))$ for $\lambda(t)$ from a continuous noisy-OR with $w_i=\omega$ for either times $t_i=\mathrm{ActivationTime}(H)$ or $t_i=\mathrm{ExplosionTime}(C_2)+\vert\mathrm{Location}(C_2)-\mathrm{Location}(C_1)\vert/\mu$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cross-domain-causal-induction&quot;&gt;Cross-domain causal induction&lt;/h2&gt;

&lt;h3 id=&quot;ontology-5&quot;&gt;Ontology&lt;/h3&gt;

&lt;p&gt;Types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Cause}$ ($N_C\sim P_C$)
    &lt;ul&gt;
      &lt;li&gt;$\mathrm{InDomain}$&lt;/li&gt;
      &lt;li&gt;$\mathrm{OutDomain}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\mathrm{Effect}$ ($N_E\sim P_E$)&lt;/li&gt;
  &lt;li&gt;$\mathrm{Trial}$ ($N_T\sim P_T$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Predicates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(\mathrm{Cause}, \mathrm{Trial})\rightarrow [0, 1]$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(\mathrm{Effect}, \mathrm{Trial})\rightarrow [0, 1]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;plausible-relations-5&quot;&gt;Plausible relations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(C, T)\rightarrow \mathrm{Active}(E, T)$, true for all $T$ with probability $p$ for each $C$, $E$ pair when $C$ is an $\mathrm{InDomain}$ cause, and with probability $q$ for each $C$, $E$ pair where $C$ is an $\mathrm{OutDomain}$ cause.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;functional-forms-5&quot;&gt;Functional forms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathrm{Present}(C, T)\sim \mathrm{Bernoulli}(\cdot{})$&lt;/li&gt;
  &lt;li&gt;$\mathrm{Active}(E, T)\sim \mathrm{Bernoulli}(\nu)$ for $\nu$ from a noisy-OR with $w_0=\epsilon$ and $w_i=1-\epsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The framework outlined by Griffiths &amp;amp; Tenenbaum is an extremely rich and flexible framework for causal reasoning across a wide range of domains. It would be really interesting to try to apply this to even more complex physical domains—for example, can this framework be extended to explain some of the results from Michotte experiments, such as the launching effect? It’s not immediately clear to me how you would do this, but my guess is that it would be somewhat similar to the exploding can example, which includes both spatial and temporal causes.&lt;/p&gt;
</description>
        <pubDate>Sun, 10 Jan 2016 07:15:53 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Griffiths2009.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/10/Griffiths2009.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>Learning overhypotheses with hierarchical Bayesian models</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kemp2007&quot;&gt;Kemp, C., Perfors, A., &amp;amp; Tenenbaum, J. B. (2007). Learning overhypotheses with hierarchical Bayesian models. &lt;i&gt;Developmental Science&lt;/i&gt;, &lt;i&gt;10&lt;/i&gt;(3), 307–321. doi:10.1111/j.1467-7687.2007.00585.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kemp et al. formulate a hierarchical Bayesian model (HBM) for learning overhypotheses and demonstrate how it can account for two kinds of empirical results (the shape bias, and grouping into ontological kinds).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;The model is parameterized as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbf{z} &amp;\sim \mathrm{CRP}(\gamma)\\
\alpha^k &amp;\sim \mathrm{Exponential}(\lambda)\\
\mathbf{\beta}^k &amp;\sim \mathrm{Dirichlet}(1)\\
\mathbf{\theta}^i &amp;\sim \mathrm{Dirichlet}(\alpha^{z_i}\mathbf{\beta}^{z_i})\\
\mathbf{y}^i\ \vert\ n^i &amp;\sim \mathrm{Multinomial}(\mathbf{\theta}^i)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\mathbf{y}^i$ is the data (e.g. counts of features in category $i$), $n^i$ is the number of observations for category $i$, $\mathbf{\theta}^i$ specifies the distribution of features in category $i$, $\alpha^k$ is the degree of uniformity for features across categories within the ontological kind $z_i$, $\mathbf{\beta}^k$ is the distribution of features across all categories within the ontological kind $z_i$, and $\mathbf{z}$ is the partition (assignment) of categories to ontological kinds.&lt;/p&gt;

&lt;p&gt;Given training data $\mathbf{y}$, the model can be fit to reflect the distribution of features within categories and across categories within a particular ontological kind, and also to reflect the assignment of categories to ontological kinds. Given a test exemplar $T$ (with known category) and three choice objects, the model needs to produce the probability that each choice object belongs to the same category as the exemplar. It’s not entirely clear to me what they specifically compute to get this. One possibility would be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(c_C=i|i)\propto \int p(C|\mathbf{\theta}^i)p(\mathbf{\theta}^i|\mathbf{y}^i)\ \mathrm{d}\mathbf{\theta}^i&lt;/script&gt;

&lt;p&gt;where $c_C$ is the category of the choice object, $C$ are the features of the choice object, and $i$ is the category of the exemplar.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a nice use of a HBM to show how theories/overhypotheses might be implemented in a computational-level framework. I do wonder thought how much of the heavy lifting is being done by the fact that they’ve only included features that could be relevant and they’ve discretized the feature domains. In the real world, things are going to be much more ambiguous than that. First, how does the child determine what features are relevant in the first place (why pay attention to overall shape, rather than ear shape)? Second, what if some dimensions (e.g. size) are continuous values rather than discrete values? In the continuous case, modeling the data as feature counts will not longer work.&lt;/p&gt;

&lt;p&gt;It would be interesting to see if this type of framework could be applied as a way of classifying objects based on how they should be simulated. For example, this might be applicable to first grouping things ontologically (fluids, rigid bodies, soft bodies) which might determine the overall class of simulation that needs to be run. Then individual categories within each ontological kind might determine the types of parameters that need to be put into the simulation (e.g. mass, coefficient of friction, etc.). I’m not sure this is exactly the right framework for making those sorts of approximations but it might be a good place to start for at least looking at how we make the necessary decisions for setting up a simulation.&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 11:05:17 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20learning/2016/01/09/Kemp2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20learning/2016/01/09/Kemp2007.html</guid>
        
        
        <category>Theory learning</category>
        
      </item>
    
      <item>
        <title>Thought experiments</title>
        <description>&lt;p&gt;&lt;span id=&quot;Brown2014&quot;&gt;Brown, J. R., &amp;amp; Fehige, Y. (2014). Thought Experiments. In E. N. Zalta (Ed.), &lt;i&gt;The Stanford Encyclopedia of Philosophy&lt;/i&gt; (Fall 2014). Retrieved from http://plato.stanford.edu/archives/fall2014/entries/thought-experiment/&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This entry in the Stanford Encyclopedia of Philosophy gives an outline of what thought experiments are, the history of thought experiments, and the main schools of thought regarding whether thought experiments are useful and how they work.&lt;/p&gt;

&lt;p&gt;First, while the entry does not give a specific definition of thought experiments, it does describe some common features of them: visualizing a situation in our imagination, “running” it, seeing what happens, and drawing a conclusion. Thought experiments do not necessarily result in some objective truth (e.g., following the thought experiment about throwing a spear through the edge of the universe, we now know of topologies where the space could in fact be simultaneously finite and unbounded, like a circle). Thought experiments frequently cannot be run as real experiments “for physical, technological, ethical, or financial reasons… but this needn’t be a defining condition of thought experiments”.&lt;/p&gt;

&lt;p&gt;One possible taxonomy for thought experiments categorizes them firstly as either &lt;em&gt;constructive&lt;/em&gt; or &lt;em&gt;destructive&lt;/em&gt;. Destructive thought experiments may illustrate a contradiction in a theory (e.g. Galileo’s falling objects), show that the theory is in contradiction with other beliefs (unrelated to the theory) that we hold (e.g. Schroedinger’s cat), undermine a premise of the though experiment itself (e.g. Thomson’s violinist), or slightly modify the original version of the thought experiment in order to produce an outcome which calls the conclusions from the original thought experiment into question. Constructive thought experiments provide positive support for a theory by illustrating the implications of a theory’s claims (e.g. Newton’s cannonball).&lt;/p&gt;

&lt;p&gt;The first stage of philosophical investigation of thought experiments began in the 18th and 19th centuries (Novalis, Hans-Christian Orsted). It was again revived in the beginning of the 20th century, marking the 2nd stage (Duhem, Mach, Meinong), and then again in the first part of the second half of the 20th century (Koyre, Kuhn, Popper), marking the 3rd stage. The current investigation is the 4th stage (Brown, Norton).&lt;/p&gt;

&lt;p&gt;There are a number of prominent views regarding thought experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Skeptical objection&lt;/strong&gt; (e.g. Duhem, Wilkes) — a denial that thought experiments are useful, and that they are no substitute for a real experiment. Most skeptical objections are specific to particular fields, rather than to thought experiments as a whole.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intuition&lt;/strong&gt; (e.g. Brown) — one version of the intuition account is &lt;em&gt;Platonic&lt;/em&gt; intuition, in which the claim is that what is determined in a thought experiment is “a priori (though still fallible) knowledge of nature, since there are no new data involved, nor is the conclusion derived from old data”. The other version is &lt;em&gt;naturalistic&lt;/em&gt; intuition, though it’s not entirely clear to me what this is.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Argument&lt;/strong&gt; (e.g. Norton) — thought experiments are really just inductive or deductive arguments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conceptual constructivism&lt;/strong&gt; (e.g. Van Dyck, Gendler, Camilleri, Kuhn) — thought experiments enable conceptual change; i.e. they “[help] us to re-conceptualize the world in a new way”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Experimentalism&lt;/strong&gt; (e.g. Mach, Sorenson, Buzzoni) — thought experiments are just experiments which generate “uncontrollable images of facts acquired in past experiences with the world”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mental model&lt;/strong&gt; — thought experiments are the manipulation of a non-propositional mental model, rather than a physical model. The mental model account ties into the idea of “literary fiction as thought experiments”. The idea is that fiction brings us to construct a hypothetical scenario and use our imagination to let it play out.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One more recent development in the understanding of thought experiments is the question of whether computer simulations can be thought experiments.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;There is something interesting about thought experiments—and mental simulation in general—in that we can imagine scenarios which are physically impossible and which may violate many natural laws. At the same time, as some of Schwartz’s experiments show, we can’t always use imagery in ways that violate natural laws. So why is it that we can in some cases, but cannot in others? There is something interesting going on between the interplay of actual perceptual or motor imagery (which presumably must approximately conform to experience with the world as it is goverened by our sensory modalities rather than higher level cognition) and the capacity for abstract of symbolic thought. I can &lt;em&gt;conceptualize&lt;/em&gt; the idea of gravity going in the opposite direction, and even run thought experiments regarding what the consequences would be of that. But I can’t do things like &lt;em&gt;visualize&lt;/em&gt; a glass with water in it rotating upside down without the water spilling out. So it seems to me that certain types of thought experiments—particularly ones which violate physical laws—are &lt;em&gt;not&lt;/em&gt; using information from low level sensorimotor processes. Or at least not using the full information the same way. So where does that information come from, then? Do we construct more abstract models about how the world works initially based on low-level information, but which are conceptual/abstract enough to be manipulated during the course of a thought experiment?&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 10:21:27 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Brown2014.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Brown2014.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>The role of imagistic simulation in scientific thought experiments</title>
        <description>&lt;p&gt;&lt;span id=&quot;Clement2009&quot;&gt;Clement, J. J. (2009). The Role of Imagistic Simulation in Scientific Thought Experiments. &lt;i&gt;Topics In Cognitive Science&lt;/i&gt;, &lt;i&gt;1&lt;/i&gt;(4), 686–710. doi:10.1111/j.1756-8765.2009.01031.x&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Clement poses the &lt;em&gt;fundamental paradox of thought experiments&lt;/em&gt; as being “How can findings that carry conviction result from a new experiment conducted entirely within the head?” (pg. 687). He attempts to provide a resolution to this paradox based on the idea of “imagistic simulation” (a.k.a. mental simulation, mental imagery, etc.) with evidence provided through a case study of a single expert subject (S2). S2 is posed with the following “spring problem”:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A weight is hung on a spring (Fig. 1). The original spring is replaced with a spring made of the same kind of wire, with the same number of coils, but with coils that are twice as wide in diameter. Will the spring stretch from its natural length more, less, or the same amount under the same weight? (Assume the mass of the spring is negligible). Why do you think so? (pg. 689)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clement specifies his definition of “thought experiment” (TE) as being “the act of considering an untested, concrete system (the ‘experiment’ or case) and attempting to predict aspects of its behavior” (pg. 690-1). He isolates a number of TEs produced by S2, and analyzes the use of imagistic simulation in those TEs. He finds that S2 spontaneously engaged in “personal action projection (spontaneously redescribing a system action in terms of a human action) consistent with the use of kinesthetic imagery, depictive getures (gestures that depict objects, forces, locations, or movements of entities), and imagery reports” (pg. 694). Some of these are characteristic of using static imagery, but others are characteristic of &lt;em&gt;dynamic&lt;/em&gt; imagery.&lt;/p&gt;

&lt;p&gt;To explain the use of dynamic imagery, Clement appeals to the idea of &lt;a href=&quot;https://en.wikipedia.org/wiki/Motor_program#Schmidt.E2.80.99s_schema_theory&quot;&gt;motor schema theory&lt;/a&gt; in which imagistic simulations are driven by the use of motor programs/schema. Specifically, he identifies four possible components to an imagistic simulation which allow such simulations to apply to situations which have not previously been encountered:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Applying a schema to a use outside its normal domain&lt;/li&gt;
  &lt;li&gt;Converting implicit knowledge into explicit knowledge&lt;/li&gt;
  &lt;li&gt;Including spatial reasoning&lt;/li&gt;
  &lt;li&gt;Combining multiple schemas into a &lt;em&gt;compound simulation&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Additionally, Clement argues that imagistic simulations are used to generate “enhanced” imagery:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…the main source of conviction in the simulations is the tapping of implicit knowledge embedded in motor schemas and its conversion into explicit knowledge. The extreme case makes differences in implicit expectations larger and more ‘perceivable’ in this case. (pg. 698)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clement argues that this formulation of imagistic simulation resolves the TE paradox:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To address the TE paradox, the idea of perceptual motor schemas running imagistic simulations, and the four more specific sources of conviction within imagistic simulations… can account for ways that a TE can &lt;em&gt;feel&lt;/em&gt; empirical (via the inspection of imagery) or necessary (via confident schema extension or spatial reasoning). Yet these processes actually involve a considerable amount of nonformal reasoning and inference that goes beyond prior observations. (pg. 704)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In discussing the case study with S2, Clement also touches on the distinction between evaluative (disconfirmatory/confirmatory) and generative thought experiments.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I like this characterization of what thought experiments (and perhaps, even, mental simulations more generally) are; I think Clement is right in tying the use of thought experiments to action &lt;em&gt;and&lt;/em&gt; perception.&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 07:35:52 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Clement2009.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Clement2009.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>&quot;What if...&quot;: The use of conceptual simulations in scientific reasoning</title>
        <description>&lt;p&gt;&lt;span id=&quot;Trickett2007&quot;&gt;Trickett, S. B., &amp;amp; Trafton, J. G. (2007). “What if…”: The Use of Conceptual Simulations in Scientific Reasoning. &lt;i&gt;Cognitive Science&lt;/i&gt;, &lt;i&gt;31&lt;/i&gt;(5), 843–875. doi:10.1080/03640210701530771&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Trickett &amp;amp; Trafton experimentally explore the use of &lt;em&gt;conceptual simulations&lt;/em&gt; by expert scientists when reasoning about problems in their domain of expertise. They have two main hypotheses: that conceptual simulations are a core strategy used in scientific reasoning, and that they are used in particular to reason about situations in which there are high levels of uncertainty (e.g. partial knowledge, violation of expectation, etc.). They define conceptual simulation as:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…a three-step process that consists of first, visualizing some situation; second, carrying out one or more operations on it; and third, seeing what happens. The third part of the process—seeing what happens—is crucial. It distinguishes “what if” thinking from purely imagining because during this third phase &lt;em&gt;causal reasoning&lt;/em&gt; occurs to the results of the manipulation(s) of the second phase.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In their experiments, Trickett &amp;amp; Trafton found that scientists do spontaneously use conceptual simulation and that they use it in cases where their expectations are violated (i.e. they have more uncertainty):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The research shows how conceptual simulation helps resolve uncertainty: conceptual simulation facilitates reasoning about hypotheses by generating an altered representation under the purported conditions expressed in the hypothesis and providing a source of comparison with the actual data, in the process of alignment by similarity detection. (pg. 866)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Additionally, the results of these experiments combined with other results from the literature suggest that conceptual simulations are used in situations where there the answer truly is unknown. In other cases, people can rely on background knowledge, existing models, etc.:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Frequently, studies of experts employ problems that are well-understood for an expert and that can be solved by recalling either this very problem (i.e., by model-based search) or another that shares the same deep structure (i.e., by analogy; cf. Chi et al., 1981). In contrast, our studies show experts reasoning about problems for which neither they nor anyone else knows the answer. (pg. 867)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That is, conceptual simulation is a type of model &lt;em&gt;construction&lt;/em&gt; (pg. 866).&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In Experiment 1, Trickett &amp;amp; Trafton performed an &lt;em&gt;in vivo&lt;/em&gt; study of scientists across several different domains of science. The scientists were filmed while they analyzed their own data, either individually (with a verbal protocol) or collaboratively. The utterances of the scientists were coded for instances of conceptual simulation, for hypotheses, for and for other scientific reasoning strategies (data focus, empirical test, consult a colleague, tie-in with theory and domain knowledge, analogy, or alignment). They found that data focus was the most commonly used strategy. The next most frequently used strategies were tie-in with theory, alignment, and conceptual simulation; these were used at approximately the same frequency.&lt;/p&gt;

&lt;p&gt;In analyzing the relationship between strategies, they found that conceptual simulations were almost always followed by a process of alignment, which was then usually either the end of the chain of reasoning, or which was followed by a return to data focus. Trickett &amp;amp; Trafton hypothesized that this sequence of conceptual simulation followed by alignment was used “to link the internal (result of the conceptual simulation) and external (phenomena in the data) representations” (pg. 858).&lt;/p&gt;

&lt;p&gt;They additionally coded the data for hypotheses that were generated either based on evidence that violated expectations or which was consistent with expectations. They found that conceptual simulation more frequently followed violation of expectation hypotheses than those that did not have a violation of expectation, suggesting that the scientists used conceptual simulation in situations where they were more uncertain.&lt;/p&gt;

&lt;p&gt;To causally test the previous hypothesis (that conceptual simulations are used in situations with higher uncertainty), Trickett &amp;amp; Trafton ran a second experiment. In Experiment 2, they recruited expert cognitive psychologists and gave them different scenarios and results of phenomena they were familiar with. The results were either consistent with the given scenario (Expectation Confirmation, EC) or inconsistent (Expectation Violation, EV). The scientists were instructed to engage in a process of explaining the data, and again were recorded doing so. Consistent with the results of Experiment 1, they found that conceptual simulations were used more frequently in the EV condition than in the EC condition, at a rate of approximately 2:1.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;These types of “conceptual simulations”, as well as thought experiments like those described by &lt;a href=&quot;/quals/thought%20experiments/2016/01/08/Gendler1998.html&quot;&gt;Gendler&lt;/a&gt;, are really fascinating in that they seem to be qualitatively a very different sort of simulation than, for example, motor simulation or even certain types of mental imagery (like that which is used in language understanding). I think a relevant question is: are such types of conceptual simulations drawing on the same types of simulation processes that serve lower levels of reasoning? I would expect the answer to be “sometimes”, but I don’t have a good intuition for why certain low-level simulations would be available for high-level conceptual reasoning (e.g. imagery) while others wouldn’t (e.g. accurate simulation of physics via the motor system).&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 04:33:40 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Trickett2007.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/09/Trickett2007.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>Galileo and the indispensability of scientific thought experiment</title>
        <description>&lt;p&gt;&lt;span id=&quot;Gendler1998&quot;&gt;Gendler, T. S. (1998). Galileo and the Indispensability of Scientific Thought Experiment. &lt;i&gt;The British Journal For the Philosophy of Science&lt;/i&gt;, &lt;i&gt;49&lt;/i&gt;(3), 397–424.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Gendler argues that thought experiments are important and justified for the use of scientific inquiry, using Galileo’s thought experiment demonstrating that two objects with different masses fall at the same rate. The thought experiment goes, imagine that two objects with different weights are strapped together. Because the lighter object falls slower than the heavier object, it must slow the heavier object down, and thus the speed that they fall together must be somewhere in between the speed of the heavy object alone and the light object alone. But, together, they also have a greater mass, meaning that together they should fall faster than the heavy object alone. Thus, for there to not be a contradiction, the objects must actually fall at the same speed.&lt;/p&gt;

&lt;p&gt;Gendler addresses the claim that thought experiments are just another form of deductive/inductive reasoning from a set of explicit premises, and explains what they add over and above pure argumentation. Specifically, she argues against the &lt;em&gt;Elimination Thesis&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The Elimination Thesis&lt;/strong&gt;: Any conclusion reached by a (successful) scientific thought experiment will also be demonstrable by a non-thought-experimental argument. (pg. 398)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;She breaks the Elimination Thesis down into two separate claims:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The Dispensibility Thesis&lt;/strong&gt;: Any good scientific thought experiment can be replaced, without loss of demonstrative force, by a non-thought-experimental argument. (pg. 401)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The Derivativity Thesis&lt;/strong&gt;: The justificatory force of any good scientific thought experiment can only be explained by the fact that it can be replaced, without loss of demonstrative force, by a non-thought-experimental argument. (pg. 401)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, Gendler rephrases the thought experiment in terms of a few propositions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1) Natural speed is mediative [averaging].&lt;/li&gt;
  &lt;li&gt;(2) Weight is additive.&lt;/li&gt;
  &lt;li&gt;(3) Natural speed is not directly proportional to weight.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, she goes about disproving the Dispensibility Thesis by giving an example set of alternate premises that an Aristotelian might adhere to, given their existing belief that objects fall at the same weight:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(4) Natural speed is not physically determinate for strapped-bodies.&lt;/li&gt;
  &lt;li&gt;(5) Weight is not physically determinate for strapped-bodies.&lt;/li&gt;
  &lt;li&gt;(6) Natural speed and weight are mediative for strapped-bodies that are &lt;em&gt;united&lt;/em&gt; (two objects). Natural speed and weight are additive for strapped-bodies that are &lt;em&gt;unified&lt;/em&gt; (one object).&lt;/li&gt;
  &lt;li&gt;(7) Natural speed and weight for strapped-bodies are determined by a &lt;em&gt;degree of connectedness&lt;/em&gt; ($C$) such that the speed/weight of $B_1$-strapped-to-$B_2$ where $B_1$ has $w_1$ and $B_2$ has $w_2$ will be: $C(w_1+w_2)+(1-C)(w_1+w_2)/2$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is, the Aristotelian takes this “degree of connectedness” as being a key relevant property, and thus is able to preserve the idea that speed and weight are proportional. Of course, this claim seems a bit ridiculous, and that is because, as Gendler claims, the thought experiment is revealing to us tacit assumptions that we did not realize we had:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(8) Natural speed and weight are physically determined.&lt;/li&gt;
  &lt;li&gt;(9) Entification [number of objects/entities that something counts as] is not physically determined.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, it is important to note that “prior to the thought experiment, the Aristotelian is explicitly committed to the &lt;em&gt;negation&lt;/em&gt; of (3), and this background commitment serves as a filter through which apparently contrary evidence will inevitably be reinterpreted” (pg. 409). Thus, it does not necessarily follow that anyone given (1) and (2) would reach the conclusion of (3)—particularly if they already thought (3) was false, they would look for ways of explaining away or disproving (1) and (2) so that (3) would remain false.&lt;/p&gt;

&lt;p&gt;Next, Gendler moves on to disproving the Derivativity Thesis by showing that through the thought experiment knowledge has been gained, both in the sense that something new has been learned, and that this new knowledge is justified.&lt;/p&gt;

&lt;p&gt;The first part of this argument (that something new has been learned) is a straightforward claim: presumably, altering one’s beliefs so that they &lt;em&gt;negate&lt;/em&gt; a piece of knowledge counts as having learned something “new”, as it is not simply a combination of previous beliefs. But even more importantly, the Aristotelian is led to think of speed as a different type of concept entirely: rather than being a derivative of weight, it is something else. As Gendler puts it, “it brings the Aristotelian to recognize the inadequacy of his conceptual framework for dealing with phenomena which—through the contemplation of this imaginary case—he comes to recognize as always having been part of his world.” (pg. 412)&lt;/p&gt;

&lt;p&gt;The second part of the argument (that the knowledge is justified) is trickier. Gendler asks, “Why should we think that our pre-theoretical beliefs about the structure of the physical world are reliable?” (pg. 414). She doesn’t really give an answer to this question, but does argue that we do have (implied veridical) knowledge of the world, but it is not accessible by argument alone: it requires something like a thought experiment to tap into it.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a somewhat different take on the notion of simulation than what I’ve been thinking about so far. Although Gendler doesn’t refer to thought experiments as simulations per se, they are related in the sense of being a mental reproduction of something in the world.&lt;/p&gt;

&lt;p&gt;I have more thoughts about this, but I’m too tired tonight to get them in a coherent enough form to write down. I will include them on my notes for the other papers in this topic (thought experiments) tomorrow.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jan 2016 12:31:41 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/thought%20experiments/2016/01/08/Gendler1998.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/thought%20experiments/2016/01/08/Gendler1998.html</guid>
        
        
        <category>Thought experiments</category>
        
      </item>
    
      <item>
        <title>Against simulation: the argument from error</title>
        <description>&lt;p&gt;&lt;span id=&quot;Saxe2005&quot;&gt;Saxe, R. (2005). Against simulation: the argument from error. &lt;i&gt;Trends In Cognitive Sciences&lt;/i&gt;, &lt;i&gt;9&lt;/i&gt;(4), 174–179. doi:10.1016/j.tics.2005.01.012&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Saxe, like &lt;a href=&quot;/quals/theory%20of%20mind/2016/01/08/Gopnik1992.html&quot;&gt;Gopnik &amp;amp; Wellman&lt;/a&gt;, argues in favor of folk psychological theories rather than simulation theory.&lt;/p&gt;

&lt;p&gt;Saxe specifically cautions against the use of the &lt;em&gt;mirror system&lt;/em&gt; as evidence for the simulation theory of mind. The mirror system consists of neurons which are active both when performing an action and watching someone else perform an action. However, the mirror system (e.g., right inferior parietal cortex, inferior frontal gyrus) is &lt;em&gt;not&lt;/em&gt; the same system as the one that is recruited when reasoning about true or false beliefs (bilateral temporo-parietal junction, right anterior superior temporal sulcus, medial prefrontal cortex, posterior cingulate).&lt;/p&gt;

&lt;p&gt;Saxe also recaps some of the behavioral and developmental evidence against the simulation theory, and gives some more recent examples as well. For example,  an adult and a child are sitting at a table with a circular dish of red and green beads, and a square dish of yellow beads. Both the child and the adult see that a bead from the square dish was put in the bag, but only the child knows the color of the bead (green). When the child is asked what color the adult thinks the bead is, children overwhelmingly say “red”. That is, they seem to think something along the lines of, “ignorance means you get it wrong”. Saxe gives an eloquent explanation of how this is inconsistent with the strong notion of simulation theory:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The ‘incorrect inputs’ defence does not work, though, for the systematic errors described above, such as young children’s conflation of ignorance and ‘being wrong’. Remember that children who know that the selected bead is green reported that the ignorant adult observer, ‘A’, thinks the bead is red. If the child were simulating A, she might accurately express A’s ignorance, or else she might assimilate A to the self and judge that A thinks the bead is green. Simulation Theory offers no account of children’s actual systematic error. It is not enough to say that they used incorrect inputs: the theory must explain why the inputs were wrong in just this way.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, Saxe suggest a compromise between theory theory and simulation theory. In this formulation, there is both a theory and a simulator, but that theory determines (or at least influences) what inputs are used in the simulator.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I find the idea that there is really something somewhere in between theory theory and simulation theory more plausible than either on their own, though I would expect the simulation theory not so much to be using the same decision making process that we use when make our own decisions, but a learned generative model of it. One way to think about it might be like this: at any given moment, there are many competing desires and urges governing our behavior (I need to study for quals because they are in less than two weeks, but I want to play video games, and yet I’m also a bit hungry, and also the apartment needs to be cleaned, and ooh, that looks shiny). These all combine in some way and one behavior is ultimately produced (I am studying for quals). In the moment, perhaps, I might be able to explain &lt;em&gt;my own&lt;/em&gt; behavior as being the input that has the highest weight (though I am doubtful this level of introspection exists). But what about explaining my past behavior? I no longer have access to all the inputs, as those presumably reside only in short term memory. Thus, I need to have some function that allows me to reason about &lt;em&gt;my own&lt;/em&gt; behavior with missing information. One possibility for this is to have a full generative (joint) model over actions, desires, beliefs, and perceptions. Then, with such a joint model, any subset of these variables can be conditioned on to produce predictions or inferences. Such a model can also be used to reason about other people, though in that case something additional is needed to choose the appropriate values of things to condition on. Furthermore, the structure of a generative model such as this needs to be learned, which is also where the notion of a theory comes in—a meta form of reasoning that guides the process of constructing, and then using, the model.&lt;/p&gt;

</description>
        <pubDate>Fri, 08 Jan 2016 09:45:02 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/08/Saxe2005.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/theory%20of%20mind/2016/01/08/Saxe2005.html</guid>
        
        
        <category>Theory of mind</category>
        
      </item>
    
  </channel>
</rss>
