<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quals Reading Notes</title>
    <description>Notes on readings for my qualifying exams.
</description>
    <link>http://jhamrick.github.io/quals/</link>
    <atom:link href="http://jhamrick.github.io/quals/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 03 Jan 2016 16:10:14 -0800</pubDate>
    <lastBuildDate>Sun, 03 Jan 2016 16:10:14 -0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>On discriminative vs. generative classifiers: a comparison of logistic regression and naive Bayes</title>
        <description>&lt;p&gt;&lt;span id=&quot;Ng2002&quot;&gt;Ng, A. Y., &amp;amp; Jordan, M. I. (2002). On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes. &lt;i&gt;Advances In Neural Information Processing Systems&lt;/i&gt;, &lt;i&gt;14&lt;/i&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Ng &amp;amp; Jordan give a nice analysis of generative vs. discriminative classifiers, and show that there are actually two modes where either type of model might be preferred. They focus in particular on logistic regression vs. naive Bayes, but say that their analysis should be extendable to other generative-discriminative pairs of models.&lt;/p&gt;

&lt;p&gt;There are two key results in their analysis:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The asymptotic error of a generative linear classifier is greater than the asymptotic error of a discriminative linear classifier (which converges to the best linear classifier overall).&lt;/li&gt;
  &lt;li&gt;The number of training examples required for a discriminative linear classifier to reach its asymptotic error is $O(n)$, where $n$ is the &lt;a href=&quot;https://en.wikipedia.org/wiki/VC_dimension&quot;&gt;VC dimension&lt;/a&gt;. In contrast, the number of examples for a generative linear classifier is $O(\log{n})$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;They show these propositions to be empirically true across a number of experiments on the UCI datasets.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The results from Ng &amp;amp; Jordan suggest that generative classifiers may be more useful in situations where there are small amounts of data. However, if there is a lot of data available, it may be more useful to use a discriminative classifier because it will ultimately have less error. The intuition behind this (I think?) has to do with the fact that the generative classifier needs to make assumptions about the distributions for both the likelihood and the prior, while the discriminative model does not. In some cases, if the assumptions are correct, then the generative classifier should have the same asymptotic error as the discriminative classifier. If my intuition about this is correct, then the question is essentially: can you estimate how incorrect your assumptions are, and then use that estimate (combined with knowledge about how much data you have) to determine whether to train a generative vs. discriminative classifier?&lt;/p&gt;

&lt;p&gt;Of course, there are also other reasons to potentially use a generative model besides just faster asymptotic error. In many cases, it may be necessary to be able to invert the model (i.e. sometimes you may need $p(x\vert y)$, and sometimes you may need $p(y\vert x)$). Intuitively, it seems like if you need to do this, it is going to be more efficient to train a generative model (from which you can compute both $p(y\vert x)$ and $p(x\vert y)$) rather than training multiple discriminative models.&lt;/p&gt;

&lt;p&gt;One question I have about this is regarding training discriminative models with generative models—if you learn a generative model first, and then use samples from it to train a discriminative model, how does that affect the error of the discriminative model? Can the discriminative model only do as well as the generative model, in that case? I want to say the answer is yes, but perhaps combined with true data from the world (in addition to samples from the generative model) the discriminative model could eventually achieve a lower error.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 07:45:06 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/03/Ng2002.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/03/Ng2002.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</title>
        <description>&lt;p&gt;&lt;span id=&quot;Clark2013&quot;&gt;Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. &lt;i&gt;The Behavioral And Brain Sciences&lt;/i&gt;, &lt;i&gt;36&lt;/i&gt;(3), 181–204. doi:10.1017/S0140525X12000477&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this BBS article, Clark lays out a grand unified theory of cognition based on the idea that our brains are predictive machines. At the high levels of processing, our brains construct hypotheses/predictions of our percepts, and at the low levels of processing, our sensory systems compute error signals between what is predicted and what is actually sensed. He begins with the following assumption about what the brain is trying to accomplish:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How, simply on the basis of patterns and changes in its own internal states, is [the brain] to alter and adapt its resources so as to tune itself to act as a useful node… for the origination of adaptive responses? Notice how different this conception is to ones in which the problem is posed as one of establishing a mapping relation between environmental and inner states. The task is not to find such a mapping but to infer the nature of the signal source (the world) from just the varying input signal itself. (pg. 183)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Clark next defines his overarching theory as that of “action-oriented predictive processing”, and quotes from Hawkins &amp;amp; Blakeslee (2004):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As strange as it sounds, when your own behavior is involved, your predictions not only precede sensation, they determine sensation. Thinking of going to the next pattern in a sequence causes a cascading prediction of what you should experience next. As the cascading predition unfolds, it generates the motor commands necessary to fulfil the prediction. Thinking, predicting, and doing are all part of the same unfolding of sequences moving down the cortical hierarchy. (pg. 186)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, action is just a way to bring the world into alignment with the brain’s predictions.&lt;/p&gt;

&lt;p&gt;Clark goes on to describe a large body of work that supports these views, particularly from the realm of neuroscience and perceptual science. He also explains how the action-oriented predictive processing scheme unifies the levels of analysis, in that it is a neural implementation of what are actually generative Bayesian models.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I agree with a lot of what Clark says in this article, though there are a few points that I disagree with.&lt;/p&gt;

&lt;p&gt;First, “predictive” is &lt;em&gt;not&lt;/em&gt; the same as “generative” (though all generative models are predictive, in a sense). Discriminative models are also predictive, in the sense that you receive an input $x$ and predict an output $y$. Generative models, by contrast, can jointly predict $x$ and $y$ simultaneously, making it possible to predict $y$ from $x$ but also the other way around (predicting $x$ from $y$). Thus, generative models have more predictive power in the sense that they can be used to predict more relationships between the data, but that doesn’t mean discriminative models aren’t predictive. Clark uses “predictive” and “generative” interchangeably, but I think that is a mistake.&lt;/p&gt;

&lt;p&gt;I definitely agree that having a generative model is incredibly powerful because it allows us to make &lt;em&gt;a priori&lt;/em&gt; predictions in the absence of data. However, generative models can be more difficult to learn, and while they may be overall more general, that may come with the cost of a loss of precision for specific prediction tasks. Thus, discriminative models are important, too, and it is almost certainly incorrect to say that the brain never makes use of simple mappings.&lt;/p&gt;

&lt;p&gt;I’m not sure I agree with Clark’s characterization of action. I do like the idea that our minds make predictions, and then we use action to make those predictions come true, but I think it is important to make a distinction between predictions of the generative process in the world (i.e., what is going to happen next in the absence of action) versus predictions of how the default process might change &lt;em&gt;if&lt;/em&gt; we were to act on it.&lt;/p&gt;

&lt;p&gt;I am also not sure if I agree with the idea that the brain is trying to reduce &lt;em&gt;error&lt;/em&gt;, especially since “error” is not a well-defined term. Clark seems to use it in the sense of reducing entropy, but there’s a lot of other ways it could be used. By definition, any type of learning system is trying to optimize some objective function—the “error”—so while technically true, this  isn’t really that new of a concept, so I don’t feel that it provides all that much explanatory power.&lt;/p&gt;

&lt;p&gt;The more important component of Clark’s argument is that of the large part generative models probably play. Yet, Clark doesn’t go into details about how those models are actually constructed, beyond referring to a few things like the Helmholtz machine. That is where I think the truly difficult problems lie—determining &lt;em&gt;what&lt;/em&gt; the generative models are, &lt;em&gt;how&lt;/em&gt; we construct them in the first place, and &lt;em&gt;when&lt;/em&gt; a generative model is the thing that’s constructed as opposed to a discriminative model. There are particularly difficult questions to be answered in domains where the models must be able to handle incredibly high-dimensional data and parse it into complex, structured models. Saying the brain constructs generative models is a useful starting point (and it’s a starting point that I agree with), but it really doesn’t say anything about what the actual structure of those models is.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2016 06:05:44 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/03/Clark2013.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/03/Clark2013.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>The role of generative knowledge in object perception</title>
        <description>&lt;p&gt;&lt;span id=&quot;Battaglia2012&quot;&gt;Battaglia, P. W., Kersten, D., &amp;amp; Schrater, P. R. (2012). The Role of Generative Knowledge in Object Perception. In J. Trommershauser, K. P. Körding, &amp;amp; M. S. Landy (Eds.), &lt;i&gt;Sensory Cue Integration&lt;/i&gt;. Oxford University Press.&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this chapter, Battaglia et al. describe the differing roles of the &lt;em&gt;generative process&lt;/em&gt;, people’s &lt;em&gt;generative knowledge&lt;/em&gt;, and how they related to perception (particularly for object perception). They first define several challenges/observations regarding perception:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The mapping from the world to our senses is often not invertible (e.g. recovering 3D shape from 2D data)&lt;/li&gt;
  &lt;li&gt;Many sensory cues are not actually measurements of the relevant property we’re interested in, but provide only “auxiliary” information&lt;/li&gt;
  &lt;li&gt;Sensory cues vary in quality relative to each other, depending on external and internal factors (e.g. fog, cataracts), and as a function of the world state.&lt;/li&gt;
  &lt;li&gt;Objects’ spatial and material properties follow highly predictable patterns&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Next, they define the &lt;em&gt;sensory generative process&lt;/em&gt; to be the true process in the world that generates our sensations. In contrast, &lt;em&gt;sensory generative knowledge&lt;/em&gt; is people’s assumptions about how the sensory generative process works. In some cases, the generative process and generative knowledge may be the same, though not necessarily. If generative knowledge is at least a good approximation to the generative process, though, it can provide important information about how to interpret our sensations (addressing the challenges listed previously). Battaglia et al. make the distinction between being &lt;em&gt;subjectively optimal&lt;/em&gt; (in which people make optimal use of their generative knowledge, but the knowledge does not match the generative process), &lt;em&gt;objectively optimal&lt;/em&gt; (in which people make optimal use of their generative knowledge, which matches the generative process), and &lt;em&gt;suboptimal&lt;/em&gt; (in which people do not make optimal use of their generative knowledge).&lt;/p&gt;

&lt;p&gt;Battaglia et al. also describe how a Bayesian observer model can account for several basic phenomena in perception: performing basic Bayesian inference, combining multiple cues, discounting nuisance information based on prior knowledge, and explaining away nuisance information based on auxiliary cues. In particular, discounting and explaining away rely heavily on generative knowledge, while basic Bayes and cue combination could theoretically be learned just via a discriminative mapping.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This chapter makes two important points:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Our internal models aren’t necessarily accurate imitations of the true generative process, but that doesn’t mean that people don’t make optimal use of the knowledge they have. Hence, when constructing models of cognition, it is important to be explicit about what the generative process is in the world that is generating people’s observations, and what generative knowledge we as scientists think people actually have.&lt;/li&gt;
  &lt;li&gt;Having structured generative knowledge is important, because it makes it easier to interpret ambiguous sensations into reliable perceptions. For simpler things, such as cue combination, we don’t necessarily need to have a full generative model since a simple discriminative model can often be sufficient. But, for reasoning about more complex systems, and to be able to explain phenomena like discounting or explaining away, generative knowledge is crucially important.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 14:18:13 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/02/Battaglia2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/02/Battaglia2012.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>The Helmholtz machine</title>
        <description>&lt;p&gt;&lt;span id=&quot;Dayan1995&quot;&gt;Dayan, P., Hinton, G. E., Neal, R. M., &amp;amp; Zemel, R. S. (1995). The Helmholtz machine. &lt;i&gt;Neural Computation&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;(5), 889–904. doi:10.1162/neco.1995.7.5.889&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Dayan et al. propose a method for learning about the underlying structure in data using self-supervised learning in a neural network. Specifically, they construct the network to have bottom-up &lt;em&gt;recognition&lt;/em&gt; weights and top-down &lt;em&gt;generative&lt;/em&gt; weights. The network is then trained according to a &lt;em&gt;wake-sleep algorithm&lt;/em&gt;, where the generative weights are trained during the “wake” phase and the recognition weights are trained during the “sleep” phase by simulating training examples from the generative model.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;The recognition probability of unit $j$ in layer $\ell$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_j^\ell(\phi,\mathbf{s}^{\ell-1})=\sigma\left(\sum_i s_i^{\ell-1}\phi_{i,j}^{\ell-1,\ell}\right)&lt;/script&gt;

&lt;p&gt;where $\sigma$ is the sigmoid function and $\phi$ are the recognition weights. As mentioned earlier, the recognition weights are trained during the sleep phase by simulating training data from the generative model.&lt;/p&gt;

&lt;p&gt;The generative probability of unit $j$ in layer $\ell$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_j^\ell(\theta,\mathbf{s}^{\ell+1})=\sigma\left(\sum_i s_i^{\ell+1}\theta_{i,j}^{\ell+1,\ell}\right)&lt;/script&gt;

&lt;p&gt;where $s$ are the unit activities. The generative model is trained by presenting data to input units and then activating units according to $q_j^\ell$. Then, the generative weights are updated to minimized the KL-divergence between the actual activations and the generative probabilities $p_j^\ell$.&lt;/p&gt;

&lt;p&gt;The wake and sleep phases are computed iteratively and over time the weights should converge such that $Q(d)=P(\theta,d)$.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;The Helmholtz machine is a very cool idea, in that it is an unsupervised way of (approximately) learning highly complex structures by jointly training a recognition and generative model. This can be thought of as a way of implementing the EM algorithm in a neural network; it can also be interpreted as a particular type of autoencoder (e.g. if it is a one-layer Helmholtz machine, it is a folded-over autoencoder).&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 10:25:21 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/generative%20models/2016/01/02/Dayan1995.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/generative%20models/2016/01/02/Dayan1995.html</guid>
        
        
        <category>Generative models</category>
        
      </item>
    
      <item>
        <title>Learning compound multi-step controllers under unknown dynamics</title>
        <description>&lt;p&gt;&lt;span id=&quot;Han2015&quot;&gt;Han, W., Levine, S., &amp;amp; Abbeel, P. (2015). Learning Compound Multi-Step Controllers under Unknown Dynamics. &lt;i&gt;Proceedings Of the 28th IEEE/RSJ International Conference on Intelligent Robots and Systems&lt;/i&gt;. Retrieved from http://rll.berkeley.edu/reset_controller/reset_controller.pdf&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Han et al. build on the work of &lt;a href=&quot;/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Levine2015.html&quot;&gt;Levine et al.&lt;/a&gt; to learn controllers for tasks involving multiple steps. This is a difficult problem for traditional approaches to control, which assume that tasks are stationary (i.e., that the initial conditions are the same), which isn’t usually the case for compound tasks because the initial state of each step depends on the end state of the previous step. To address this, Han et al. formulate a way to learn both forward and &lt;em&gt;reverse&lt;/em&gt; controllers simultaneously. Then, in the compound task, if a forward controller fails, the reverse controller can be used to reset the movement and try again. Training compound controllers in this way is much more effective than trying to learn one global controller.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;Training the forward and reset controllers is actually pretty straightfoward. Han et al. run the forward controller and reset controllers in sequence $N$ times, and use these samples to update both controllers according to the task-specific loss function (for the forward controller) or a reset cost (for the reset controller).&lt;/p&gt;

&lt;p&gt;Training the compound controller follows largely the same idea. Each forward controller is executed in sequence. If a controller fails, then the corresponding reverse controller is used to undo the action, and the forward controller is run again. This is repeated until the controller succeeds. After $N$ samples are collected for a controller, it is refit according to those samples. These $N$ samples count as one “iteration”, and all controllers are trained for $K$ iterations.&lt;/p&gt;

&lt;p&gt;As in the previous work, the individual linear-Gaussian controllers can also be used to train a more general parameterized policy (such as a neural network). The benefit of training both forward and reset controllers, though, means that the system can autonomously train the neural network by running the forward controllers, using those samples as training data for the network, and resetting automatically using the reset controller.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This way of training compound controllers is exciting, because it could potentially be integrated with higher-level planning algorithms to determine how to accomplish more sophisticated and complex tasks. This brings up an interesting distinction in the idea of simulation—that you can potentially have simulations at different levels of abstraction, as well as at different levels of granularity. In this case, the robot can learn a dynamics model for multiple motion primitives (the individual linear-Gaussian controllers) but it might also need to be able to learn a higher-level (perhaps more qualitative) form of simulation in order to reason about how to accomplish the task in the first place. In the case of screwing in a bolt, the robot might need high-level qualitative knowledge about how the task works (first need to pick up the wrench, then bring it to the bolt, then turn it because bolts need to be turned to go further into the hole, then repeat, etc.), but as shown by this work, it also needs low-level knowledge about the dynamics of the task in order to actually execute the subparts of the action.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 08:22:37 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Han2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Han2015.html</guid>
        
        
        <category>Planning under uncertain dynamics</category>
        
      </item>
    
      <item>
        <title>Learning contact-rich manipulation skills with guided policy search</title>
        <description>&lt;p&gt;&lt;span id=&quot;Levine2015&quot;&gt;Levine, S., Wagener, N., &amp;amp; Abbeel, P. (2015). Learning Contact-Rich Manipulation Skills with Guided Policy Search. &lt;i&gt;Proceedings Of the IEEE International Conference on Robotics and Automation&lt;/i&gt;. Retrieved from http://arxiv.org/abs/1501.05611v1&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This paper builds on previous work (Levine &amp;amp; Abbeel, NIPS 2013) that learns policies for object manipulation using a two step process. The first step is to learn local linear-Gaussian controllers for a few specific training examples. Then, the linear-Gaussian controllers are used to train parameters for a more complex policy (e.g., using a neural network) using a method called &lt;em&gt;guided policy search&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;h2 id=&quot;linear-gaussian-controllers&quot;&gt;Linear-Gaussian controllers&lt;/h2&gt;

&lt;p&gt;First, they assume linear-Gaussian dynamics, i.e.:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{x}_{t+1}\vert \mathbf{x}_t,\mathbf{u}_t)=\mathcal{N}(f_{\mathbf{x}t}\mathbf{x}_t+f_{\mathbf{u}t}\mathbf{u}_t, \mathbf{F}_t)&lt;/script&gt;

&lt;p&gt;They can fit these dynamics using samples collected under the previous version of the controller, and from there compute the linear-Gaussian controller:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{u}_t\vert \mathbf{x}_t)=\mathcal{N}(\hat{\mathbf{u}}_t+\mathbf{k}_t+\mathbf{K}_t(\mathbf{x}_t-\hat{\mathbf{x}}_t),Q_{\mathbf{u}, \mathbf{u}t}^{-1})&lt;/script&gt;

&lt;p&gt;where $Q_{\mathbf{u}, \mathbf{u}t}$ is the Hessian of the $Q$-function, $\mathbf{k}_t=-Q_{\mathbf{u},\mathbf{u}t}^{-1}Q_{\mathbf{u}t}$ and $\mathbf{K}_t=-Q_{\mathbf{u},\mathbf{u}t}^{-1}Q_{\mathbf{u},\mathbf{x}t}$ and (I think?) where $\hat{\mathbf{u}}_t$ and $\hat{\mathbf{x}}_t$ are the mean state and control from the samples. The $Q$-function is computed using a dynamic programming algorithm, which I won’t go into the details of.&lt;/p&gt;

&lt;p&gt;The controller is then updated subject to the constraint that the KL-divergence between the trajectory distribution $p(\tau)=\prod_t p(\mathbf{x}_{t+1}\vert \mathbf{x}_t,\mathbf{u}_t)p(\mathbf{u}_t\vert \mathbf{x}_t)$ and the old trajectory distribution is not more than a threshold $\epsilon$. They solve this optimization using dual gradient descent, which I also won’t go into the details of here.&lt;/p&gt;

&lt;p&gt;Levine et al. minimize the number of samples needed by also estimating a Gaussain mixture model prior on the global dynamics, and adaptively adjust both the step size $\epsilon$ and sample count according to an estimate of the additional cost at each iteration due to unmodeled changes in the dynamics.&lt;/p&gt;

&lt;h2 id=&quot;guided-policy-search&quot;&gt;Guided policy search&lt;/h2&gt;

&lt;p&gt;Given the learned linear-Gaussian dynamics and controller, they can be used to train a more sophisticated policy with large numbers of parameters (e.g., a neural network). Rather than directly computing a policy, supervised learning is used to learn the policy using trajectories sampled from the linear-Gaussian controllers (this is kind of a form of learning-by-demonstration, I think, where the demonstrations are a combination of the actual trajectories run using the linear-Gaussian controller on the real robot, and synthesized trajectories from the linear-Gaussian controllers). The trajectories are then reoptimized as well to better match the state distribution of the policy, i.e.:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\theta,p(\tau)}E_{p(\tau)}[\ell(\tau)]\ \mathrm{s.t.}\ D_{\mathrm{KL}}(p(\mathbf{x}_t)\pi_\theta(\mathbf{u}_t\vert \mathbf{x}_t)\vert\vert p(\mathbf{x}_t, \mathbf{u}_t))=0\ \forall t&lt;/script&gt;

&lt;p&gt;where $p(\tau)$ is the trajectory distribution (obtained from the linear-Gaussian dynamics) and $\pi_\theta$ is the parameterized policy. They solve this using another (different) application of dual gradient descent.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;This is a cool way to combine deep learning with a more structured modeling approach. Essentially, the linear-Gaussian controllers are learning a generative model of the dynamics for specific motions, from which an optimal policy can be extracted with relatively little computation. The generative model is then used to train the discriminative model (the neural network). It’s an interesting contrast, though, because I generally think of generative models as being &lt;em&gt;more&lt;/em&gt; general, whereas in this case it’s actually that there are multiple generative models, each of which is local and therefore relatively fragile. But by combining them together a more powerful and flexible discriminative model can be trained.&lt;/p&gt;

&lt;p&gt;Overall, it seems to me that this approach is quite similar similar to the approach taken by &lt;a href=&quot;/quals/physical%20reasoning%20without%20dynamics%20models/2015/12/30/Paraschos2015.html&quot;&gt;Paraschos et al.&lt;/a&gt;, at least in terms of using linear-Gaussian dynamics to learn local (or primitive) motion policies. This paper, though, goes the next step and shows how then those local motion policies can be combined in order to train a more sophisticated and general policy.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 04:56:38 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Levine2015.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/planning%20under%20uncertain%20dynamics/2016/01/02/Levine2015.html</guid>
        
        
        <category>Planning under uncertain dynamics</category>
        
      </item>
    
      <item>
        <title>The simulation heuristic</title>
        <description>&lt;p&gt;&lt;span id=&quot;Kahneman1981&quot;&gt;Kahneman, D., &amp;amp; Tversky, A. (1981). &lt;i&gt;The simulation heuristic&lt;/i&gt;. Retrieved from http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA099504&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Kahneman &amp;amp; Tversky discuss how people construct mental simulations of hypothetical scenarios, and in particular, counterfactual scenarios. They argue that people rely on a “simulation heuristic”, which is defined as constructing scenarios in order to estimate the probability of events.&lt;/p&gt;

&lt;p&gt;In the introduction, Kahneman &amp;amp; Tversky provide a nice definition of what they mean by simulation (pg. 1-2):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Our starting point is a common introspection: there appear to be many situations in which questions about events are answered by an operation that resembles the running of a simulation model. The simulation can be constrained and controlled in several ways: the starting conditions for a ‘run’ can be left at their realistic default value, or modified to assume some special contingency; the outcomes can be left unsecified, or else a target state may be set, with the task of finding a path to that state from the initial conditions. A simulation does not necessarily produce a single story, which starts at the beginning and ends with a definite outcome. Rather, we construe the output of simulation as an assessment of the ease with which the model could produce different outcomes, given its initial conditions and operating parameters. Thus, we suggest that mental simulation yields a measure of the propensity of one’s model of the situation to generate various outcomes, much as the propensities of a statistical model can be assessed by Monte Carlo techniques.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They also give a few use cases that seem to involve simulation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prediction&lt;/li&gt;
  &lt;li&gt;Assessing the probability of a specified event&lt;/li&gt;
  &lt;li&gt;Assessing conditioned probabilities&lt;/li&gt;
  &lt;li&gt;Counterfactual assessments&lt;/li&gt;
  &lt;li&gt;Assessments of causality&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;methods&quot;&gt;Methods&lt;/h1&gt;

&lt;p&gt;In this paper, they focus primarily on counterfactual assessments (i.e., “if only…” scenarios). The first type of task that they focus on is that involving regret. The example scenario is that of two people who miss their flight, but by different margins (5 minutes vs. 30 minutes), and ask participants to ask who feels worse. Almost universally people respond that the person who missed their flight by 5 minutes feels worse than the other person. Kahneman and Tversky argue that this is because it is easier to imagine making it to the airport 5 minutes earlier than it is to make it to the airport 30 minutes earlier.&lt;/p&gt;

&lt;p&gt;The second type of task they focus on involves people actually producing alternate scenarios that could have prevented something happening (for example, someone dying in a car crash). There were two conditions; one in which the driver left work early (“time” version), and one in which they took an unusual route (“route” version). In the time version, participants came up with alterations to the scenario like “he should have left at a different time” (26%), “he should have crossed the intersection more quickly” (31%), or “the other driver shouldn’t have been driving” (29%). In the route version, the majority of participants said “he shouldn’t have taken the different route” (51%), with some also saying “he should have crossed the intersection more quickly” (22%) or “the other driver shouldn’t have been driving” (20%).&lt;/p&gt;

&lt;p&gt;The route version involves an unusal element (the route), and correspondingly, participants are vastly more likely to undo this element in their counterfactual simulation than the participants in the time version. Kahneman and Tversky explain this as one of three types of changes that can occur:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Downhill change (increases probability, decreases surprise)&lt;/li&gt;
  &lt;li&gt;Uphill change (decreases probability, increases surprise)&lt;/li&gt;
  &lt;li&gt;Horizontal change (arbitrary value is changed, no change in probability or surprise)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In particular, the change in the route version is an example of a downhill change; thus, Kahneman and Tversky argue that when constructing simulations, people are biased towards scenarios that are more probable and less surprising. Moreover, people are biased towards making alterations to the scenario that are related to the main object or character (e.g., the behavior of the original driver, and not the other driver).&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;n/a&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;Kahneman and Tversky characterize the simulation heuristic as being biased towards downhill changes. I wonder if this could be explained by something like a stochastic search in the space of scenarios (e.g. something like Monte-Carlo tree search), where people are trying to maximize something like the posterior probability of the scenario given the alternate outcome. Due to the prior, this would result in a bias towards less surprising scenarios (e.g. undoing the change in route). This bias isn’t necessarily a bad thing, though. The prior is a well-motivated component, because you need something to constrain the space of possible scenarios—the narrative structure of the situation still needs to be coherent, and the structure of the prior ensures that.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Jan 2016 02:21:27 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/memory%20and%20imagination/2016/01/02/Kahneman1981.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/memory%20and%20imagination/2016/01/02/Kahneman1981.html</guid>
        
        
        <category>Memory and imagination</category>
        
      </item>
    
      <item>
        <title>The future of memory: remembering, imagining, and the brain</title>
        <description>&lt;p&gt;&lt;span id=&quot;Schacter2012&quot;&gt;Schacter, D. L., Addis, D. R., Hassabis, D., Martin, V. C., Spreng, R. N., &amp;amp; Szpunar, K. K. (2012). The Future of Memory: Remembering, Imagining, and the Brain. &lt;i&gt;Neuron&lt;/i&gt;, &lt;i&gt;76&lt;/i&gt;(4), 677–694. doi:10.1016/j.neuron.2012.11.001&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this paper, Schacter et al. review neuroscientific evidence for the relationship between recalling episoding memories and constructing imagined events. Broadly, the conclusion is that these two processes are very similar and rely on a lot of the same mechanisms, but also have measureable differences. They make four main points:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Remembered events are always related to the past, but imagined events could be set in the past, present, or future. Thus, remembering vs. imagining shouldn’t be interpreted as past vs. future.&lt;/li&gt;
  &lt;li&gt;There are differences between the processes involved in remembering and imagining. For example, imagining requires synthesizing an entirely new event and possibly concepts, while remembering—while perhaps a constructive process—need only retrieve existing pieces of information. Remembering the past has also been associated with a larger number and more vivid sensory details&lt;/li&gt;
  &lt;li&gt;In terms of specific component processes, it seems that regions like the MPFC (medial prefrontal cortex) and posterior cingulate are involved in simulations of the self and social scenarios; regions like the medial temporal lobe and retrosplenial cortex are more involved in memory-based scene construction. The hippocampus is also involved in imagining and remembering, though its specific role isn’t as clear; there is some evidence that it’s involved specifically with retrieving relevant details, combining them into something coherent, and then storing the result.&lt;/li&gt;
  &lt;li&gt;The &lt;em&gt;default network&lt;/em&gt; is involved in both remembering and imagining and includes regions like the medial temporal lobe, frontal lobe, posterior cingulate cortex, retrosplenial cortex, and lateral parietal and temporal areas. This network can be coupled with other networks to support goal-directed simulations (particularly those that are autogbiographical).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the conclusion, Schacter et al. suggest that “a key function of memory is to provide a basis for predicting the future via imagined scenarios and that the ability to flexibly recombine elements of past experience into simulations of novel future events is therefore an adaptive process”.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think the most important takeaway for me is that mental simulation of (at least episodic) events is closely tied to the ability to retrieve existing memories and somehow recombine them. The interesting question for me, though, is &lt;em&gt;how&lt;/em&gt; this recombining is actually accomplished, and the way it is different between reconstructing or reinterpreting memories and creating new simulations. One hypothesis might be that when either remembering or imagining, people first construct or retrieve some high-level narrative structure of the event. Then, they fill in the details, either by retrieving the specific details that were stored (in the case of remembering), or by sampling specific details (in the case of simulation). This might be one way to explain why simulations tend to have fewer details, too: for remembering, you perhaps attempt to retrieve all the stuff the was stored, while for simulating, you only sample details until the simulation is “coherent enough”. For some definition of “coherent” and “enough”, this might not need to include things like small perceptual or sensory details.&lt;/p&gt;

&lt;p&gt;On a different note, this paper illustrates a different way in which “simulation” is used—here, they are using simulation to refer to imagined &lt;em&gt;episodic&lt;/em&gt; events. This is in contrast to the simulation of mental imagery, in which simulation tends to mean something more along the lines of imagined objects or actions. Of course, to simulating an episodic event probably requires being able to simulate objects and actions, but it additionally requires being able to piece together multiple things into a coherent, high-level picture. Schacter et al. actually touch on this a bit in the paper (pg. 681):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Scene construction entails retrieving and intergrating perceptual, semantic, and contextual information into a coherent spatial context. Scene construction is held to be more complex than “simple” visual imagery for individual objects because it relies on binding together disparate types of information into a coherent whole, and likely involves processes mediated by several regions within the default network, most notably the medial temporal lobe. Scene construction is thought to be a critical component of both memory and imagination as mental simulations, whether of the past, future, or purely fictional, because they are all usually framed within a spatial context.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Fri, 01 Jan 2016 10:51:04 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/memory%20and%20imagination/2016/01/01/Schacter2012.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/memory%20and%20imagination/2016/01/01/Schacter2012.html</guid>
        
        
        <category>Memory and imagination</category>
        
      </item>
    
      <item>
        <title>Are things that are hard to physically move also hard to imagine moving?</title>
        <description>&lt;p&gt;&lt;span id=&quot;Flusberg2011&quot;&gt;Flusberg, S. J., &amp;amp; Boroditsky, L. (2011). Are things that are hard to physically move also hard to imagine moving? &lt;i&gt;Psychonomic Bulletin And Review&lt;/i&gt;, &lt;i&gt;18&lt;/i&gt;(1), 158–164. doi:10.3758/s13423-010-0024-2&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Flusberg &amp;amp; Boroditsky begin with the following important question (pg. 158-159):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;On the one hand, there appear to be tight links among perception, action and mental imagery, both in behavior and in the underlying neural circuitry… On the other hand, what often seems to separate our imagination from the everyday world is that it appears free from the constraints imposed by the physical environment. Indeed, people often imagine actions or events that they have never seen or never could experience themselves… to what extent do our experiences with objects in the real world impinge on our ability to represent and manipulate those objects in mental imagery?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To get at this question, they perform the classic mental rotation task but prime participants first by having them physically rotate the objects. The key manipulation was that some of the objects were more difficult to physically rotate than the others (denoted by color). Flusberg &amp;amp; Boroditsky found that participants had longer response times when using &lt;em&gt;motor imagery&lt;/em&gt; after performing the physical rotations (whereas there was no effect when using &lt;em&gt;visual imagery&lt;/em&gt;), and also that there was an effect of the angle of rotation on the difficulty of rotation (i.e. the slope for the more difficult objects was higher than that for the less difficult objects, which is what would be expected from manually performing the rotations). However, they only saw this effect after excluding some of the participants post-hoc.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think this is a cool study, though I would have liked to see a more in depth evaluation—for example, more angles of rotation, and more than two levels of difficulty. I also think it would have been better to have participants do the mental rotation task on entirely different objects than the ones they physically rotated, because it’s not clear to me whether they measured an effect of &lt;em&gt;prior knowledge&lt;/em&gt; (i.e. abstract knowledge about the difficulty of manipulating the object) or an effect of &lt;em&gt;prior experience&lt;/em&gt; (i.e. they are just replaying an experience they had). I would be inclined to believe they actually are measuring prior knowledge (as they claim), but I would like to know for sure.&lt;/p&gt;

&lt;p&gt;Methodology aside, the fact that there is a difference between motor imagery and visual imagery is quite interesting. I want to know &lt;em&gt;why&lt;/em&gt; this difference exists, though; I’m not convinced there is actually a hard line that can be drawn between the two. Typically, tasks like mental rotation do still involve some amount of motor activation in the brain, meaning that motor imagery isn’t entirely separate from visual imagery. Perhaps the distinction shouldn’t be between motor/visual, but just based on the mechanism by which the object is moving (or, put another way, how the simulated scene is constructed). If you are asked to imagine moving it yourself, then it makes sense that you would imagine what the effects of your motor actions are. If you are asked to imagine the object itself moving, then it perhaps you are imagining (for example) the object sitting on a rotating disk. It would be interesting to try to come up with a task that would require the “visual imagery” condition to still engage in some sort of motor imagery, but motor imager that is irrelevant to how difficult the object is to move. Would that cause an effect of difficulty to appear in the visual imagery condition? I hypothesize that it wouldn’t, which would imply that the distinction isn’t between motor and visual imagery but in terms of how the simulation is constructed, somehow.&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Jan 2016 09:29:15 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Flusberg2011.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Flusberg2011.html</guid>
        
        
        <category>Mental imagery</category>
        
      </item>
    
      <item>
        <title>The emulation theory of representation: motor control, imagery, and perception</title>
        <description>&lt;p&gt;&lt;span id=&quot;Grush2004&quot;&gt;Grush, R. (2004). The emulation theory of representation: motor control, imagery, and perception. &lt;i&gt;The Behavioral And Brain Sciences&lt;/i&gt;, &lt;i&gt;27&lt;/i&gt;(3), 377–96; discussion 396–442. doi:10.1017/S0140525X04000093&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Wow, this was such a cool paper to read! Grush lays out the “emulation” theory of motor control/perception/imagery/etc. which is very similar to the theory that I have been thinking of (but that I haven’t been able to explain nearly as eloquently). It is what he calls a “pseudo-closed-loop control” scheme, inspired by the ideas behind Kalman filtering. The core ideas from the Kalman filter are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;There is some process that is created by a combination of a driving force as well as the internal process mechanism (and some noise in that mechanism). Measurements are made of the state of this process, but they are also corrupted by noise.&lt;/li&gt;
  &lt;li&gt;There is an internal model (emulator) of the process mechanisms. Using this internal model plus the driving force, the next state of the system is computed.&lt;/li&gt;
  &lt;li&gt;A measurement is made of the simulated state, and it is compared to the measurement that was made from the true process. The difference between these two measurements is combined with the inverse of the observation in order to compute a residual correction.&lt;/li&gt;
  &lt;li&gt;Some proportion of correction is applied to the estimated state in order to produce a corrected estimate of the measurement.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In terms of cognition, the process is something like the state of a limb, or a representation of a scene. The driving force is the motor commands that are issued to change the state of the system. So, the hypothesis is essentially that during motor control or perception, the mind uses an internal model (the emulator) to keep track of a better estimate of the state of the system. But, by having an emulator, it can also be used for other things (such as mental imagery). In the case of imagery, the emulator itself can be used to make predictions about the future state of the system (without applying corrections).&lt;/p&gt;

&lt;h2 id=&quot;motor-imagery&quot;&gt;Motor imagery&lt;/h2&gt;

&lt;p&gt;Grush makes the distinction between &lt;em&gt;emulation theory&lt;/em&gt; and &lt;em&gt;simulation theory&lt;/em&gt;, where simulation theory is the idea that motor imagery comes from simulating motor commands, but not actually executing those controls. Emulation theory includes that aspect, but also includes the idea of having a separate model that emulates the state of the system. To quote from the paper:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;The differenceis that the simulation theory does not posit anything corresponding to an emulator&lt;/em&gt;; as far as I can tell, the simulator theory is conceived against the backdrop of closed-loop control, and motor imagery hypothesized to be the free-spinning of the controller (motor centers) when disengaged from the plant (body). In the emulation theory, by contrast, imagery is not produced by the mere free-spinning opertion of the efferent motor areas, but by the efferent motor areas driving an emulator of the musculoskeletal system. (pg. 382)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;visual-imagery&quot;&gt;Visual imagery&lt;/h2&gt;

&lt;p&gt;In discussing how emulation theory applies to perception and visual imagery, Grush makes the distinction between &lt;em&gt;modal emulators&lt;/em&gt; and &lt;em&gt;amodal emulators&lt;/em&gt;. A modal emulator is an emulator which mimics one of the senses (e.g., by representing the state as pixels). An amodal emulator, by contrast, represents the state using a more structured abstract or symbolic representation (e.g., the underlying geometry, pose, and kinematics of objects and where the agent is in relation to them). The amodal emulator is clearly more flexible and encodes much more information than modal emulators; however, the modal emulators may be better at predicting information that is specific to a particular sensory domain. Thus, Grush proposes that the mind probably makes use of an amodal emulator as well as multiple modal emulators for vision, motor control, audition, etc.&lt;/p&gt;

&lt;h2 id=&quot;sensation-and-perception&quot;&gt;Sensation and perception&lt;/h2&gt;

&lt;p&gt;Grush goes as far as to say that modal emulators are really used in &lt;em&gt;sensation&lt;/em&gt; (i.e. receiving and processing sensory signals) while a particular amodal emulator, the &lt;em&gt;environment emulator&lt;/em&gt;, is what really produces our &lt;em&gt;perception&lt;/em&gt; (the interpretation of our sensations). The environment emulator is object-centric, and presumably is what allows us to do things like predict physical events.&lt;/p&gt;

&lt;p&gt;There are two points that Grush makes here that are, in my opinion, very important. The first is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… the current scheme, exactly because it treats perception as one aspect of an integrated information processing strategy, sheds light on the nature of perception itself. In the first place, the scheme highlights the extent to which the outcome of the perceptual process, the state estimate embodied in the emulator, is tuned to sensorimotor requirements. The emulator represents objects and the environment &lt;em&gt;as things engaged with&lt;/em&gt; in certain ways as opposed to how they are considered apart from their role in the organism’s environmental engagements. (pg. 393)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think this is an important point because I think the way we reason about the world—and the objects in it—is precisely because we have to interact in the world. The point of having an ability for mental simulation (emulation) isn’t because it’s useful for higher level problem solving or creativity, but because it’s essential for planning and predicting what is going to happen.&lt;/p&gt;

&lt;p&gt;The second important point is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another shift in emphasis suggested by this account is that perception is shown to be not a matter of starting with materials provided in sensation and filling in blanks until a completed percept is available. Rather, completed percepts of the environment are the starting point, in that the emulator always has a potentially self-contained environment emulator estimate up and running… The role played by sensation is to constrain the configuration and evolution of this representation. In motto form, &lt;em&gt;perception is a controlled hallucination process&lt;/em&gt;. (pg. 393)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I love the characterization of perception as a controlled hallucination process. This is also exactly in line with a Bayesian interpretation: the prior is the internal model of how the world works (the hallucination), and the likelihood constrains the prior based on actual observations of the real world.&lt;/p&gt;

&lt;h1 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h1&gt;

&lt;p&gt;I think the emulation theory proposed by Grush is exactly the right way to think about perception and action. However, it is also a very high-level schematic of how to approach the problem: there are many questions that aren’t answered. For example:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How are the internal models learned? Here, perspectives from robotics might be particularly useful. For example, something like the “distal teaching” approach discussed by &lt;a href=&quot;/quals/physical%20reasoning%20with%20dynamics%20models/2015/12/20/Nguyen-Tuong2011.html&quot;&gt;Nguyen-Tuong &amp;amp; Peters&lt;/a&gt; for training a mixed model (forward plus inverse models) could be used to learn new forward models and their inverses.&lt;/li&gt;
  &lt;li&gt;The emulation theory as discussed here just takes as input to the controller some high level goal, which may or may not be produced by using the emulator to make further predictions. What is the tradeoff between using the emulator for tracking the current state of the system vs. doing planning? Or are there multiple emulators used?&lt;/li&gt;
  &lt;li&gt;How does the controller decide what control signal to produce? Again, this is something that could potentially engage the emulator, which means there is also potentially a tradeoff for how the emulator gets used. For both this point and the previous point, work in robotics may provide some insight.&lt;/li&gt;
  &lt;li&gt;The Kalman filtering approach described by Grush assumes that the forward dynamics are the same for both the real and imagined process. What if they aren’t? What sorts of implications does this model mismatch have?&lt;/li&gt;
  &lt;li&gt;If such emulators are used for imagery, how is the initial state for them constructed in the first place?&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 01 Jan 2016 07:46:02 -0800</pubDate>
        <link>http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Grush2004.html</link>
        <guid isPermaLink="true">http://jhamrick.github.io/quals/mental%20imagery/2016/01/01/Grush2004.html</guid>
        
        
        <category>Mental imagery</category>
        
      </item>
    
  </channel>
</rss>
