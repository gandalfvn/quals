<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Analysis by synthesis: A (re-)emerging program of research for language and vision</title>
    <meta name="description" content="Notes on readings for my qualifying exams.
">

    <link rel="stylesheet" href="/quals/css/main.css">
    <link rel="canonical" href="http://jhamrick.github.io/quals/analysis%20by%20synthesis/2015/11/11/bever2010.html">

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/quals/">Quals Reading Notes</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/quals/about/">About</a>
          
        
          
          <a class="page-link" href="/quals/categories/">Categories</a>
          
        
          
        
          
        
          
        
        <a class="page-link" href="/quals/readings.pdf">Reading List</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Analysis by synthesis: A (re-)emerging program of research for language and vision</h1>
    <p class="post-meta">Nov 11, 2015 • Analysis by synthesis</p>
  </header>

  <article class="post-content">
    <p><span id="Bever2010">Bever, T. G., &amp; Poeppel, D. (2010). Analysis by Synthesis: A (Re-)Emerging Program of Research for Language and Vision. <i>Biolinguistics</i>, <i>43</i>(2), 174–200. Retrieved from http://www.psych.nyu.edu/clash/dp_papers/bever.poeppel.pdf</span></p>

<h1 id="summary">Summary</h1>

<p>Bever &amp; Poeppel describe the <em>analysis by synthesis</em> (AxS) approach and how it applies to language. AxS was apparently first proposed by <a href="/quals/analysis%20by%20synthesis/2015/11/11/halle1962.html">Halle &amp; Stevens</a> as a hypothesis for how speech production works. It follows the following steps:</p>

<ol>
  <li>Form a rough hypothesis about the input based on simple cues</li>
  <li>Synthesize a full simulation of the input based on that hypothesis</li>
  <li>Compare the simulated input to the real input</li>
  <li>If the two match, then the structure of 2 is taken to be the true structure of the input</li>
</ol>

<p>If they don’t match, then some iterative process of error reduction is performed. This approach is motivated by the idea that “it is computationally intractable to go directly from the more concrete to the more abstract representation by way of filters or other kinds of ‘bottom-up’ triggering templates” (pg. 177).</p>

<p>Bever &amp; Poepple go on to discuss how AxS is related to the motor theory of speech perception, how it is already being (implicitly) used in many automatic speech recognition systems, how it relates to AxS in vision, and the compatibility with Bayesian models.</p>

<h2 id="motor-theory-of-speech-perception">Motor theory of speech perception</h2>

<p>The <em>motor theory of speech perception</em> states that “listeners are reconstructing the articulatory gestures of the speaker, and using those as the trigger for the perception of the underlying intended sequence of phones as though they actually occurred acoustically” (pg. 179). To me, this sounds a bit like the “simulation theory” in theory of mind.</p>

<h2 id="automatic-speech-recognition">Automatic speech recognition</h2>

<p>They note that automatic speech recognition systems often use generative models, for example of “words-to-waveforms”. This is different from the AxS approach proposed by Halle &amp; Stevens, however, in that AxS uses a model of the articulatory system, while the ASR approaches store “vectors representing [Gaussian] mean and variance of spectral slices”. I suppose this difference essentially makes AxS closer to the motor theory of speech perception. At the computational level, I’m not sure how much this distinction matters, assuming the “fixed” model of speech production is detailed enough to capture the full range of effects that can actually occur when speaking a word.</p>

<h2 id="analysis-by-synthesis-in-vision">Analysis by synthesis in vision</h2>

<p>Bever &amp; Poepple reference <a href="/quals/analysis%20by%20synthesis/2015/11/10/yuille2006.html">Yuille &amp; Kersten</a>, among others, and discuss how AxS has been fruitful in vision. They note that given the wide array of cross-modal effects, if such a AxS process exists in vision, then it is likely to occur for audition/speech perception as well, perhaps utilizing some sort of general-purpose mechanism.</p>

<h2 id="bayesian-approach">Bayesian approach</h2>

<p>Ultimately, they say that there is no conflict with the Bayesian approach to implementing AxS. I agree with this; there’s no fundamental difference, using a Bayesian model is just a different way of formalizing it.</p>

<h1 id="methods">Methods</h1>

<p>n/a</p>

<h1 id="algorithm">Algorithm</h1>

<p>n/a</p>

<h1 id="takeaways">Takeaways</h1>

<p>AxS seems to be a promising approach in both vision and language. However, as I discussed a bit in my notes on <a href="/quals/analysis%20by%20synthesis/2015/11/10/yuille2006.html">Yuille &amp; Kersten</a>, I don’t necessarily see why the “synthesis” component is so crucial. If that’s what is necessary to compute the probability (e.g., if you are using some approach like Approximate Bayesian Computation), then sure, but I’m not convinced that it is. As long as you can compute the PDF at a particular point, then you can evaluate the likelihood of the data given the model without necessarily needing to created a synthesized version of the image or sound production.</p>

<p>So in general, my takeaway from the “analysis by synthesis” approach doesn’t so much have to do with synthesis itself, but of the combination of low-level cues to generate hypotheses, and top-down knowledge to constrain those hypotheses. The reason for having this particular approach (as opposed to just doing inference over all hypotheses) is that the space of hypotheses is large enough so as to make it intractable to consider all possible hypotheses.</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Quals Reading Notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Quals Reading Notes</li>
          <li><a href="mailto:"></a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/jhamrick">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">jhamrick</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/jhamrick">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">jhamrick</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Notes on readings for my qualifying exams.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
