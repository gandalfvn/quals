\documentclass[12pt]{article}
\input{heading}
\begin{document}

% \question{How has simulation been used in the context of planning and decision making for robotic systems? In what ways has been successful, and in what ways has it failed?}

\question{How does use of simulation in computer science and robotics relate to the use of simulation in cognitive science? Are there ways in which simulation is used in robotics and computer science that could be applicable to cognitive modeling? Be sure to discuss multiple types of simulation, including probabilistic simulation, simulations in planning (e.g. Monte-Carlo Tree Search), physical simulation for computer graphics, and combinations thereof.}

\subsection*{Introduction}

Simulation has been used in many contexts in computer science, ranging from simulation for physically-based animation, to probabilistic simulation, to simulated planning for learning Markov Decision Processes (MDPs). These areas of computer science have developed algorithms that I believe can play key roles in cognitive modeling, particularly in the framework of resource-rational analysis \citep{Griffiths2015}. First, I will describe how notions of simulation in physically-based animation are themselves approximating a computational-level solution, and that they are inherently biased towards a compatibility with human perception. Second, I will discuss how simulation in control theory applies to motor control, and how it relates to the idea of analysis-by-synthesis \citep{Halle1959,Halle1962} in cognitive science. Third, I will argue that contrary to recent claims, simulation is not incompatible with planning; and, that recent planning methods are an intriguing class of approximate algorithms that should be explored in resource-rational analysis, particularly in concert with metacognition. Finally, I will describe how simulation is used to approximate probabilistic inference.

\subsection*{Simulation in physically-based animation}

In physically-based animation for computer graphics, the goal is generally to determine the solution to a differential equation. However, these differential equations typically do not have closed-form symbolic solutions and therefore must be simulated numerically. This leads to many choices about which representations and approximations to use in order to get a visually plausible animation. I see two parallels between physically-based animation and cognitive modeling. The first is related in particular to the notion of resource-rational analysis \citep{Griffiths2015}, and the second is related to the preference in physically-based animation for \textit{plausibility} over \textit{accuracy}.

In a resource-rational analysis, one first determines what the computational-level solution to a problem is. The second step is to pick a class of \textit{approximate} process-level solutions to the computational-level problem, and ask the question of how that approximate solution should be used in order to make optimal use of finite cognitive resources. Typically, resource-rational analysis focuses on approximate solutions to the intractable problem of performing Bayesian inference; however, this need not be the only computational-level problem. Physically-based animation deals with another intractable computational-level. In this case, the question is about how to compute what the value of a particular physical quantity will be (e.g., position, orientation, velocity, topology, etc.) at a future point in time, given the current value for that quantity \citep{Witkin1997}. However, as this is an intractable problem, the field of physically-based animation is based on developing approximate solutions to this problem (or, in the terminology of the field, finding tractable ways to model the physical system). Thus, many approximate solutions already exist to the computational-level problem of determining future physical quantities and are good candidates for a resource-rational analysis regarding how \textit{people} reason about future physical quantities.

I will give two examples of potential classes of approximate solutions, both in the domain of liquid simulation. In physically-based animation, there are multiple ways that fluids and liquids may be simulated. One class are \textit{grid-based} Eulerian methods for simulating large bodies of liquid such as the water in a swimming pool \citep{Stam1999}. This class allows for different granularities of discretization in terms of the size of the grid squares; this type of discretization is also similar to existing qualitative models of physical reasoning \citep{Forbus2011} and thus might be a way to tie a computational-level analysis of physical reasoning to algorithmic-level models like qualitative simulation. A second class of models for liquid simulation are \textit{particle-based} Lagrangian methods for simulating the detailed dynamics of small quantities of liquid \citep{Muller2003}. Specifically, these types of particle methods better capture behaviors such as splashing and interaction with other objects. A well-known example is smooth particle hydrodynamics (SPH), which discretizes the liquid into particles, each representing a small volume of the liquid. Particle-based methods like SPH allow a modeler to choose the number of particles, thus providing another avenue by which the amount of computation can be varied. Indeed, this approach is already being explored by some researchers (Bates et al., CogSci 2015).

Methods in physically-based animation usually emphasize stability, plausibility, and efficiency over accuracy \citep{Witkin1997,Stam1999,Muller2002,Guendelman2003,Bridson2003,Muller2003}. In particular, the keyword here is \textit{plausibility}--what is plausible behavior if not something that satisfies human perceptual expectations? I would argue that methods for physically-based animation are already inherently biased towards approximations that are cognitively plausible, and are thus more applicable than the analogous solutions in engineering (which are more focused on accuracy and precision). To given an example, researchers in computer graphics have developed a method of model reduction in which the principle vibration modes of a deformable object are computed, similar to finding the principle components of a matrix \citep{Nealen2006}. Physical behavior can then be simulated in those subspaces, revealing the ways that an object might deform ``in general'' (e.g., for a rectangular rod affixed to a wall, the modes might include bending left/right, bending up/down, twisting, etc.). Because human researchers agree that this algorithm produces perceptually plausible behavior, it is worth investigating whether such an algorithm could be used as a model for how people predict the behavior of deformable objects.

\subsection*{Simulation in control}

One area in which the idea of simulation occurs in cognitive science is the notion of forward and inverse models in motor control \citep{Kawato1999,Flanagan2003}. The idea is that the motor system uses forward models to predict sensations occurring from actions, i.e. $x_{t+1}=f(x_t,u_t)$, where $x_t$ is the state at time $t$ (which may include position, velocity, and sense data) and where $u_t$ is the action taken at time $t$ (which is usually interpreted as force). Inverse models, also known as controllers, determine what action should be taken in order to move from one state to the next, i.e. $u_t=g(x_t,x_{t+1})$. How are these models used, and how are they learned?

Another idea from cognitive science is that of \textit{analysis-by-synthesis}. Originally proposed in the domain of speech recognition \citep{Halle1962,Halle1959}, analysis-by-synthesis is the idea of learning about the world by synthesizing--or simulating--data from one's generative knowledge about how the world works. This idea has been explored in multiple domains, including machine learning in the form of the Helmholtz machine \citep{Dayan1995}, as well as visual perception \citep{Yuille2006}. In the context of forward and inverse models in motor control, a hypothesis based on analysis-by-synthesis would suggest that the sensorimotor system first learns a generative forward model of $p(x_{t+1},x_t,u_t)$ which is then used to learn a discriminative inverse model of $p(u_t|x_{t+1},x_t)$. This would be consistent with the finding that forward models are acquired before control in motor learning \citep{Flanagan2003}.

Recent advances in robotics have experimented with models similar to those that would be predicted by analysis-by-synthesis. \cite{Paraschos2015} have developed a framework for learning \textit{probabilistic motion primitives} (ProMPs) using Gaussian Process (GP) regression. For each motion primitive, they learn a joint distribution over states and controls, $p(x_t, u_t)$, where the states include additional information about velocity and sensory information. Using this generative distribution, they can flexibly combine motion primitives; condition on specific states, controls, or sensory information; and chain motion primitives in sequence. \cite{Levine2015} also learn independent local forward models and controllers and then use the marginal distributions over $p(x_t,u_t)$ from the local models to simulate training data for a discriminative model $p(u_t|x_t)$ instantiated in a neural network. A robot trained with the method from \cite{Levine2015} was able to complete highly dexterous manipulations (such as putting a peg in a hole) or complex, multi-step tasks such as turning a screw with a wrench \citep{Han2015}.

\subsection*{Simulation in planning}

There are two notions of simulation in planning: in one case, planning might involve running simulations (for example, physical simulations) in order to determine the effects of actions; in the other case, the simulation is the online planning process itself. Recent criticism of the use of simulation has argued that simulation is infeasible in planning systems \citep{Davis,Davis2014} and I will discuss some recent research in opposition to that argument. I will also discuss simulated episodes in reinforcement learning as a resource-rational approximation to the general problem of reinforcement learning.

\cite{Davis,Davis2014} argue that simulation (specifically, physical simulation) cannot in general be used for tasks beyond physical prediction. One area that they call out in particular is planning, suggesting that simulation is too precise and/or computationally intensive to be feasible in this domain. I offer two counterexamples to this claim. First, \cite{Aoude2013} simulate trajectories of moving obstacles using a combination of GP regression and rapidly-expanding random trees (RRTs), and demonstrate that planning can not only make use of this type of simulation, but that such information is crucial for making safety guarantees regarding how likely it is for a collision to occur. Second, \cite{Kitaev2015} demonstrate how physical simulation of object dynamics is important for planning robot trajectories when contact with those objects is inevitable. They also show how physical simulation need not depend on Monte Carlo approximations; instead, they use a differentiable dynamics model that allows them to perform the necessary optimization without the need for many Monte Carlo samples.\footnote{\cite{Kitaev2015} did not develop the differentiable physics engine that they use. The physics engine is called MuJoCo and was developed by Mordatch et al.}

Turning to the more general problem of planning: how do people decide what actions to take? The framework specified Markov Decision Processes (MDPs) provide a computational-level solution to this question. Given a set of states $S$, actions $A$, and potentially unknown transitions $p(S^\prime|A,S)$ and rewards $p(R|A,S)$, how should an agent act so as to maximize their expected long-term reward? There are many reinforcement learning algorithms that aim to solve this problem \citep{Sutton1998}, many of which can be run in simulation: that is, the agent does not take real actions, only simulated actions, and observes transitions and rewards according to an internal model of the environment.\footnote{Although reinforcement learning algorithms are frequently classified as \textit{model-based} or \textit{model-free}, a model-free algorithm can still be used in simulation provided the agent has separately learned a model of the environment.} While traditional RL algorithms have usually used point estimates of rewards and transitions, recent Bayes-adaptive methods have proposed maintaining full distributions over possible transition and reward models \citep{Dearden1999}. This has the benefit of allowing the agent to perform efficient inference in complex structured domains \citep{Ross2008}, similar to how probabilistic models of cognition have showed how agents might quickly learn higher level structures such as overhypotheses \citep{Kemp2007}. Even more recently, an algorithm known as Monte-Carlo Tree Search \citep{Browne2012} has been proposed as a more feasible way to perform inference in Bayes-adaptive MDPs, specifically in the formulation of the Bayes-Adaptive Monte Carlo Planning (BAMCP) algorithm \citep{Guez2013}. The BAMCP method suggests a class of approximation algorithms that could apply to how people solve the reinforcement learning problem (which it has been suggested that they do, e.g. \cite{Baker2014}), in which the amount of computation can be adjusted through the number of simulations that are run. Moreover, there are interesting questions regarding the meta-level decision problem of how to optimally use simulation in the context of planning \citep{Hay2012}. While \cite{Hay2012} discussed how the meta-level decision problem applies to MCTS in general, the question has not been posed for the full Bayes-adaptive version (as far as I am aware).

\subsection*{Simulation in probabilistic inference}

Probabilistic inference is perhaps the area of computer science that has already been most widely explored in cognitive science, as it is a fundamental component in probabilistic models of cognition \citep{Tenenbaum2011}. Indeed, \cite{Griffiths2015} use probabilistic simulation as an example of the types of approximation algorithms that should be used in resource-rational analysis. For example, \cite{Lieder2012} perform a resource-rational analysis involving simulation via the Metropolis-Hastings algorithm in order to explain the anchoring bias \citep{Tversky1974}. Similarly, \cite{Abbott2011} use probabilistic simulation via the particle filtering algorithm to explain primacy and recency effects.

Another type of probabilistic simulation known as \textit{Hamiltonian Monte-Carlo} \citep{Neal2011} has parallels to the same types of approximations I discussed earlier in the section on physically-based animation. HMC must similarly solve an intractable differential equation, and does so using the Leapfrog method. The Leapfrog method is an \textit{explicit} integration method, which means that it directly solves for $f(x+\delta x)$ (though it does so in a way that is more robust than Euler's method, which is the canonical explicit method). However, physically-based animation often relies on \textit{implicit} time integration, in which a system of equations with $f(x+\delta x)$ on either side implicitly give the solution; however, the system of equations must further be solved to get the actual solution. Implicit methods tend to be more numerically stable than explicit solutions, and thus it is worth asking the question: could HMC be computed with implicit time integration and still maintain its desirable properties? If so, then changing the discretization of the time integration would be a way to manipulate the amount of computation that HMC requires beyond just the number of samples.

Simulation may also occur in probabilistic inference not through direct Monte-Carlo sampling, but as a prior. \cite{Schulman2013b} make clever use of physical simulation to compute the M-step of the EM algorithm in order to track the topology of deformable objects such as rope, cloth, and foam. To enforce the likelihood term, they convert the likelihood into a potential and then apply that potential as a force in the physics simulation itself. The simulation is biased towards a low-energy state (the prior), and thus the result of stepping the simulation forward after applying the likelihood potential gives a posterior prediction of the current topology of the object. This type of way of thinking about physical simulation in particular offers a different perspective on the idea of physical reasoning being the result of Monte-Carlo sampling.

\subsection*{Conclusion}

To conclude, I have outlined here several areas of computer science that have much to offer, particularly in terms of thinking about how different forms of simulation might be candidate approximation algorithms for use in resource-rational analysis.

\references
\end{document}